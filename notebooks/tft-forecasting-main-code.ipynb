{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# 📊 TFT Time Series Forecasting Pipeline Configuration for a Target Stock\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# === 📦 Imports === #\n",
    "\n",
    "# 🧪 Core Libraries\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 📈 Time Series Modeling (Darts)\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import mae, rmse, smape, mape, r2_score, rmsle, mse\n",
    "\n",
    "# ⚙️ Machine Learning\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# 📋 Utilities\n",
    "from tabulate import tabulate\n",
    "\n",
    "# ❗ Clean log output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 📁 Custom Module\n",
    "import signal_sigma.config.cfg as cfg\n",
    "\n",
    "# DataGathering: Fetch stock & Yahoo macro data\n",
    "from signal_sigma.data_gathering import DataGathering\n",
    "\n",
    "# DataPreparator: Prepare dataset for modeling\n",
    "from signal_sigma.data_preparator import DataPreparator\n",
    "\n",
    "# FeatureEngineering: Generate technical indicators\n",
    "from signal_sigma.feature_engineering import FeatureEngineering\n",
    "\n",
    "# FredMacroProcessor: FED macroeconomic data + composites\n",
    "from signal_sigma.fred_macro import FredMacroProcessor\n",
    "\n",
    "# MarketMacroCompressor: Compress Yahoo macro indicators\n",
    "from signal_sigma.market_macro_compressor import MarketMacroCompressor\n",
    "\n",
    "# TemporalFeatureCombiner: Combine temporal features\n",
    "from signal_sigma.temporal_feature_combiner import TemporalFeatureCombiner\n",
    "\n",
    "# FeatureSelector: Select features for modeling\n",
    "from signal_sigma.features_selection import ReducedFeatureSelector\n",
    "\n",
    "# LossHistory: Track model loss history\n",
    "from signal_sigma.loss_history import LossHistory\n",
    "\n",
    "# === 🛠️ Configuration Parameters === #\n",
    "\n",
    "# 📁 Data save path\n",
    "path_stock = \"../data/Stock_market_data\"\n",
    "\n",
    "# 📅 Time range\n",
    "start_date = \"2014-01-01\"\n",
    "end_date = \"2025-05-09\"\n",
    "\n",
    "# 🎯 Target stock for forecasting\n",
    "target_stock = \"AAPL\"\n",
    "\n",
    "# ⏲️ Model training parameters\n",
    "output_len = 15\n",
    "n_epochs = 200\n",
    "\n",
    "# 📈 Ticker list for model universe\n",
    "tickers = cfg.STOCK_TICKERS\n",
    "\n",
    "# 📉 Macro indicators (Yahoo Finance-based)\n",
    "# Volatility Index – fear gauge\n",
    "macro_tickers = {\"^VIX\": \"VIX_Index\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 🔁 Step 1: Gather raw stock  === #\n",
    "gatherer = DataGathering(\n",
    "    stock_tickers=tickers,\n",
    "    macro_tickers=macro_tickers,\n",
    "    start_date=\"2014-01-01\",\n",
    "    end_date=\"2025-05-10\",\n",
    ")\n",
    "\n",
    "raw_data_stock_yahoo = (\n",
    "    gatherer.run()\n",
    ")  # Fetch & structure historical OHLCV + Yahoo macro data\n",
    "print(\"\\n =======================================\")\n",
    "# === 📈 Step 2: FRED macroeconomic indicators (CPI, Fed Rate, Unemployment...) === #\n",
    "processor = FredMacroProcessor(start_date=start_date, save_path=path_stock)\n",
    "macro_FRED_df = (\n",
    "    processor.run_pipeline()\n",
    ")  # Fetch + construct 3 composite macroeconomic indicators\n",
    "macro_FRED_df.to_csv(\n",
    "    f\"{path_stock}/tft_macro_FRED_df_{start_date}_{end_date}.csv\",\n",
    "    index=True,\n",
    "    date_format=\"%Y-%m-%d\",\n",
    ")\n",
    "\n",
    "ferd_feature_list = macro_FRED_df.columns.to_list()\n",
    "print(\"\\n =======================================\")\n",
    "# === 📊 Step 3: Yahoo macro compression (combine VIX, Gold, Bitcoin, etc.) === #\n",
    "compressor = MarketMacroCompressor(\n",
    "    start=start_date\n",
    ")  # Use same start date for alignment\n",
    "macro_Yahoo_df = (\n",
    "    compressor.generate_macro_features()\n",
    ")  # Output: 3 macro features from Yahoo indicators\n",
    "macro_Yahoo_df.to_csv(\n",
    "    f\"{path_stock}/tft_macro_Yahoo_df_{start_date}_{end_date}.csv\",\n",
    "    index=True,\n",
    "    date_format=\"%Y-%m-%d\",\n",
    ")\n",
    "yahoo_feature_list = macro_Yahoo_df.columns.to_list()\n",
    "print(\"\\n =======================================\")\n",
    "# === 🔗 Step 4: Merge all datasets by date (left join) === #\n",
    "raw_data_stock_economy = raw_data_stock_yahoo.merge(\n",
    "    macro_FRED_df, left_index=True, right_index=True, how=\"left\"\n",
    ").merge(macro_Yahoo_df, left_index=True, right_index=True, how=\"left\")\n",
    "raw_data_stock_economy.ffill(inplace=True)\n",
    "print(\"\\n =======================================\")\n",
    "# Explanation:\n",
    "# - All three DataFrames must have a datetime index\n",
    "# - left join preserves all dates across all datasets\n",
    "# - Missing values may exist — will be handled later\n",
    "\n",
    "# === 🧬 Step 5: Optional — Save clean raw data for reuse or inspection === #\n",
    "tft_raw_data_stock_economy = copy.deepcopy(\n",
    "    raw_data_stock_economy\n",
    ")  # Make a deep copy (safe backup)\n",
    "tft_raw_data_stock_economy.to_csv(\n",
    "    f\"{path_stock}/tft_raw_data_stock_economy_{start_date}_{end_date}.csv\",\n",
    "    index=True,\n",
    "    date_format=\"%Y-%m-%d\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"✅ Saved full merged data to 'tft_raw_data_stock_economy_{start_date}_{end_date}.csv'\"\n",
    ")\n",
    "print(\"\\n =======================================\")\n",
    "print(\"[INFO] tft_raw_data_stock_economy summary:\")\n",
    "print(\"→ Shape      :\", tft_raw_data_stock_economy.shape)\n",
    "print(\"→ NaNs       :\", tft_raw_data_stock_economy.isnull().sum().sum())\n",
    "print(\n",
    "    \"→ Index Range:\",\n",
    "    tft_raw_data_stock_economy.index.min(),\n",
    "    \"→\",\n",
    "    tft_raw_data_stock_economy.index.max(),\n",
    ")\n",
    "print(\"→ Columns    :\", list(tft_raw_data_stock_economy.columns), \"...\")\n",
    "print(\"\\n =======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2-1: Prepare modeling dataset (Featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare modeling dataset directly from  CSV file or from the output of last cell\n",
    "tft_raw_data_stock_economy = pd.read_csv(\n",
    "    f\"{path_stock}/tft_raw_data_stock_economy_{start_date}_{end_date}.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=\"date\",\n",
    ").sort_index()\n",
    "\n",
    "tft_macro_Yahoo_df = pd.read_csv(\n",
    "    f\"{path_stock}/tft_macro_Yahoo_df_{start_date}_{end_date}.csv\",\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    ").sort_index()\n",
    "\n",
    "tft_macro_FRED_df = pd.read_csv(\n",
    "    f\"{path_stock}/tft_macro_FRED_df_{start_date}_{end_date}.csv\",\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    ").sort_index()\n",
    "\n",
    "\n",
    "# === 📈 Step 2: FRED macroeconomic indicators (CPI, Fed Rate, Unemployment...) === #\n",
    "ferd_feature_list = tft_macro_FRED_df.columns.to_list()\n",
    "print(\"\\n =======================================\")\n",
    "# === 📊 Step 3: Yahoo macro compression (combine VIX, Gold, Bitcoin, etc.) === #\n",
    "yahoo_feature_list = tft_macro_Yahoo_df.columns.to_list()\n",
    "print(\"\\n =======================================\")\n",
    "\n",
    "preparator = DataPreparator(\n",
    "    data=tft_raw_data_stock_economy,\n",
    "    target_stock=target_stock,\n",
    "    stock_list=tickers,\n",
    "    time_cutoff=\"2017-01-01\",  # please not change this value\n",
    ")\n",
    "model_dataset = preparator.prepare()\n",
    "model_dataset.to_csv(\n",
    "    f\"{path_stock}/tft_model_dataset_feature_{end_date}.csv\",\n",
    "    index=True,\n",
    "    date_format=\"%Y-%m-%d\",\n",
    ")\n",
    "print(\n",
    "    f\" Saved full Featured Engineering  'tft_model_dataset_feature_{end_date}.csv.csv'\"\n",
    ")\n",
    "\n",
    "model_dataset_copy = copy.deepcopy(model_dataset)  # Backup copy\n",
    "print(\"\\n [INFO] Loading finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine temporally grouped features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Combine Features from FERD\n",
    "ferd_feature_list\n",
    "print(f\" Number of Combine Features from FERD\", len(ferd_feature_list))\n",
    "print(f\" Features from FERD\", ferd_feature_list)\n",
    "print(\"\\n =======================================\")\n",
    "\n",
    "yahoo_feature_list\n",
    "print(f\" Number of Combine Features from Yahoo\", len(yahoo_feature_list))\n",
    "print(f\" Features from Yahoo\", yahoo_feature_list)\n",
    "print(\"\\n =======================================\")\n",
    "\n",
    "\n",
    "# Combine temporally grouped features\n",
    "combiner = TemporalFeatureCombiner(model_dataset)\n",
    "model_dataset_combined = combiner.combine()\n",
    "\n",
    "model_dataset_combined_copy = copy.deepcopy(model_dataset_combined)  # Backup copy\n",
    "\n",
    "combine_col_all = model_dataset_combined.columns.to_list()\n",
    "\n",
    "print(\n",
    "    f\" Number of all avaialbe in model_dataset_combined (Additional TI) \",\n",
    "    len(combine_col_all),\n",
    ")\n",
    "print(f\" all avaialbe in model_dataset_combined (Additional TI)\", combine_col_all)\n",
    "print(\"\\n =======================================\")\n",
    "\n",
    "\n",
    "# Keep only the combined features + 'target'\n",
    "seasonal_cols = [\n",
    "    col for col in model_dataset_combined.columns if col.startswith(\"season_\")\n",
    "]\n",
    "print(\"\\n [INFO] seasonal_cols :\", seasonal_cols)\n",
    "print(f\" Number of seasonal_cols \", len(seasonal_cols))\n",
    "print(\"\\n =======================================\")\n",
    "\n",
    "\n",
    "original_cols = set(model_dataset.columns) - {\"target\"}\n",
    "combined_features = [\n",
    "    col for col in model_dataset_combined.columns if col not in original_cols\n",
    "]\n",
    "print(\"\\n [INFO] only Additional Combine Features based on TI :\", combined_features)\n",
    "print(f\" Number of Additional Combine Features based on TI\", len(combined_features))\n",
    "print(\"\\n =======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Feature selectio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features into one list - select the feature you want for the training\n",
    "top_5_feature_ai_offer = [\n",
    "    \"momentum_rsi_75\",\n",
    "    \"trend_macd\",\n",
    "    \"return_5d\",\n",
    "    \"price_investment_trend_7d\",\n",
    "    \"volatility_atr_14\",\n",
    "]\n",
    "\n",
    "\n",
    "low_dimension_feature = (\n",
    "    top_5_feature_ai_offer + combined_features + yahoo_feature_list + ferd_feature_list\n",
    ")  # + seasonal_cols  #if needed\n",
    "# Remove duplicates (preserving order)\n",
    "low_dimension_feature = list(dict.fromkeys(low_dimension_feature))\n",
    "print(\"\\n [INFO] low_dimension_feature :\", low_dimension_feature)\n",
    "print(f\" Number of Features based low_dimension_feature\", len(low_dimension_feature))\n",
    "print(\"\\n =======================================\")\n",
    "\n",
    "model_dataset_low_dimension = model_dataset_combined[low_dimension_feature]\n",
    "\n",
    "model_dataset.to_csv(\n",
    "    f\"{path_stock}/model_dataset_{end_date}.csv\", index=True, date_format=\"%Y-%m-%d\"\n",
    ")\n",
    "print(f\" Saved full Featured Engineering  'model_dataset_{end_date}.csv.csv'\")\n",
    "\n",
    "model_dataset_low_dimension.to_csv(\n",
    "    f\"{path_stock}/model_dataset_low_dimension_{end_date}.csv\",\n",
    "    index=True,\n",
    "    date_format=\"%Y-%m-%d\",\n",
    ")\n",
    "print(\n",
    "    f\" Saved full Featured Engineering  'model_dataset_low_dimension_{end_date}.csv.csv'\"\n",
    ")\n",
    "\n",
    "print(\"\\n =======================================\")\n",
    "print(\"[INFO] model_dataset_combined summary:\")\n",
    "print(\"→ Shape      :\", model_dataset_combined.shape)\n",
    "print(\"→ NaNs       :\", model_dataset_combined.isnull().sum().sum())\n",
    "print(\n",
    "    \"→ Index Range:\",\n",
    "    model_dataset_combined.index.min(),\n",
    "    \"→\",\n",
    "    model_dataset_combined.index.max(),\n",
    ")\n",
    "print(\"→ Columns    :\", list(model_dataset_combined.columns), \"...\")\n",
    "print(\"\\n =======================================\")\n",
    "print(\"[INFO] model_dataset_low_dimension summary:\")\n",
    "print(\"→ Shape      :\", model_dataset_low_dimension.shape)\n",
    "print(\"→ NaNs       :\", model_dataset_low_dimension.isnull().sum().sum())\n",
    "print(\n",
    "    \"→ Index Range:\",\n",
    "    model_dataset_low_dimension.index.min(),\n",
    "    \"→\",\n",
    "    model_dataset_low_dimension.index.max(),\n",
    ")\n",
    "print(\"→ Columns    :\", list(model_dataset_low_dimension.columns), \"...\")\n",
    "print(\n",
    "    \"\\n[INFO] ✅ Ready: Reduced feature dataset with only combined features + target.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔄 Step 2-2: Automatic Features Slection baed on Mutual Information , XGBOOST and VIF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_entry_feature_selection = [\n",
    "    col for col in model_dataset_combined.columns if col not in seasonal_cols\n",
    "]\n",
    "col_entry_feature_selection = list(dict.fromkeys(col_entry_feature_selection))\n",
    "print(\"\\n =======================================\")\n",
    "print(\n",
    "    \"search for the top features in : \",\n",
    "    model_dataset_combined[col_entry_feature_selection].columns,\n",
    ")\n",
    "print(\"\\n =======================================\")\n",
    "# model_dataset_low_dimension\n",
    "selector = ReducedFeatureSelector(\n",
    "    data=model_dataset_combined[col_entry_feature_selection],\n",
    "    target_col=\"target\",\n",
    "    top_n=10,\n",
    ")\n",
    "automatic_selected_features, model_scores = selector.select_features()\n",
    "\n",
    "print(\"\\n[INFO] Final Selected Features:\")\n",
    "print(automatic_selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔄 Step 2-3: Prepre the Columns for Datasets Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine with seasonal columns and exclude 'target' correctly\n",
    "\n",
    "# model_dataset_low_dimension = model_dataset_combined[low_dimension_feature] # manual selection\n",
    "\n",
    "# model_dataset_combined[low_dimension_feature]\n",
    "\n",
    "# automatic_selected_features = [['volatility_donchian_low_3', 'volatility_donchian_high_3']+ ['target']]\n",
    "\n",
    "# ['target']\n",
    "\n",
    "# feature selection based ML (RF, XGBOOST, CORR())\n",
    "model_dataset_low_dimension = model_dataset_combined[\n",
    "    automatic_selected_features + [\"target\"]\n",
    "]\n",
    "\n",
    "model_df = model_dataset_low_dimension\n",
    "features_list_model = [\n",
    "    col for col in model_dataset_low_dimension.columns if col != \"target\"\n",
    "]\n",
    "print(\"\\n[INFO] numbers of features_list for model:\", len(features_list_model))\n",
    "print(\"\\n[INFO] features_list for model:\", features_list_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2-4:Reindexing to daily frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 🧾 Step 2: Inspect & Standardize Time Index (Reindexing to Daily Frequency)\n",
    "# ------------------------------------------------------------------------------\n",
    "# if you want to use the combine neeed to use below othervise disable them\n",
    "\n",
    "print(\"\\n[INFO] Loaded model_dataset info ---before--- Reindexing to daily frequency\")\n",
    "\n",
    "# Show current dataset shape\n",
    "print(\"[INFO] Loaded model_dataset shaped: \", model_df.shape)\n",
    "\n",
    "# Count total missing values (NaNs) before interpolation\n",
    "print(\"[INFO] Loaded model_dataset NaN: \", model_df.isnull().sum().sum())\n",
    "\n",
    "# Print dataset type to confirm it's a DataFrame\n",
    "print(\"[INFO] Loaded model_dataset type: \", type(model_df))\n",
    "\n",
    "# Show time index range (first and last date)\n",
    "print(\n",
    "    \"[INFO] Loaded model_dataset index:\",\n",
    "    model_df.index.min(),\n",
    "    \"--- to ---\",\n",
    "    model_df.index.max(),\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n[INFO] Loaded model_dataset info ---After---  Reindexing to daily frequency\")\n",
    "\n",
    "# ✅ Generate full date range from min to max using daily frequency\n",
    "full_index = pd.date_range(\n",
    "    start=model_df.index.min(), end=model_df.index.max(), freq=\"D\"\n",
    ")\n",
    "\n",
    "# 🧠 Check if the time index is irregular or not sorted\n",
    "if model_df.index.inferred_freq is None or not model_df.index.is_monotonic_increasing:\n",
    "    print(\"[INFO] Reindexing to daily frequency and interpolating missing values...\")\n",
    "\n",
    "    # 📅 Reindex the DataFrame to ensure a daily time index (even if days were missing)\n",
    "    model_df = model_df.reindex(full_index)\n",
    "\n",
    "    # 🔧 Fill gaps using linear interpolation in both directions (forward & backward)\n",
    "    model_df.interpolate(method=\"linear\", limit_direction=\"both\", inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# ✅ Show updated dataset info after reindexing and interpolation\n",
    "\n",
    "print(\"[INFO] Loaded model_dataset shaped: \", model_dataset.shape)\n",
    "print(\"[INFO] Loaded model_dataset NaN: \", model_dataset.isnull().sum().sum())\n",
    "print(\"[INFO] Loaded model_dataset type: \", type(model_dataset))\n",
    "print(\n",
    "    \"[INFO] Loaded model_dataset index:\",\n",
    "    model_dataset.index.min(),\n",
    "    \"--- to ---\",\n",
    "    model_dataset.index.max(),\n",
    ")\n",
    "\n",
    "print(\"\\n[INFO] Finished Reindexing to daily frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔁 Step 3: Convert to Darts TimeSeries format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# 🎯 Step 3-1: Convert Target Column to Darts TimeSeries\n",
    "# ---------------------------------------------\n",
    "\n",
    "from darts import TimeSeries  # ⏳ Darts TimeSeries object for forecasting\n",
    "import numpy as np  # 🧮 NumPy for handling NaN and array stats\n",
    "\n",
    "print(\"\\n[INFO] Step 3: Convert to Darts TimeSeries format\\n\")\n",
    "\n",
    "# # XXX: On Apple Silicon, Darts may require float32 instead of float64\n",
    "# # to avoid issues with PyTorch. This is a workaround for compatibility.\n",
    "# cols = model_df.select_dtypes(include=[\"float64\"]).columns\n",
    "# model_df[cols] = model_df[cols].astype(np.float32)\n",
    "# # # Or alternatively, you can set the device to CPU\n",
    "# # import torch\n",
    "# # torch_device = torch.device(\"cpu\")\n",
    "\n",
    "# ✅ Extract target column as a DataFrame (not Series) to retain column name\n",
    "raw_target_df = model_df[[\"target\"]]\n",
    "\n",
    "# ✅ Convert to Darts TimeSeries:\n",
    "# - value_cols=\"target\" tells Darts which column contains values\n",
    "# - freq=\"D\" sets the expected time step to daily\n",
    "# - fill_missing_dates=True fills missing dates in the index to ensure regular spacing\n",
    "raw_target = TimeSeries.from_dataframe(\n",
    "    raw_target_df, value_cols=\"target\", freq=\"D\", fill_missing_dates=True\n",
    ")\n",
    "\n",
    "# 📋 Log target series info\n",
    "print(\"[INFO] raw_target\")\n",
    "print(f\"[INFO]   • Type        : {type(raw_target)}\")  # Confirm TimeSeries object\n",
    "print(f\"[INFO]   • Length      : {len(raw_target)}\")  # Total time steps\n",
    "print(f\"[INFO]   • Shape       : {raw_target.shape}\")  # Shape: (time, components, 1)\n",
    "print(\n",
    "    f\"[INFO]   • Frequency   : {raw_target.freq_str}\"\n",
    ")  # Frequency string (should be 'D')\n",
    "print(\n",
    "    f\"[INFO]   • NaNs        : {np.isnan(raw_target.values()).sum()}\"\n",
    ")  # Total NaNs in the time series\n",
    "print(\n",
    "    f\"[INFO]   • Time Range  : {raw_target.time_index.min()} → {raw_target.time_index.max()}\"\n",
    ")  # Date range\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 🎯 Step 3-2:Convert Feature Columns to Darts TimeSeries\n",
    "# ------------------------------------------------\n",
    "\n",
    "# ✅ Extract multiple feature columns for covariates\n",
    "raw_covariates_df = model_df[features_list_model]\n",
    "\n",
    "# ✅ Convert multivariate features to TimeSeries:\n",
    "# - value_cols=features_list_model ensures all selected columns are included\n",
    "# - fill_missing_dates ensures continuity\n",
    "raw_covariates = TimeSeries.from_dataframe(\n",
    "    raw_covariates_df, value_cols=features_list_model, freq=\"D\", fill_missing_dates=True\n",
    ")\n",
    "\n",
    "# 📋 Log covariates series info\n",
    "print(\"[INFO] raw_covariates\")\n",
    "print(f\"[INFO]   • Type        : {type(raw_covariates)}\")  # Confirm TimeSeries object\n",
    "print(f\"[INFO]   • Length      : {len(raw_covariates)}\")  # Total time steps\n",
    "print(f\"[INFO]   • Shape       : {raw_covariates.shape}\")  # (time, features, 1)\n",
    "print(f\"[INFO]   • Frequency   : {raw_covariates.freq_str}\")  # 'D' for daily\n",
    "print(\n",
    "    f\"[INFO]   • NaNs        : {np.isnan(raw_covariates.values()).sum()}\"\n",
    ")  # Total missing values\n",
    "print(\n",
    "    f\"[INFO]   • Time Range  : {raw_covariates.time_index.min()} → {raw_covariates.time_index.max()}\"\n",
    ")  # Time span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3-3: Sanity Check – Are raw_target and raw_covariates aligned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# ✅ Step 3-3: Sanity Check – Are raw_target and raw_covariates aligned?\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "aligned = True\n",
    "\n",
    "# 1. Check if time ranges are equal\n",
    "if (\n",
    "    raw_target.start_time() != raw_covariates.start_time()\n",
    "    or raw_target.end_time() != raw_covariates.end_time()\n",
    "):\n",
    "    print(\"[WARNING] ⚠️ Time range mismatch between target and covariates!\")\n",
    "    print(f\"  → raw_target    : {raw_target.start_time()} → {raw_target.end_time()}\")\n",
    "    print(\n",
    "        f\"  → raw_covariates: {raw_covariates.start_time()} → {raw_covariates.end_time()}\"\n",
    "    )\n",
    "    aligned = False\n",
    "\n",
    "# 2. Check if lengths are equal\n",
    "if len(raw_target) != len(raw_covariates):\n",
    "    print(\"[WARNING] ⚠️ Length mismatch between target and covariates!\")\n",
    "    print(f\"  → raw_target length    : {len(raw_target)}\")\n",
    "    print(f\"  → raw_covariates length: {len(raw_covariates)}\")\n",
    "    aligned = False\n",
    "\n",
    "# 3. Check if frequencies match\n",
    "if raw_target.freq != raw_covariates.freq:\n",
    "    print(\"[WARNING] ⚠️ Frequency mismatch between target and covariates!\")\n",
    "    print(f\"  → raw_target freq    : {raw_target.freq}\")\n",
    "    print(f\"  → raw_covariates freq: {raw_covariates.freq}\")\n",
    "    aligned = False\n",
    "\n",
    "# Final check result\n",
    "if aligned:\n",
    "    print(\"[INFO] ✅ raw_target and raw_covariates are perfectly aligned and ready!\")\n",
    "else:\n",
    "    print(\"[ERROR] ❌ Alignment check failed — please fix before training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Time-based Split for Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# ✂️ Step 4: Time-based Split for Train/Val/Test\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[INFO] Step 4: Define forecast configuration and create dataset splits\\n\")\n",
    "from datetime import timedelta  # Used to offset time ranges for slicing covariates\n",
    "\n",
    "# --------------------------------------\n",
    "# 🔧 Forecast Configuration\n",
    "# --------------------------------------\n",
    "\n",
    "output_len = output_len  # Number of days we want to predict into the future\n",
    "input_len = (\n",
    "    output_len * 5\n",
    ")  # Number of past days (lookback window) to feed into the model\n",
    "\n",
    "# Validation size: Use either 2 full input-output blocks or 10% of the data length, whichever is smaller\n",
    "val_size = min(((input_len + output_len) + 5), int(0.1 * len(raw_target)))\n",
    "\n",
    "# Logging forecast settings\n",
    "print(\n",
    "    f\"[INFO] Forecast Horizon (output_len): {output_len} days\"\n",
    ")  # Expected prediction range\n",
    "print(\n",
    "    f\"[INFO] Input Sequence Length (input_len): {input_len} days\"\n",
    ")  # Input length to model\n",
    "print(\n",
    "    f\"[INFO] Validation Size (val_size): {val_size} samples\"\n",
    ")  # Size of validation block\n",
    "print(f\"[INFO] Total available time steps: {len(raw_target)}\")  # Full dataset length\n",
    "\n",
    "# --------------------------------------\n",
    "# 🟩 Create Target Splits\n",
    "# --------------------------------------\n",
    "\n",
    "# Training target: exclude the validation and test periods at the end\n",
    "train_target_raw = raw_target[: -val_size - output_len]\n",
    "\n",
    "# Validation target: right before the test set, ending just before the final output_len\n",
    "val_target_raw = raw_target[-val_size - output_len : -output_len]\n",
    "\n",
    "# Testing target: final output_len days, reserved for out-of-sample evaluation\n",
    "test_target_raw = raw_target[-output_len:]\n",
    "\n",
    "# Log time step sizes of each target set\n",
    "print(\"\\n[INFO] Target TimeSeries Splits:\")\n",
    "print(f\"[INFO]   • Train Target      : {len(train_target_raw)} time steps\")\n",
    "print(f\"[INFO]   • Validation Target : {len(val_target_raw)} time steps\")\n",
    "print(f\"[INFO]   • Test Target       : {len(test_target_raw)} time steps\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 📌 Create Aligned Covariate Splits\n",
    "# --------------------------------------\n",
    "\n",
    "# Training covariates: match training target range + output_len (needed for decoder)\n",
    "train_covariates_raw = raw_covariates.slice(\n",
    "    train_target_raw.start_time(),  # Start of training period\n",
    "    train_target_raw.end_time()\n",
    "    + timedelta(days=output_len),  # Extend to include future forecast period\n",
    ")\n",
    "\n",
    "# Validation covariates: match validation target range + output_len\n",
    "val_covariates_raw = raw_covariates.slice(\n",
    "    val_target_raw.start_time(),  # Start of validation period\n",
    "    val_target_raw.end_time()\n",
    "    + timedelta(days=output_len),  # Extend to include forecast period\n",
    ")\n",
    "\n",
    "# Test covariates: go back input_len days before the test start to allow full encoder sequence\n",
    "# and forward output_len days for prediction\n",
    "test_covariates_raw = raw_covariates.slice(\n",
    "    test_target_raw.start_time()\n",
    "    - timedelta(days=input_len),  # Start: back-projected from test start\n",
    "    test_target_raw.end_time()\n",
    "    + timedelta(days=output_len),  # End: includes forecast window\n",
    ")\n",
    "\n",
    "# Log time step sizes of each covariate set\n",
    "print(\"\\n[INFO] Covariate TimeSeries Splits:\")\n",
    "print(f\"[INFO]   • Train Covariates  : {len(train_covariates_raw)} time steps\")\n",
    "print(f\"[INFO]   • Val Covariates    : {len(val_covariates_raw)} time steps\")\n",
    "print(f\"[INFO]   • Test Covariates   : {len(test_covariates_raw)} time steps\")\n",
    "print(\n",
    "    f\"[INFO]   • Covariate Dim     : {raw_covariates.width}\"\n",
    ")  # Number of covariate variables (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Scale Target and Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 🧮 Step 5: Normalize TimeSeries Data Using Darts Scaler\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Instantiate separate scalers:\n",
    "# - `t_scaler` for the target series (to avoid leakage)\n",
    "# - `f_scaler` for covariates (features)\n",
    "t_scaler = Scaler()  # Target scaler\n",
    "f_scaler = Scaler()  # Feature/covariates scaler\n",
    "\n",
    "# Fit the target scaler on the training target only and transform all splits\n",
    "train_target = t_scaler.fit_transform(\n",
    "    train_target_raw\n",
    ")  # Learn scaling on training target\n",
    "val_target = t_scaler.transform(val_target_raw)  # Apply same scale to validation target\n",
    "test_target = t_scaler.transform(test_target_raw)  # Apply same scale to test target\n",
    "print(\"\\n[INFO] Step 5🧾 target scaler Time Index Ranges after Scaling:\")\n",
    "print(\n",
    "    f\"[INFO] Train Target         : {train_target.time_index.min()} → {train_target.time_index.max()}\"\n",
    ")\n",
    "print(\n",
    "    f\"[INFO] Validation Target    : {val_target.time_index.min()} → {val_target.time_index.max()}\"\n",
    ")\n",
    "print(\n",
    "    f\"[INFO] Test Target          : {test_target.time_index.min()} → {test_target.time_index.max()}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the covariate scaler on the training covariates only and transform all splits\n",
    "train_covariates = f_scaler.fit_transform(\n",
    "    train_covariates_raw\n",
    ")  # Learn scaling on training covariates\n",
    "val_covariates = f_scaler.transform(\n",
    "    val_covariates_raw\n",
    ")  # Apply same scale to validation covariates\n",
    "test_covariates = f_scaler.transform(\n",
    "    test_covariates_raw\n",
    ")  # Apply same scale to test covariates\n",
    "\n",
    "print(\"\\n[INFO] Step 5🧾 covariate scaler Time Index Ranges after Scaling:\")\n",
    "print(\n",
    "    f\"[INFO] Train Covariates    : {train_covariates.time_index.min()} → {train_covariates.time_index.max()}\"\n",
    ")\n",
    "print(\n",
    "    f\"[INFO] Validation Covariates: {val_covariates.time_index.min()} → {val_covariates.time_index.max()}\"\n",
    ")\n",
    "print(\n",
    "    f\"[INFO] Test Covariates      : {test_covariates.time_index.min()} → {test_covariates.time_index.max()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Initialize and Train TFT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_logger = LossHistory()\n",
    "from darts.models import NBEATSModel\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# 📊 Define the N-BEATS model\n",
    "model = NBEATSModel(\n",
    "    input_chunk_length=input_len,  # 🔁 Number of past time steps used as input (backcast length)\n",
    "    output_chunk_length=output_len,  # 🔮 Number of future time steps to predict (forecast length)\n",
    "    # 🧱 Model architecture\n",
    "    generic_architecture=True,  # 🧩 Use the generic block architecture (as in the original paper)\n",
    "    num_stacks=30,  # 📚 Number of stacks (each stack can learn different components)\n",
    "    num_blocks=1,  # 🧱 Number of blocks per stack (often 1 is sufficient)\n",
    "    num_layers=4,  # 🧬 Number of FC layers in each block\n",
    "    layer_widths=512,  # 📐 Width of each fully connected layer in a block\n",
    "    # 🎯 Forecasting strategy\n",
    "    likelihood=QuantileRegression(\n",
    "        quantiles=[0.1, 0.5, 0.9]\n",
    "    ),  # 🎯 Probabilistic forecasting (quantile intervals)\n",
    "    loss_fn=None,  # ❌ Not needed if using a likelihood function\n",
    "    # 🧪 Training settings\n",
    "    n_epochs=n_epochs,  # 🔁 Number of training epochs\n",
    "    batch_size=64,  # 📦 Size of each mini-batch\n",
    "    model_name=\"nbeats_forecast\",  # 🏷 Name for checkpointing/saving\n",
    "    random_state=42,  # 🎲 For reproducibility\n",
    "    force_reset=True,  # 🧽 Reset model state before training\n",
    "    # 💾 Model saving/checkpointing\n",
    "    save_checkpoints=True,  # 💾 Automatically save intermediate models\n",
    "    # ⚙️ Additional PyTorch Lightning settings\n",
    "    pl_trainer_kwargs={\n",
    "        \"callbacks\": [\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=5\n",
    "            ),  # 🛑 Stop early if validation loss doesn't improve\n",
    "            loss_logger,  # 📊 Optional: Your custom loss logging callback\n",
    "        ],\n",
    "        \"log_every_n_steps\": 10,  # 📋 Logging frequency per training step\n",
    "        \"enable_model_summary\": True,  # 🧠 Print model architecture summary\n",
    "        \"enable_progress_bar\": True,  # 📈 Show training progress visually\n",
    "        \"logger\": True,  # 📓 Use built-in logger (TensorBoard compatible)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# 🧠 Step 6: Initialize and Train TFT Model\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer (TFT) model from Darts\n",
    "\n",
    "loss_logger = LossHistory()  # Instantiate the callback\n",
    "model = TFTModel(\n",
    "    input_chunk_length=input_len,  # 🔁 Number of historical time steps used as input\n",
    "    output_chunk_length=output_len,  # 🔮 Number of future time steps to predict\n",
    "    hidden_size=32,  # 💡 Number of hidden units in LSTM layers (feature extractor size)\n",
    "    lstm_layers=1,  # 🔄 Number of LSTM layers used in the encoder-decoder architecture\n",
    "    dropout=0.1,  # 🕳 Dropout rate for regularization (to prevent overfitting)\n",
    "    batch_size=64,  # 📦 Number of samples per training batch\n",
    "    n_epochs=n_epochs,  # 🔁 Number of training epochs\n",
    "    num_attention_heads=1,  # 🎯 Heads in multi-head attention layer for learning temporal patterns\n",
    "    force_reset=True,  # 🧽 Force fresh training, resetting previous weights and checkpoints\n",
    "    save_checkpoints=True,  # 💾 Save intermediate models automatically during training\n",
    "    # 🎯 Quantile regression for probabilistic forecasting (predicts intervals)\n",
    "    likelihood=QuantileRegression(quantiles=[0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]),\n",
    "    # 🗓️ Add time-based encoders to give model temporal awareness\n",
    "    add_encoders={\n",
    "        \"cyclic\": {\n",
    "            \"past\": [\n",
    "                \"month\",\n",
    "                \"day\",\n",
    "                \"weekday\",\n",
    "            ]  # Cyclical encodings (e.g., sin/cos for months, days)\n",
    "        },\n",
    "        \"datetime_attribute\": {\n",
    "            \"past\": [\"year\", \"month\", \"weekday\"]  # Raw datetime attributes\n",
    "        },\n",
    "        \"position\": {\n",
    "            \"past\": [\"relative\"]  # Relative time encoding (position in sequence)\n",
    "        },\n",
    "    },\n",
    "    # ⚙️ Pass advanced settings to PyTorch Lightning trainer\n",
    "    # ⚙️ Pass advanced settings to PyTorch Lightning trainer\n",
    "    pl_trainer_kwargs={\n",
    "        \"callbacks\": [\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=3),  # ⏸ Early stopping callback\n",
    "            loss_logger,  # 📊 Our custom loss tracking callback\n",
    "        ],\n",
    "        \"log_every_n_steps\": 10,  # 📝 How often to log training steps\n",
    "        \"enable_model_summary\": True,  # 📋 Show model layer summary\n",
    "        \"enable_progress_bar\": True,  # 📊 Show progress bar during training\n",
    "        \"logger\": True,  # 🧾 Enable default logger (e.g., TensorBoard)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 🚀 Fit (Train) the Model\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "model.fit(\n",
    "    series=train_target,  # 🎯 Target series for training\n",
    "    future_covariates=train_covariates,  # 🔮 Associated features for training (aligned with future steps)\n",
    "    # past_covariates=train_covariates,     # 🔮 Associated features for training (aligned with future steps)\n",
    "    val_series=val_target,  # 🎯 Validation target series (to monitor overfitting)\n",
    "    val_future_covariates=val_covariates,  # 🔮 Validation covariates aligned with val_target\n",
    "    verbose=True,  # 📣 Print training progress to console\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Plot Train vs Validation Loss Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# Step 7: Plot Train vs Validation Loss Over Epochs\n",
    "# -------------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "plt.rcParams[\"savefig.format\"] = \"svg\"\n",
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_logger.epochs, loss_logger.train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(\n",
    "    loss_logger.epochs, loss_logger.val_losses, label=\"Validation Loss\", marker=\"o\"\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Train vs Validation Loss Over Epochs -\\n Feature:{features_list_model}\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔮 Single-Shot Forecasting\n",
    "\n",
    "Single-shot forecasting refers to generating predictions for the entire future horizon **in one step**, using the most recent available data. It is commonly used for:\n",
    "\n",
    "- ✅ Final model evaluation on a holdout test set.\n",
    "- ✅ Real-world inference for upcoming time periods.\n",
    "- ✅ Visualizing forecast uncertainty with prediction intervals.\n",
    "\n",
    "This method returns:\n",
    "- **Point forecasts** (e.g., median or mean prediction).\n",
    "- **Probabilistic forecasts** via quantiles (e.g., p10, p50, p90), capturing uncertainty in predictions.\n",
    "\n",
    "> 📌 **Use this when**: You want a fast, one-time forecast from the current moment forward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 8: Single-Shot Forecast for Singel Point and Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# 🔮 Step 8: Forecast with Singel Point and Quantiles\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# 🔮 Generate probabilistic forecast using quantiles over the next `output_len` time steps\n",
    "quantil_forecast = model.predict(\n",
    "    n=output_len,  # Forecast length (same as test horizon)\n",
    "    series=val_target,  # Most recent known target values to condition on\n",
    "    future_covariates=test_covariates,  # Covariate features for decoder input\n",
    "    num_samples=2000,  # Enables quantile forecast estimation from samples\n",
    ")\n",
    "\n",
    "# 🎯 Extract and inverse-transform specific quantile forecasts (p10 to p90)\n",
    "quantil_forecast_p02 = t_scaler.inverse_transform(\n",
    "    quantil_forecast.quantile_timeseries(0.02)\n",
    ")\n",
    "quantil_forecast_p10 = t_scaler.inverse_transform(\n",
    "    quantil_forecast.quantile_timeseries(0.1)\n",
    ")\n",
    "quantil_forecast_p25 = t_scaler.inverse_transform(\n",
    "    quantil_forecast.quantile_timeseries(0.25)\n",
    ")\n",
    "quantil_forecast_p50 = t_scaler.inverse_transform(\n",
    "    quantil_forecast.quantile_timeseries(0.5)\n",
    ")  # Median forecast\n",
    "quantil_forecast_p75 = t_scaler.inverse_transform(\n",
    "    quantil_forecast.quantile_timeseries(0.75)\n",
    ")\n",
    "quantil_forecast_p90 = t_scaler.inverse_transform(\n",
    "    quantil_forecast.quantile_timeseries(0.9)\n",
    ")\n",
    "quantil_forecast_p98 = t_scaler.inverse_transform(\n",
    "    quantil_forecast.quantile_timeseries(0.98)\n",
    ")\n",
    "\n",
    "# 📈 Predict a single-point (deterministic) forecast — defaults to median or mean\n",
    "point_forecast = model.predict(\n",
    "    n=output_len, series=val_target, future_covariates=test_covariates\n",
    ")\n",
    "point_forecast_inv = t_scaler.inverse_transform(\n",
    "    point_forecast\n",
    ")  # Undo scaling to get actual values\n",
    "\n",
    "# 🧭 Align predictions and ground truth on overlapping time index\n",
    "common_index = quantil_forecast_p50.time_index.intersection(\n",
    "    test_target_raw.time_index\n",
    ")  # test_target_raw = raw_target[-output_len:]\n",
    "\n",
    "# 📊 Extract raw numerical arrays for plotting/evaluation\n",
    "true_vals = test_target_raw[common_index].values().squeeze()  # Actual target values\n",
    "p02_vals = quantil_forecast_p02[common_index].values().squeeze()  # 2% quantile\n",
    "p10_vals = quantil_forecast_p10[common_index].values().squeeze()  # 10% quantile\n",
    "p25_vals = quantil_forecast_p25[common_index].values().squeeze()\n",
    "p50_vals = quantil_forecast_p50[common_index].values().squeeze()\n",
    "p75_vals = quantil_forecast_p75[common_index].values().squeeze()\n",
    "p90_vals = quantil_forecast_p90[common_index].values().squeeze()\n",
    "p98_vals = quantil_forecast_p98[common_index].values().squeeze()\n",
    "point_vals = (\n",
    "    point_forecast_inv[common_index].values().squeeze()\n",
    ")  # Deterministic forecast\n",
    "time_index = common_index  # Corresponding time range\n",
    "\n",
    "# 📋 Create a DataFrame to hold actual and predicted values\n",
    "forecast_df = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": time_index,\n",
    "        \"true_test\": true_vals,  # Ground truth\n",
    "        \"p02\": p02_vals,  # Lower bound forecast\n",
    "        \"p10\": p10_vals,  # Lower bound forecast\n",
    "        \"p25\": p25_vals,  # Lower bound forecast\n",
    "        \"p50\": p50_vals,  # Median prediction\n",
    "        \"p75\": p75_vals,  # Median prediction\n",
    "        \"p90\": p90_vals,  # Upper bound forecast\n",
    "        \"p98\": p98_vals,  # Upper bound forecast\n",
    "        \"point_forecast\": point_vals,  # Deterministic (point) prediction\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "forecast_df[\"FEATURE\"] = \", \".join(\n",
    "    features_list_model\n",
    ")  # A single string label repeated\n",
    "\n",
    "forecast_df[\"p_w_ave_all\"] = (\n",
    "    0.05 * forecast_df[\"p02\"]\n",
    "    + 0.10 * forecast_df[\"p10\"]\n",
    "    + 0.15 * forecast_df[\"p25\"]\n",
    "    + 0.40 * forecast_df[\"p50\"]\n",
    "    + 0.15 * forecast_df[\"p75\"]\n",
    "    + 0.10 * forecast_df[\"p90\"]\n",
    "    + 0.05 * forecast_df[\"p98\"]\n",
    ")\n",
    "\n",
    "\n",
    "# 📉 Calculate residual errors (true - predicted) for evaluation\n",
    "forecast_df[\"residual_point\"] = forecast_df[\"true_test\"] - forecast_df[\"point_forecast\"]\n",
    "forecast_df[\"residual_p50\"] = forecast_df[\"true_test\"] - forecast_df[\"p50\"]\n",
    "forecast_df[\"residual_p75\"] = forecast_df[\"true_test\"] - forecast_df[\"p75\"]\n",
    "forecast_df[\"residual_p25\"] = forecast_df[\"true_test\"] - forecast_df[\"p25\"]\n",
    "forecast_df[\"residual_ave\"] = forecast_df[\"true_test\"] - forecast_df[\"p_w_ave_all\"]\n",
    "forecast_df[\"quantile_width_80\"] = forecast_df[\"p90\"] - forecast_df[\"p10\"]\n",
    "forecast_df[\"quantile_width_75\"] = forecast_df[\"p75\"] - forecast_df[\"p25\"]\n",
    "forecast_df[\"quantile_width_96\"] = forecast_df[\"p98\"] - forecast_df[\"p02\"]\n",
    "forecast_df[\"z_residual_point\"] = (\n",
    "    forecast_df[\"residual_point\"] / forecast_df[\"quantile_width_80\"]\n",
    ")\n",
    "forecast_df[\"z_residual_ave\"] = (\n",
    "    forecast_df[\"residual_ave\"] / forecast_df[\"quantile_width_80\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n[INFO] Forecast vs True Table:\")\n",
    "print(tabulate(forecast_df.head(15), headers=\"keys\", tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_forecast_with_intervals(forecast_df, title=\"TFT Forecast vs Actual\"):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot true values\n",
    "    plt.plot(\n",
    "        forecast_df.index,\n",
    "        forecast_df[\"true_test\"],\n",
    "        label=\"True Price\",\n",
    "        color=\"black\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    # Plot median prediction\n",
    "    plt.plot(\n",
    "        forecast_df.index,\n",
    "        forecast_df[\"p50\"],\n",
    "        label=\"Predicted Median (p50)\",\n",
    "        linestyle=\"--\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "\n",
    "    # Plot median prediction\n",
    "    # 🔴 Plot weighted average of all quantiles\n",
    "    plt.plot(\n",
    "        forecast_df.index,\n",
    "        forecast_df[\"p_w_ave_all\"],\n",
    "        label=\"Predicted Weighted Average\",\n",
    "        linestyle=\"-.\",  # ← corrected line style\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    # Plot deterministic point forecast\n",
    "    plt.plot(\n",
    "        forecast_df.index,\n",
    "        forecast_df[\"point_forecast\"],\n",
    "        label=\"Point Forecast\",\n",
    "        linestyle=\":\",\n",
    "        color=\"green\",\n",
    "    )\n",
    "\n",
    "    # Shaded quantile prediction interval (80%)\n",
    "    plt.fill_between(\n",
    "        forecast_df.index,\n",
    "        forecast_df[\"p10\"],\n",
    "        forecast_df[\"p90\"],\n",
    "        color=\"lightblue\",\n",
    "        alpha=0.4,\n",
    "        label=\"80% Prediction Interval (p10–p90)\",\n",
    "    )\n",
    "\n",
    "    # Shaded extreme interval (96%)\n",
    "    plt.fill_between(\n",
    "        forecast_df.index,\n",
    "        forecast_df[\"p02\"],\n",
    "        forecast_df[\"p98\"],\n",
    "        color=\"orange\",\n",
    "        alpha=0.2,\n",
    "        label=\"96% Prediction Interval (p02–p98)\",\n",
    "    )\n",
    "\n",
    "    # Title and labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_forecast_with_intervals(\n",
    "    forecast_df,\n",
    "    title=f\"AAPL TFT Forecast vs True Price -\\n Feature:{features_list_model}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_z_residuals(forecast_df, title=\"Residual Z-Score vs Time\"):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "\n",
    "    z = forecast_df[\"z_residual_point\"]\n",
    "    dates = forecast_df.index\n",
    "\n",
    "    # Color bars based on severity\n",
    "    colors = [\n",
    "        \"red\" if abs(val) > 1.0 else \"orange\" if abs(val) > 0.5 else \"green\"\n",
    "        for val in z\n",
    "    ]\n",
    "\n",
    "    plt.bar(dates, z, color=colors, alpha=0.7)\n",
    "    plt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    plt.axhline(1.0, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "    plt.axhline(-1.0, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "    plt.axhline(0.5, color=\"orange\", linestyle=\"--\", linewidth=1)\n",
    "    plt.axhline(-0.5, color=\"orange\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Z-Score of Residual (True - Forecast) / Quantile Width\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_z_residuals(\n",
    "    forecast_df, title=f\"Z-Score of Forecast Residuals \\n Feature:{features_list_model}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Point Forecast Residual over time   -  Histogram of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# 🔮 Step 9: Point Forecast Residual over time   -  Histogram of residuals\n",
    "# -------------------------------------------------------------------------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Residual over time\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(\n",
    "    forecast_df.index,\n",
    "    forecast_df[\"residual_point\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Point Forecast Residual\",\n",
    ")\n",
    "plt.axhline(0, linestyle=\"--\", color=\"gray\")\n",
    "plt.title(f\"Residuals Over Time \\n Feature:{features_list_model}\")\n",
    "plt.ylabel(\"True - Forecast\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(forecast_df[\"residual_point\"], bins=10, kde=True, color=\"steelblue\")\n",
    "plt.axvline(0, linestyle=\"--\", color=\"black\")\n",
    "plt.title(f\"Distribution of Residuals \\n Feature:{features_list_model}\")\n",
    "plt.xlabel(\"Residual (True - Point Forecast)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# 🔮 Step 9: Average  Forecast Residual over time   -  Histogram of residuals\n",
    "# -------------------------------------------------------------------------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Residual over time\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(\n",
    "    forecast_df.index,\n",
    "    forecast_df[\"residual_ave\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Average Forecast Residual\",\n",
    ")\n",
    "plt.axhline(0, linestyle=\"--\", color=\"gray\")\n",
    "plt.title(f\"Residuals Over Time \\n Feature:{features_list_model}\")\n",
    "plt.ylabel(\"True - Average Forecast\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(forecast_df[\"residual_ave\"], bins=10, kde=True, color=\"steelblue\")\n",
    "plt.axvline(0, linestyle=\"--\", color=\"black\")\n",
    "plt.title(f\"Distribution of Residuals \\n Feature:{features_list_model}\")\n",
    "plt.xlabel(\"Residual (True - Average Forecast)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 : Complete Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# 🔮 Step 10 : Complete Evaluation Metrics\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def full_metrics_np(true, pred, label):\n",
    "    true = np.asarray(true)\n",
    "    pred = np.asarray(pred)\n",
    "\n",
    "    mask = (true > 0) & (pred > 0)\n",
    "    true_pos = true[mask]\n",
    "    pred_pos = pred[mask]\n",
    "\n",
    "    return {\n",
    "        \"Forecast\": label,\n",
    "        \"MAE\": round(mean_absolute_error(true, pred), 4),\n",
    "        \"RMSE\": round(np.sqrt(mean_squared_error(true, pred)), 4),\n",
    "        \"MSE\": round(mean_squared_error(true, pred), 4),\n",
    "        \"R²\": round(r2_score(true, pred), 4),\n",
    "        \"MAPE\": round(np.mean(np.abs((true - pred) / true)) * 100, 4),\n",
    "        \"SMAPE\": round(\n",
    "            np.mean(2 * np.abs(true - pred) / (np.abs(true) + np.abs(pred))) * 100, 4\n",
    "        ),\n",
    "        \"log-MAPE\": (\n",
    "            round(np.mean(np.abs(np.log(true_pos / pred_pos))) * 100, 4)\n",
    "            if len(true_pos) > 0\n",
    "            else None\n",
    "        ),\n",
    "        \"RMSLE\": (\n",
    "            round(\n",
    "                np.sqrt(np.mean(np.square(np.log1p(pred_pos) - np.log1p(true_pos)))), 4\n",
    "            )\n",
    "            if len(true_pos) > 0\n",
    "            else None\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "metrics_point = full_metrics_np(\n",
    "    forecast_df[\"true_test\"], forecast_df[\"point_forecast\"], \"Point Forecast\"\n",
    ")\n",
    "metrics_p50 = full_metrics_np(\n",
    "    forecast_df[\"true_test\"], forecast_df[\"p50\"], \"Quantile Forecast P50\"\n",
    ")\n",
    "metrics_p90 = full_metrics_np(\n",
    "    forecast_df[\"true_test\"], forecast_df[\"p90\"], \"Quantile Forecast P90\"\n",
    ")\n",
    "metrics_p10 = full_metrics_np(\n",
    "    forecast_df[\"true_test\"], forecast_df[\"p10\"], \"Quantile Forecast P10\"\n",
    ")\n",
    "metrics_p_ave = full_metrics_np(\n",
    "    forecast_df[\"true_test\"], forecast_df[\"p_w_ave_all\"], \"Quantile Forecast Ave\"\n",
    ")\n",
    "\n",
    "\n",
    "final_metrics_df = pd.DataFrame(\n",
    "    [metrics_point, metrics_p50, metrics_p90, metrics_p10, metrics_p_ave]\n",
    ")\n",
    "print(\"\\n[INFO] Complete Evaluation Metrics:\")\n",
    "final_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Plot Forecast vs Actual (Quantiles + True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# 🔮Step 11:  Plot Forecast vs Actual (Quantiles + True)\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Timestamp\n",
    "\n",
    "# 🔪 Slice future portion of the target to zoom on recent data\n",
    "raw_target_time_filter = raw_target.slice(\n",
    "    Timestamp(\"2025-01-01\"), raw_target.end_time()\n",
    ")\n",
    "print(\n",
    "    f\"[INFO] Sliced range: {raw_target_time_filter.start_time()} → {raw_target_time_filter.end_time()}\"\n",
    ")\n",
    "\n",
    "# 🎯 Split time (forecast starts here)\n",
    "split_time = quantil_forecast_p50.start_time()\n",
    "\n",
    "# 🎨 Extract values for fill_between\n",
    "time_index = quantil_forecast_p50.time_index\n",
    "p10_vals = quantil_forecast_p10.values().squeeze()\n",
    "p50_vals = quantil_forecast_p50.values().squeeze()\n",
    "p90_vals = quantil_forecast_p90.values().squeeze()\n",
    "point_vals = point_forecast_inv.values().squeeze()\n",
    "true_vals = test_target_raw.values().squeeze()\n",
    "\n",
    "# 🗓️ Align time range for test set\n",
    "test_time_index = test_target_raw.time_index\n",
    "\n",
    "# 📊 Begin Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot past (train/val) actuals\n",
    "raw_target_time_filter.plot(label=\"Train/Val True\", linewidth=2, color=\"black\")\n",
    "\n",
    "# Plot true test set\n",
    "plt.plot(test_time_index, true_vals, label=\"Test True\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Plot quantile forecasts\n",
    "plt.plot(\n",
    "    time_index, p50_vals, label=\"Forecast Median (p50)\", color=\"magenta\", linewidth=2\n",
    ")\n",
    "plt.plot(\n",
    "    time_index, p10_vals, label=\"Forecast Lower (p10)\", linestyle=\"--\", color=\"skyblue\"\n",
    ")\n",
    "plt.plot(\n",
    "    time_index, p90_vals, label=\"Forecast Upper (p90)\", linestyle=\"--\", color=\"green\"\n",
    ")\n",
    "\n",
    "# Plot deterministic point forecast\n",
    "plt.plot(\n",
    "    time_index, point_vals, label=\"Forecast Point (mean)\", linestyle=\":\", color=\"orange\"\n",
    ")\n",
    "\n",
    "# Add confidence interval shading\n",
    "plt.fill_between(\n",
    "    time_index,\n",
    "    p10_vals,\n",
    "    p90_vals,\n",
    "    color=\"lightgray\",\n",
    "    alpha=0.4,\n",
    "    label=\"Confidence Interval (p10–p90)\",\n",
    ")\n",
    "\n",
    "# Add vertical forecast start marker\n",
    "plt.axvline(\n",
    "    split_time, color=\"gray\", linestyle=\"--\", linewidth=2, label=\"Forecast Start\"\n",
    ")\n",
    "\n",
    "# Style plot\n",
    "plt.title(f\"TFT Forecast vs. True - Full Signal ({target_stock})\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧪 Step 12: Quantile Evaluation Metrics and Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# 🧪 Step 12: Quantile Forecast Evaluation - Interpretability and Visualization\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n[INFO] Step 12-1: Evaluate Quantile Forecast Performance\")\n",
    "\n",
    "# 🎯 1. Extract true and predicted values for the test set\n",
    "true_vals = test_target_raw.values().squeeze()  # Actual values\n",
    "p10_vals = quantil_forecast_p10.values().squeeze()  # 10% quantile forecast\n",
    "p50_vals = quantil_forecast_p50.values().squeeze()  # Median (50%) forecast\n",
    "p90_vals = quantil_forecast_p90.values().squeeze()  # 90% quantile forecast\n",
    "\n",
    "# ✅ 2. Interval Coverage Rate (ICR): % of actuals falling in [p10, p90]\n",
    "coverage = ((true_vals >= p10_vals) & (true_vals <= p90_vals)).mean()\n",
    "print(\n",
    "    f\"[INFO] Interval Coverage (p10–p90): {coverage:.3f} → Ideal: ~0.80 (well-calibrated)\"\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ 3. Pinball Loss Function: measures quantile regression accuracy\n",
    "def pinball_loss(y_true, y_pred, q):\n",
    "    delta = y_true - y_pred\n",
    "    return np.mean(np.maximum(q * delta, (q - 1) * delta))\n",
    "\n",
    "\n",
    "pinball_losses = {\n",
    "    \"p10\": pinball_loss(true_vals, p10_vals, 0.1),\n",
    "    \"p50\": pinball_loss(true_vals, p50_vals, 0.5),\n",
    "    \"p90\": pinball_loss(true_vals, p90_vals, 0.9),\n",
    "}\n",
    "print(\"[INFO] Pinball Losses (lower is better):\", pinball_losses)\n",
    "\n",
    "# ✅ 4. Average Interval Width (Uncertainty Range)\n",
    "interval_widths = p90_vals - p10_vals\n",
    "mean_interval_width = interval_widths.mean()\n",
    "print(\n",
    "    f\"[INFO] Mean Interval Width (p90 - p10): {mean_interval_width:.3f} → Lower is sharper, but not too narrow\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# 📊 VISUALIZE Forecast Uncertainty Metrics\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 🔷 (Top Left) Interval Coverage Bar\n",
    "axs[0, 0].bar([\"p10–p90\"], [coverage], color=\"lightblue\")\n",
    "axs[0, 0].axhline(0.80, color=\"gray\", linestyle=\"--\", label=\"Ideal 80%\")\n",
    "axs[0, 0].set_title(\"Interval Coverage\")\n",
    "axs[0, 0].set_ylim(0, 1)\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# 🔷 (Top Right) Pinball Loss for Different Quantiles\n",
    "axs[0, 1].bar(pinball_losses.keys(), pinball_losses.values(), color=\"salmon\")\n",
    "axs[0, 1].set_title(\"Pinball Loss (Quantile Loss)\")\n",
    "axs[0, 1].set_ylabel(\"Loss\")\n",
    "axs[0, 1].set_xlabel(\"Quantile\")\n",
    "\n",
    "# 🔷 (Bottom Left) Interval Width Over Time\n",
    "axs[1, 0].plot(quantil_forecast_p90.time_index, interval_widths, color=\"purple\")\n",
    "axs[1, 0].set_title(\"Forecast Interval Width Over Time\")\n",
    "axs[1, 0].set_xlabel(\"Date\")\n",
    "axs[1, 0].set_ylabel(\"Width (p90 - p10)\")\n",
    "axs[1, 0].grid(True)\n",
    "\n",
    "# 🔷 (Bottom Right) Quantile Calibration Curve\n",
    "quantiles = np.linspace(0.05, 0.95, 19)\n",
    "empirical_coverages = [\n",
    "    (true_vals <= quantil_forecast.quantile_timeseries(q).values().squeeze()).mean()\n",
    "    for q in quantiles\n",
    "]\n",
    "axs[1, 1].plot(quantiles, empirical_coverages, marker=\"o\", label=\"Empirical Coverage\")\n",
    "axs[1, 1].plot(\n",
    "    quantiles, quantiles, linestyle=\"--\", color=\"gray\", label=\"Ideal Calibration\"\n",
    ")\n",
    "axs[1, 1].set_title(\"Quantile Calibration Curve\")\n",
    "axs[1, 1].set_xlabel(\"Nominal Quantile\")\n",
    "axs[1, 1].set_ylabel(\"Empirical Coverage\")\n",
    "axs[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# 🧪 Step 12-2: Residual Analysis - Median (p50) Forecast\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[INFO] Step 12-2: Residual Analysis for p50 Forecast\")\n",
    "\n",
    "# ⚠️ Residual = Actual - Forecast (p50)\n",
    "residuals = true_vals - p50_vals\n",
    "\n",
    "# 🔍 Time Series of Residuals\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(test_target_raw.time_index, residuals, color=\"crimson\")\n",
    "plt.axhline(0, linestyle=\"--\", color=\"gray\", label=\"Zero Line\")\n",
    "plt.title(\"Residuals Over Time (True - Median Forecast)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Residual Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 📈 Residual Distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(residuals, kde=True, color=\"skyblue\", bins=30)\n",
    "plt.axvline(0, linestyle=\"--\", color=\"black\")\n",
    "plt.title(\"Residual Distribution (Should Center Around Zero)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# 🧪 Step 12: Quantile Evaluation Metrics\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[INFO] Step 12-1: Evaluate Quantile Forecast Performance\")\n",
    "\n",
    "# Extract true values and predicted quantiles\n",
    "true_vals = test_target_raw.values().squeeze()\n",
    "p10_vals = quantil_forecast_p10.values().squeeze()\n",
    "p50_vals = quantil_forecast_p50.values().squeeze()\n",
    "p90_vals = quantil_forecast_p90.values().squeeze()\n",
    "\n",
    "# ✅ Interval Coverage: proportion of true values falling within [p10, p90]\n",
    "coverage = ((true_vals >= p10_vals) & (true_vals <= p90_vals)).mean()\n",
    "print(f\"[INFO] Interval Coverage (p10–p90): {coverage:.3f}\")\n",
    "\n",
    "\n",
    "# ✅ Pinball Loss (quantile loss function)\n",
    "def pinball_loss(y_true, y_pred, q):\n",
    "    delta = y_true - y_pred\n",
    "    return np.mean(np.maximum(q * delta, (q - 1) * delta))\n",
    "\n",
    "\n",
    "pinball_losses = {\n",
    "    \"p10\": pinball_loss(true_vals, p10_vals, 0.1),\n",
    "    \"p50\": pinball_loss(true_vals, p50_vals, 0.5),\n",
    "    \"p90\": pinball_loss(true_vals, p90_vals, 0.9),\n",
    "}\n",
    "print(\"[INFO] Pinball Losses:\", pinball_losses)\n",
    "\n",
    "# ✅ Interval width (p90 - p10) analysis\n",
    "interval_widths = p90_vals - p10_vals\n",
    "mean_interval_width = interval_widths.mean()\n",
    "print(f\"[INFO] Mean Interval Width (p90 - p10): {mean_interval_width:.3f}\")\n",
    "\n",
    "# 📊 Plot evaluation diagnostics\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Coverage bar\n",
    "axs[0, 0].bar([\"p10–p90\"], [coverage], color=\"lightblue\")\n",
    "axs[0, 0].axhline(0.8, color=\"gray\", linestyle=\"--\")\n",
    "axs[0, 0].set_title(\"Interval Coverage\")\n",
    "axs[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Pinball loss bar\n",
    "axs[0, 1].bar(pinball_losses.keys(), pinball_losses.values(), color=\"salmon\")\n",
    "axs[0, 1].set_title(\"Pinball Loss\")\n",
    "\n",
    "# Interval width over time\n",
    "axs[1, 0].plot(quantil_forecast_p90.time_index, interval_widths, color=\"purple\")\n",
    "axs[1, 0].set_title(\"Interval Width Over Time\")\n",
    "axs[1, 0].set_xlabel(\"Date\")\n",
    "\n",
    "# Quantile calibration curve\n",
    "quantiles = np.linspace(0.05, 0.95, 19)\n",
    "empirical_coverages = [\n",
    "    (true_vals <= quantil_forecast.quantile_timeseries(q).values().squeeze()).mean()\n",
    "    for q in quantiles\n",
    "]\n",
    "axs[1, 1].plot(quantiles, empirical_coverages, marker=\"o\", label=\"Empirical\")\n",
    "axs[1, 1].plot(quantiles, quantiles, linestyle=\"--\", color=\"gray\", label=\"Ideal\")\n",
    "axs[1, 1].set_title(\"Quantile Calibration\")\n",
    "axs[1, 1].set_xlabel(\"Quantile\")\n",
    "axs[1, 1].set_ylabel(\"Empirical Coverage\")\n",
    "axs[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# 🧪 Step 12-2: Residual Analysis\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[INFO] Step 12-2: Residual Analysis for p50 Forecast\")\n",
    "\n",
    "# Residuals = actual - predicted\n",
    "residuals = true_vals - p50_vals\n",
    "\n",
    "# Time-indexed plot of residuals\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(test_target_raw.time_index, residuals, color=\"crimson\")\n",
    "plt.axhline(0, linestyle=\"--\", color=\"gray\")\n",
    "plt.title(\"Test Residuals Over Time (p50 Forecast)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of residuals\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(residuals, kde=True, color=\"skyblue\")\n",
    "plt.axvline(0, linestyle=\"--\", color=\"black\")\n",
    "plt.title(\"Residual Distribution (p50 Forecast)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 📐 Winkler Score Explanation\n",
    "# -----------------------------\n",
    "# - Used to evaluate prediction intervals (like p10–p90).\n",
    "# - Penalizes intervals that are too wide or miss the true value.\n",
    "# - If the true value is inside the interval → score = interval width\n",
    "# - If outside → heavy penalty added\n",
    "# - ⚠️ Lower is better.\n",
    "# - 💡 Good range: [10–30] depending on the context. <20 is typically strong.\n",
    "\n",
    "def winkler_score(y_true, y_lower, y_upper, alpha=0.8):\n",
    "    scores = []\n",
    "    for yt, l, u in zip(y_true, y_lower, y_upper):\n",
    "        width = u - l\n",
    "        if yt < l:\n",
    "            score = width + (2 / alpha) * (l - yt)\n",
    "        elif yt > u:\n",
    "            score = width + (2 / alpha) * (yt - u)\n",
    "        else:\n",
    "            score = width\n",
    "        scores.append(score)\n",
    "    return np.array(scores)\n",
    "\n",
    "# ------------------------------------\n",
    "# 📐 Continuous Ranked Probability Score (CRPS)\n",
    "# ------------------------------------\n",
    "# - Measures quality of the **entire predicted distribution** vs. true value\n",
    "# - Lower CRPS means better calibrated forecast\n",
    "# - Similar to MAE, but for probability distributions\n",
    "# - 💡 Good range: typically <1.0, excellent <0.5\n",
    "\n",
    "def crps_from_quantiles(y_true, quantiles, quantile_preds):\n",
    "    y_true = np.asarray(y_true)\n",
    "    crps_vals = np.zeros_like(y_true)\n",
    "\n",
    "    for q in quantiles:\n",
    "        preds = np.asarray(quantile_preds[q])\n",
    "        crps_vals += (y_true < preds) * (1 - q) * (preds - y_true) + \\\n",
    "                     (y_true >= preds) * q * (y_true - preds)\n",
    "\n",
    "    return crps_vals / len(quantiles)\n",
    "\n",
    "# --------------------------------------\n",
    "# ✅ Apply metrics to forecast DataFrame\n",
    "# --------------------------------------\n",
    "\n",
    "# ⚠️ Ensure forecast_df already exists and contains:\n",
    "# 'true_test', 'p10', 'p25', 'p50', 'p75', 'p90', 'p98'\n",
    "\n",
    "# Calculate Winkler Score for 80% prediction interval (p10–p90)\n",
    "winkler_scores = winkler_score(\n",
    "    y_true=forecast_df[\"true_test\"].values,\n",
    "    y_lower=forecast_df[\"p10\"].values,\n",
    "    y_upper=forecast_df[\"p90\"].values,\n",
    "    alpha=0.8  # confidence level\n",
    ")\n",
    "forecast_df[\"winkler_score_p10_p90\"] = winkler_scores\n",
    "mean_winkler = np.mean(winkler_scores)\n",
    "print(f\"✅ Mean Winkler Score (p10–p90): {mean_winkler:.2f} — lower is better (ideal < 20–30)\")\n",
    "\n",
    "# Calculate CRPS using selected quantiles\n",
    "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "quantile_preds = {\n",
    "    0.1: forecast_df[\"p10\"].values,\n",
    "    0.25: forecast_df[\"p25\"].values,\n",
    "    0.5: forecast_df[\"p50\"].values,\n",
    "    0.75: forecast_df[\"p75\"].values,\n",
    "    0.9: forecast_df[\"p90\"].values,\n",
    "}\n",
    "crps_scores = crps_from_quantiles(\n",
    "    y_true=forecast_df[\"true_test\"].values,\n",
    "    quantiles=quantiles,\n",
    "    quantile_preds=quantile_preds\n",
    ")\n",
    "forecast_df[\"crps\"] = crps_scores\n",
    "mean_crps = np.mean(crps_scores)\n",
    "print(f\"✅ Mean CRPS: {mean_crps:.4f} — lower is better (ideal < 1, excellent < 0.5)\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 📊 Plot Distributions + Interpretation\n",
    "# ----------------------------------------\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 📉 Winkler Score Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(winkler_scores, bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.axvline(mean_winkler, color=\"red\", linestyle=\"--\", label=f\"Mean: {mean_winkler:.2f}\")\n",
    "plt.axvline(20, color=\"green\", linestyle=\":\", label=\"Target Threshold (~20)\")\n",
    "plt.title(\"📊 Winkler Score Distribution\")\n",
    "plt.xlabel(\"Winkler Score (Interval Accuracy)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 📉 CRPS Score Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(crps_scores, bins=30, color=\"orange\", edgecolor=\"black\")\n",
    "plt.axvline(mean_crps, color=\"red\", linestyle=\"--\", label=f\"Mean: {mean_crps:.3f}\")\n",
    "plt.axvline(1.0, color=\"green\", linestyle=\":\", label=\"Target Threshold (1.0)\")\n",
    "plt.axvline(0.5, color=\"blue\", linestyle=\"--\", label=\"Excellent (0.5)\")\n",
    "plt.title(\"📊 CRPS Distribution\")\n",
    "plt.xlabel(\"CRPS (Forecast Distribution Quality)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔁 Rolling Forecasting\n",
    "\n",
    "Rolling forecasting simulates how a model would perform in **real-world conditions**, where predictions are made repeatedly over time as new data becomes available.\n",
    "\n",
    "Instead of forecasting just once, the model:\n",
    "- Slides a window forward in time.\n",
    "- Makes **multiple forecasts**, each based on the most recent past data.\n",
    "- Aggregates results to assess performance across different time periods.\n",
    "\n",
    "This method provides:\n",
    "- ✅ Point forecasts and quantile intervals (e.g., p10, p50, p90) for each rolling window.\n",
    "- ✅ A better understanding of **model stability**, **temporal performance drift**, and **resilience**.\n",
    "\n",
    "> 📌 **Use this when**: You want to evaluate model robustness over time or simulate live forecasting scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 13: Rolling Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# Step 13: Rolling Forecasting\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# --- 📦 Imports ---\n",
    "from darts import (\n",
    "    concatenate,\n",
    ")  # Used to concatenate multiple Darts TimeSeries objects into one\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "import pandas as pd  # For general data manipulation\n",
    "import numpy as np  # For numerical operations\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 🎯 GOAL: Simulate real-time forecasting by predicting future values\n",
    "# across multiple time windows using a trained probabilistic model.\n",
    "# It collects forecast quantiles (p10, p50, p90) and point estimates,\n",
    "# compares them with actual values, and prepares them for analysis.\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 🔧 STEP 1: Initialize empty lists to store rolling forecasts\n",
    "# -----------------------------------------------------------------------------------\n",
    "rolling_p10, rolling_p50, rolling_p90 = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")  # To store lower, median, and upper quantile forecasts\n",
    "rolling_point, rolling_truths = (\n",
    "    [],\n",
    "    [],\n",
    ")  # To store point forecasts and actual (true) values\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 📅 STEP 2: Set up rolling forecast time range\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# Get the timestamp where the test dataset ends\n",
    "forecast_end = test_target_raw.end_time()\n",
    "\n",
    "# Define when the rolling forecast should start — 700 days before the test ends\n",
    "# This provides enough history for multiple input/output cycles\n",
    "rolling_start_time = forecast_end - pd.Timedelta(days=2500)\n",
    "\n",
    "# Find the closest index in the full target series for the calculated start time\n",
    "# (used to know where to begin rolling from)\n",
    "rolling_start_idx = raw_target.time_index.get_indexer(\n",
    "    [rolling_start_time], method=\"nearest\"\n",
    ")[0]\n",
    "\n",
    "# Determine the last possible index from which an output_len-length forecast can be made\n",
    "rolling_end_idx = len(raw_target) - output_len\n",
    "\n",
    "# Generate all the start indices for the rolling windows\n",
    "# From the valid start point to the last valid forecast point\n",
    "# Advance by output_len so forecasts don’t overlap\n",
    "window_starts = list(\n",
    "    range(\n",
    "        rolling_start_idx + input_len,  # Ensure we have enough past data for input\n",
    "        rolling_end_idx + 1,  # Include the last valid point\n",
    "        output_len,  # Move window by the forecast horizon length\n",
    "    )\n",
    ")\n",
    "\n",
    "# If the very last test window wasn't included above, append it manually\n",
    "if (len(raw_target) - output_len) not in window_starts:\n",
    "    window_starts.append(len(raw_target) - output_len)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 🔁 STEP 3: Rolling forecast loop — iterate over each rolling window\n",
    "# -----------------------------------------------------------------------------------\n",
    "for i in window_starts:\n",
    "    # Slice the input series of length `input_len` ending at index i\n",
    "    input_series = raw_target[i - input_len : i]\n",
    "\n",
    "    # Get future covariates from past up to the forecast horizon\n",
    "    input_cov = raw_covariates[i - input_len : i + output_len]\n",
    "\n",
    "    # Get the actual (true) values of the target for the next output_len steps\n",
    "    actual = raw_target[i : i + output_len]\n",
    "\n",
    "    # Scale the input time series using the pre-fitted target scaler\n",
    "    scaled_input_series = t_scaler.transform(input_series)\n",
    "\n",
    "    # Scale the covariates using the pre-fitted covariate scaler\n",
    "    scaled_input_cov = f_scaler.transform(input_cov)\n",
    "\n",
    "    # Use the model to make a probabilistic forecast with sampling\n",
    "    # This gives a distribution of possible outcomes\n",
    "    forecast_prob = model.predict(\n",
    "        n=output_len,\n",
    "        series=scaled_input_series,\n",
    "        future_covariates=scaled_input_cov,\n",
    "        num_samples=100,  # Use 100 samples to estimate quantiles\n",
    "    )\n",
    "\n",
    "    # Use the same model to produce a deterministic point forecast\n",
    "    # Typically the mean or median of the distribution\n",
    "    forecast_point = model.predict(\n",
    "        n=output_len, series=scaled_input_series, future_covariates=scaled_input_cov\n",
    "    )\n",
    "\n",
    "    # Convert the 10th, 50th, and 90th percentiles from scaled back to real values\n",
    "    p10 = t_scaler.inverse_transform(\n",
    "        forecast_prob.quantile_timeseries(0.1)\n",
    "    )  # Lower bound\n",
    "    p50 = t_scaler.inverse_transform(\n",
    "        forecast_prob.quantile_timeseries(0.5)\n",
    "    )  # Median forecast\n",
    "    p90 = t_scaler.inverse_transform(\n",
    "        forecast_prob.quantile_timeseries(0.9)\n",
    "    )  # Upper bound\n",
    "\n",
    "    # Convert the point forecast back to original scale\n",
    "    point = t_scaler.inverse_transform(forecast_point)\n",
    "\n",
    "    # Store true and predicted values for this window\n",
    "    true = raw_target[i : i + output_len]\n",
    "    rolling_p10.append(p10)\n",
    "    rolling_p50.append(p50)\n",
    "    rolling_p90.append(p90)\n",
    "    rolling_point.append(point)\n",
    "    rolling_truths.append(true)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 📦 STEP 4: Concatenate all rolling predictions into full-length series\n",
    "# This allows us to compare against the full ground truth range\n",
    "# -----------------------------------------------------------------------------------\n",
    "full_p10 = concatenate(\n",
    "    rolling_p10, ignore_time_axis=True\n",
    ")  # All 10th percentile forecasts\n",
    "full_p50 = concatenate(\n",
    "    rolling_p50, ignore_time_axis=True\n",
    ")  # All 50th percentile forecasts (median)\n",
    "full_p90 = concatenate(\n",
    "    rolling_p90, ignore_time_axis=True\n",
    ")  # All 90th percentile forecasts\n",
    "full_point = concatenate(rolling_point, ignore_time_axis=True)  # All point forecasts\n",
    "full_true = concatenate(rolling_truths, ignore_time_axis=True)  # All true target values\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 📄 STEP 5: Build a DataFrame for analysis/plotting\n",
    "# Includes predictions and actuals aligned by timestamp\n",
    "# -----------------------------------------------------------------------------------\n",
    "df_all = pd.DataFrame(\n",
    "    {\n",
    "        \"time\": full_true.time_index,  # Forecasted time points\n",
    "        \"true\": full_true.values().squeeze(),  # Ground truth values\n",
    "        \"p10\": full_p10.values().squeeze(),  # 10th percentile forecast\n",
    "        \"p50\": full_p50.values().squeeze(),  # Median forecast\n",
    "        \"p90\": full_p90.values().squeeze(),  # 90th percentile forecast\n",
    "        \"point_pred\": full_point.values().squeeze(),  # Deterministic (point) forecast\n",
    "    }\n",
    ").set_index(\n",
    "    \"time\"\n",
    ")  # Set time as index for analysis\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 📉 STEP 6: Smooth the predictions for visualization\n",
    "# A 3-point rolling average is used to reduce short-term noise\n",
    "# -----------------------------------------------------------------------------------\n",
    "smooth_df = df_all.rolling(window=3, min_periods=1).mean()  # Apply simple smoothing\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 📤 STEP 7: Print the forecasted results during test period\n",
    "# This helps verify model output visually or manually\n",
    "# -----------------------------------------------------------------------------------\n",
    "print(df_all.loc[test_target_raw.start_time() : test_target_raw.end_time()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 14: 📊 Visualize Forecast with Confidence Intervals and Test Period Highlight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# Step 14: 📊 Visualize Forecast with Confidence Intervals and Test Period Highlight\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.dates as mdates  # Optional: for handling date axis formatting\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 🧭 STEP 1: Define important time boundaries\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "split_time = (\n",
    "    test_target_raw.start_time()\n",
    ")  # Start of the test period (used for shading/splitting)\n",
    "end_time = test_target_raw.end_time()  # End of the test period (last point predicted)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 🖼️ STEP 2: Create forecast plot with intervals and test region\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(16, 6))  # Set figure size for better readability\n",
    "\n",
    "# Plot the actual target values (true series) in bold black\n",
    "plt.plot(smooth_df.index, smooth_df[\"true\"], label=\"True\", color=\"black\", linewidth=2)\n",
    "\n",
    "# Plot the point forecast (deterministic prediction) as dashed green line\n",
    "plt.plot(\n",
    "    smooth_df.index,\n",
    "    smooth_df[\"point_pred\"],\n",
    "    label=\"Point Prediction\",\n",
    "    linestyle=\"--\",\n",
    "    color=\"green\",\n",
    ")\n",
    "\n",
    "# Plot the median forecast (p50) as dotted blue line\n",
    "plt.plot(\n",
    "    smooth_df.index,\n",
    "    smooth_df[\"p50\"],\n",
    "    label=\"Median Forecast (p50)\",\n",
    "    linestyle=\":\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 🎨 STEP 3: Add shaded confidence interval between p10 and p90\n",
    "# -----------------------------------------------------------------------------------\n",
    "plt.fill_between(\n",
    "    smooth_df.index,\n",
    "    smooth_df[\"p10\"],  # Lower bound (10th percentile)\n",
    "    smooth_df[\"p90\"],  # Upper bound (90th percentile)\n",
    "    color=\"skyblue\",  # Light blue fill for visibility\n",
    "    alpha=0.3,  # Transparency\n",
    "    label=\"Confidence Interval (p10–p90)\",  # Legend label\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 📏 STEP 4: Add visual cues for test period\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# Add a vertical dashed line where test set starts\n",
    "plt.axvline(\n",
    "    x=split_time, color=\"gray\", linestyle=\"--\", linewidth=2, label=\"Train/Test Split\"\n",
    ")\n",
    "\n",
    "# Highlight the entire test period with a shaded background\n",
    "plt.axvspan(split_time, end_time, color=\"lightgray\", alpha=0.3, label=\"Test Period\")\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 🧼 STEP 5: Final formatting and display\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "plt.title(\"✅ Final Forecast with Confidence & Shaded Test Region\")  # Plot title\n",
    "plt.xlabel(\"Time\")  # X-axis label\n",
    "plt.ylabel(\"Price\")  # Y-axis label\n",
    "\n",
    "plt.legend()  # Show legend with labels for each line/area\n",
    "plt.grid(True)  # Enable grid for better readability\n",
    "plt.tight_layout()  # Automatically adjust spacing\n",
    "plt.show()  # Render the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
