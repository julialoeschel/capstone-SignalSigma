{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. 📦 IMPORT REQUIRED LIBRARIES\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, accuracy_score,\n",
    "    f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score, roc_auc_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. 📥 LOAD AND PREPROCESS STOCK DATA\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Data Loading ---\n",
    "path_stock = \"../data/stock\"  # Path to the stock data file\n",
    "df = pd.read_csv(f\"{path_stock}/AAPL_stock.csv\")  # Load the stock data into a DataFrame\n",
    "\n",
    "# --- 2. Data Cleaning ---\n",
    "df.dropna(inplace=True)  # Remove rows with missing values \n",
    "df.rename(columns={  # Rename columns for consistency\n",
    "    \"Price\": \"date\",\n",
    "    \"Close\": \"close\",\n",
    "    \"High\": \"high\",\n",
    "    \"Low\": \"low\",\n",
    "    \"Open\": \"open\",\n",
    "    \"Volume\": \"volume\"\n",
    "}, inplace=True)\n",
    "df['date'] = pd.to_datetime(df['date'])  # Convert 'date' column to datetime objects\n",
    "\n",
    "# --- 3. Data Filtering ---\n",
    "date_filter_low = '2020-01-01'  # Define the start date for filtering\n",
    "date_filter_high = '2025-01-01'  # Define the start date for filtering\n",
    "df = df[df['date'] >= date_filter_low]  # Filter data after the specified date\n",
    "df = df[df['date'] <= date_filter_high]  # Filter data after the specified date\n",
    "df.drop('Ticker', axis=1, inplace=True)  # Remove the 'Ticker' column (if present)\n",
    "df.set_index('date', inplace=True)  # Set 'date' as the index\n",
    "\n",
    "# --- 4. Data Type Conversion ---\n",
    "price_cols = ['open', 'high', 'low', 'close', 'volume']  # List of price-related columns\n",
    "for col in price_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric, handle errors\n",
    "    \n",
    "# --- 5. Final Data Cleaning ---\n",
    "df.dropna(subset=price_cols, inplace=True)  # Drop rows with NaN values in price columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. 📈 BASIC EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Basic DataFrame Information ---\n",
    "#print(\"\\n--- Basic Information ---\")\n",
    "#print(\"Date Frame index: \\n \", df.index)\n",
    "#print(\"========\" * 5)\n",
    "#print(\"\\n--- Date Frame Info ---\")\n",
    "#print(df.info())\n",
    "#print(\"========\" * 5)\n",
    "#print(\"\\n--- Date Frame Describe ---\")\n",
    "#print(df.describe())\n",
    "\n",
    "# --- 2. Daily Price and Volume Trends ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df.index, df['close'], label='Close Price')\n",
    "plt.title('Close Price Over Time (Daily)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df.index, df['volume'], label='Volume', color='orange')\n",
    "plt.title('Volume Over Time (Daily)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Weekly and Monthly Aggregations with Volume ---\n",
    "# Resample data to weekly, monthly, and yearly frequencies\n",
    "df_weekly = df[['close', 'volume']].resample('W').agg({'close': 'mean', 'volume': 'sum'})\n",
    "df_monthly = df[['close', 'volume']].resample('M').agg({'close': 'mean', 'volume': 'sum'})\n",
    "\n",
    "# Plot weekly and monthly trends with volume\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))  # Create 2 subplots\n",
    "\n",
    "# Weekly Plot\n",
    "axes[0].plot(df_weekly.index, df_weekly['close'], label='Weekly Average Close', color='blue')\n",
    "axes[0].set_ylabel('Average Close Price', color='blue')\n",
    "axes[0].tick_params(axis='y', labelcolor='blue')\n",
    "axes[0].set_title('Weekly Average Close Price and Volume')\n",
    "axes[0].grid(True)\n",
    "\n",
    "ax2 = axes[0].twinx()  # Create a secondary y-axis\n",
    "ax2.bar(df_weekly.index, df_weekly['volume'], label='Weekly Volume', color='red', alpha=0.5)\n",
    "ax2.set_ylabel('Volume', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "axes[0].legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Monthly Plot (Similar structure to weekly plot)\n",
    "axes[1].plot(df_monthly.index, df_monthly['close'], label='Monthly Average Close', color='blue')\n",
    "axes[1].set_ylabel('Average Close Price', color='blue')\n",
    "axes[1].tick_params(axis='y', labelcolor='blue')\n",
    "axes[1].set_title('Monthly Average Close Price and Volume')\n",
    "axes[1].grid(True)\n",
    "\n",
    "ax3 = axes[1].twinx()\n",
    "ax3.bar(df_monthly.index, df_monthly['volume'], label='Monthly Volume', color='red', alpha=0.5)\n",
    "ax3.set_ylabel('Volume', color='red')\n",
    "ax3.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "axes[1].legend(loc='upper left')\n",
    "ax3.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 🛠 FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engineering import FeatureEngineering\n",
    "# Initialize and run feature engineering\n",
    "#fe = FeatureEngineering(df)\n",
    "#features_df = fe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# 📈 4. 🛠 FEATURE ENGINEERING AND EXTRACTION (FINAL CLEAN VERSION)\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.1 Price Behavior Features\n",
    "# ---------------------------------------------\n",
    "\n",
    "\n",
    "# ---  Basic Time Features ---\n",
    "df['year'] = df.index.year\n",
    "df['month_num'] = df.index.month\n",
    "df['day_number_week'] = df.index.weekday  # 0=Monday\n",
    "df['day_number_month'] = df.index.day\n",
    "df['week_number'] = df.index.isocalendar().week\n",
    "df['quarter'] = df.index.quarter\n",
    "\n",
    "\n",
    "# --- Time Cyclical Features ---\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month_num'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month_num'] / 12)\n",
    "\n",
    "df['dayofweek_sin'] = np.sin(2 * np.pi * df['day_number_week'] / 7)\n",
    "df['dayofweek_cos'] = np.cos(2 * np.pi * df['day_number_week'] / 7)\n",
    "\n",
    "df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n",
    "df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n",
    "\n",
    "\n",
    "# Price range and averages\n",
    "df['delta_price'] = df['high'] - df['low']  # Daily high-low range\n",
    "df['avg_price'] = (df['close'] + df['high'] + df['low'] + df['open']) / 4  # Daily average price\n",
    "df['price_ratio'] = df['delta_price'] / df['avg_price']  # Ratio of daily range to average price\n",
    "\n",
    "df['price_ratio_5d_std'] = df['price_ratio'].rolling(window=5).std()\n",
    "df['price_ratio_10d_std'] = df['price_ratio'].rolling(window=10).std()\n",
    "df['price_ratio_20d_std'] = df['price_ratio'].rolling(window=20).std()\n",
    "df['price_ratio_30d_std'] = df['price_ratio'].rolling(window=30).std()\n",
    "\n",
    "# Price ratios change\n",
    "df['price_ratio_change_1d'] = df['price_ratio'].pct_change().shift(-1)\n",
    "df['price_ratio_change_2d'] = df['price_ratio'].pct_change().shift(-2)\n",
    "\n",
    "# Investment metrics\n",
    "df['invest'] = df['volume'] * df['avg_price']  # Volume weighted by avg price\n",
    "df['invest_change_1d'] = df['invest'].pct_change().shift(-1)\n",
    "df['invest_change_2d'] = df['invest'].pct_change().shift(-2)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.2 Return and Momentum Features\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Simple returns\n",
    "df['return_1d'] = df['close'].pct_change(1)\n",
    "df['return_5d'] = df['close'].pct_change(5)\n",
    "df['return_log_1d'] = np.log(df['close'] / df['close'].shift(1))\n",
    "df['return_log_5d'] = np.log(df['close'] / df['close'].shift(5))\n",
    "\n",
    "\n",
    "# --- Standard Deviation (Volatility) based on Simple Returns ---\n",
    "\n",
    "# Volatility of daily returns over a rolling 5-day window\n",
    "df['volatility_return_1d_std'] = df['return_1d'].rolling(window=5).std()\n",
    "\n",
    "# Volatility of 5-day returns over a rolling 5-day window\n",
    "df['volatility_return_5d_std'] = df['return_5d'].rolling(window=5).std()\n",
    "\n",
    "# --- Standard Deviation based on Log Returns ---\n",
    "\n",
    "# Volatility of daily log returns over 5 days\n",
    "df['volatility_return_log_1d_std'] = df['return_log_1d'].rolling(window=5).std()\n",
    "\n",
    "# Volatility of 5-day log returns over 5 days\n",
    "df['volatility_return_log_5d_std'] = df['return_log_5d'].rolling(window=5).std()\n",
    "\n",
    "\n",
    "# Cumulative and rolling returns\n",
    "for window in [3, 5, 7, 14, 20, 30, 60]:\n",
    "    df[f'return_cum_{window}d'] = df['return_1d'].rolling(window=window).sum()\n",
    "    df[f'return_rollmean_{window}d'] = df['return_1d'].rolling(window=window).mean()\n",
    "\n",
    "# Momentum indicators\n",
    "df['momentum_1d'] = df['close'] - df['close'].shift(1)\n",
    "for window in [3, 5, 7, 14, 20, 30, 60]:\n",
    "    df[f'momentum_abs_{window}d'] = df['close'] - df['close'].shift(window)\n",
    "    df[f'momentum_rel_{window}d'] = df['close'] / df['close'].shift(window)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.3 Volatility and Risk Features\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Rolling volatility\n",
    "for window in [3, 5, 7, 14, 20, 30, 60]:\n",
    "    df[f'volatility_{window}d'] = df['return_log_1d'].rolling(window=window).std()\n",
    "\n",
    "# Volume-weighted returns\n",
    "for window in [3, 5, 7, 14, 20, 30, 60]:\n",
    "    df[f'volume_weighted_return_{window}d'] = (df['return_1d'] * df['volume']).rolling(window=window).mean()\n",
    "\n",
    "# Volatility relative to volume\n",
    "for window in [3, 5, 7, 14, 20, 30, 60]:\n",
    "    df[f'volatility_to_volume_{window}d'] = df[f'volatility_{window}d'] / df['volume'].rolling(window=window).mean()\n",
    "\n",
    "# True Range and Average True Range (ATR)\n",
    "df['tr'] = np.maximum(\n",
    "    df['high'] - df['low'],\n",
    "    np.maximum(abs(df['high'] - df['close'].shift()), abs(df['low'] - df['close'].shift()))\n",
    ")\n",
    "\n",
    "for window in [3, 5, 7, 14, 20, 25]:\n",
    "    df[f'atr_{window}d'] = df['tr'].rolling(window=window).mean()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.4 Trend Indicators\n",
    "# ---------------------------------------------\n",
    "\n",
    "# RSI Calculation (Relative Strength Index)\n",
    "def compute_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    rs = up.rolling(window=window).mean() / down.rolling(window=window).mean()\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "for window in [3, 5, 7, 14]:\n",
    "    df[f'rsi_{window}d'] = compute_rsi(df['close'], window=window)\n",
    "\n",
    "# MACD and Signal Line\n",
    "exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "df['macd_line'] = exp1 - exp2\n",
    "df['macd_signal'] = df['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.5 Key Levels: Pivot Points, Supports, Resistances\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Daily pivot points\n",
    "df['pivot_point'] = (df['high'] + df['low'] + df['close']) / 3\n",
    "df['support1'] = (2 * df['pivot_point']) - df['high']\n",
    "df['resistance1'] = (2 * df['pivot_point']) - df['low']\n",
    "df['support2'] = df['pivot_point'] - (df['high'] - df['low'])\n",
    "df['resistance2'] = df['pivot_point'] + (df['high'] - df['low'])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.6 Breakout Indicators\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Donchian Channels\n",
    "df['donchian_high_20d'] = df['high'].rolling(window=20).max()\n",
    "df['donchian_low_20d'] = df['low'].rolling(window=20).min()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.7 Fibonacci Levels (Retracement Zones)\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Classical Fibonacci retracements\n",
    "high_30d = df['high'].rolling(window=30).max()\n",
    "low_30d = df['low'].rolling(window=30).min()\n",
    "diff_30d = high_30d - low_30d\n",
    "\n",
    "fib_levels = [0, 0.236, 0.382, 0.5, 0.618, 0.786, 1]\n",
    "for lvl in fib_levels:\n",
    "    df[f'fib_{lvl}_30d'] = high_30d - (diff_30d * lvl)\n",
    "\n",
    "# Fibonacci zone flags\n",
    "df['above_fib_0.5_30d'] = (df['close'] > df['fib_0.5_30d']).astype(int)\n",
    "df['above_fib_0.618_30d'] = (df['close'] > df['fib_0.618_30d']).astype(int)\n",
    "df['inside_fib_zone_0.382_0.618'] = ((df['close'] < df['fib_0.382_30d']) & (df['close'] > df['fib_0.618_30d'])).astype(int)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.8 Moving Averages\n",
    "# ---------------------------------------------\n",
    "\n",
    "for window in [3, 5, 7, 14, 20, 50, 200]:\n",
    "    df[f'sma_{window}d'] = df['close'].rolling(window=window).mean()\n",
    "    df[f'ema_{window}d'] = df['close'].ewm(span=window, adjust=False).mean()\n",
    "\n",
    "# Bollinger Bands\n",
    "for window in [14, 20]:\n",
    "    sma = df['close'].rolling(window=window).mean()\n",
    "    rolling_std = df['close'].rolling(window=window).std()\n",
    "    df[f'bollinger_upper_{window}d'] = sma + 2 * rolling_std\n",
    "    df[f'bollinger_lower_{window}d'] = sma - 2 * rolling_std\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.9 Volume Based Indicators\n",
    "# ---------------------------------------------\n",
    "\n",
    "# VWAP (Volume Weighted Average Price)\n",
    "df['vwap'] = (df['volume'] * (df['high'] + df['low'] + df['close']) / 3).cumsum() / df['volume'].cumsum()\n",
    "\n",
    "# OBV (On Balance Volume)\n",
    "df['obv'] = (np.sign(df['close'].diff()) * df['volume']).fillna(0).cumsum()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.10 Lag and Difference Features\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Lags\n",
    "for lag in [1, 2, 3, 5, 7, 14, 20, 30]:\n",
    "    df[f'lag_close_{lag}d'] = df['close'].shift(lag)\n",
    "    df[f'lag_return_{lag}d'] = df['close'].pct_change(lag)\n",
    "\n",
    "# Differences\n",
    "df['diff_close_1d'] = df['close'].diff(1)\n",
    "df['diff_close_2d'] = df['close'].diff(2)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.11 Future Targets for Modeling\n",
    "# ---------------------------------------------\n",
    "\n",
    "df['target_direction_1d'] = (df['close'].shift(-1) > df['close']).astype(int)\n",
    "df['target_direction_3d'] = (df['close'].shift(-3) > df['close']).astype(int)\n",
    "df['target_direction_5d'] = (df['close'].shift(-5) > df['close']).astype(int)\n",
    "df['target_volatility_next_3d'] = df['return_1d'].rolling(window=3).std().shift(-3)\n",
    "df['target_volatility_next_5d'] = df['return_1d'].rolling(window=5).std().shift(-5)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.12 Golden Cross and Death Cross\n",
    "# ---------------------------------------------\n",
    "\n",
    "sma_20 = df['close'].rolling(window=20).mean()\n",
    "sma_50 = df['close'].rolling(window=50).mean()\n",
    "sma_200 = df['close'].rolling(window=200).mean()\n",
    "\n",
    "df['golden_cross_signal'] = np.where((sma_50.shift(1) < sma_200.shift(1)) & (sma_50 > sma_200), 1, 0)\n",
    "df['death_cross_signal'] = np.where((sma_50.shift(1) > sma_200.shift(1)) & (sma_50 < sma_200), -1, 0)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4.13 Final Cleanup\n",
    "# ---------------------------------------------\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.to_csv(\"../data/aapl_stock_features_final_full.csv\")\n",
    "print(\"✅ All features engineered and saved successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 Advance EDA Interpretation Guide\n",
    "\n",
    "## 1. Close Price and Moving Averages\n",
    "- **Description:** Shows raw close price with SMA20, SMA50, and SMA200.\n",
    "- **Purpose:** Helps identify long-term and short-term trends.\n",
    "- **Interpretation:** \n",
    "  - SMA20 above SMA50/SMA200 → bullish signal (Golden Cross).\n",
    "  - SMA20 below SMA50/SMA200 → bearish signal (Death Cross).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Average True Range (ATR)\n",
    "- **Description:** Measures price volatility over 3, 5, 7, and 14 days.\n",
    "- **Purpose:** Understand market volatility.\n",
    "- **Interpretation:** \n",
    "  - High ATR → high volatility.\n",
    "  - Low ATR → stable or consolidating market.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Relative Strength Index (RSI)\n",
    "- **Description:** Tracks momentum strength (3, 5, 7, 14 days).\n",
    "- **Purpose:** Detect overbought or oversold conditions.\n",
    "- **Interpretation:**\n",
    "  - RSI > 70 → overbought (potential drop).\n",
    "  - RSI < 30 → oversold (potential rise).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Pivot Points, Support, and Resistance\n",
    "- **Description:** Key horizontal levels based on price calculations.\n",
    "- **Purpose:** Detect reversal points and support/resistance zones.\n",
    "- **Interpretation:** \n",
    "  - Price near support → possible bounce.\n",
    "  - Price near resistance → possible rejection.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Donchian Channels\n",
    "- **Description:** Tracks 20-day high and low ranges.\n",
    "- **Purpose:** Identify breakouts.\n",
    "- **Interpretation:**\n",
    "  - Break above Donchian high → bullish breakout.\n",
    "  - Break below Donchian low → bearish breakout.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Volume and VWAP\n",
    "- **Description:** Combines volume and price average (VWAP).\n",
    "- **Purpose:** Confirm trend strength.\n",
    "- **Interpretation:**\n",
    "  - Price above VWAP → bullish bias.\n",
    "  - Price below VWAP → bearish bias.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Future Volatility (Predicted Standard Deviation)\n",
    "- **Description:** Forecasts expected volatility over 3, 5, and 7 days.\n",
    "- **Purpose:** Anticipate future risk levels.\n",
    "- **Interpretation:**\n",
    "  - High volatility forecast → prepare for bigger moves.\n",
    "  - Low volatility forecast → stable market expected.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Target Direction Predictions\n",
    "- **Description:** Predicts whether price will rise (1) or fall (0) in next 1 or 2 days.\n",
    "- **Purpose:** Model short-term market bias.\n",
    "- **Interpretation:** \n",
    "  - Useful for building classification models (up/down movement).\n",
    "\n",
    "---\n",
    "\n",
    "# ✨ Quick Recap\n",
    "- **Trend analysis:** Close Price, Moving Averages, Pivot Points.\n",
    "- **Momentum analysis:** RSI.\n",
    "- **Volatility/risk analysis:** ATR, Future Volatility.\n",
    "- **Breakout detection:** Donchian Channels, VWAP.\n",
    "- **Volume confirmation:** Volume bars + VWAP.\n",
    "- **Directional prediction:** Target signals (Direction1, Direction2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 📊 5. ADVANCED EDA AFTER FINAL FEATURE ENGINEERING\n",
    "# ==========================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Close Price and Moving Averages ---\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(df['close'], label='Close Price', alpha=0.7)\n",
    "plt.plot(df['sma_50d'], label='SMA 50 Days', linestyle='-.')\n",
    "plt.plot(df['sma_200d'], label='SMA 200 Days', linestyle=':')\n",
    "plt.title('Close Price and Long-Term Moving Averages')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 2. Average True Range (ATR) - Volatility ---\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(df['atr_3d'], label='ATR 3 Days')\n",
    "plt.plot(df['atr_5d'], label='ATR 5 Days')\n",
    "plt.plot(df['atr_7d'], label='ATR 7 Days')\n",
    "plt.plot(df['atr_14d'], label='ATR 14 Days')\n",
    "plt.title('Average True Range (ATR) Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('ATR Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Relative Strength Index (RSI) ---\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(df['rsi_3d'], label='RSI 3 Days')\n",
    "plt.plot(df['rsi_5d'], label='RSI 5 Days')\n",
    "plt.plot(df['rsi_7d'], label='RSI 7 Days')\n",
    "plt.plot(df['rsi_14d'], label='RSI 14 Days')\n",
    "plt.axhline(70, color='red', linestyle='--', alpha=0.5)\n",
    "plt.axhline(30, color='green', linestyle='--', alpha=0.5)\n",
    "plt.title('Relative Strength Index (RSI)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('RSI Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 4. Pivot Points, Supports, Resistances ---\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(df['close'], label='Close Price')\n",
    "plt.plot(df['pivot_point'], label='Pivot Point', linestyle='--')\n",
    "plt.plot(df['support1'], label='Support 1', linestyle='--')\n",
    "plt.plot(df['resistance1'], label='Resistance 1', linestyle='--')\n",
    "plt.title('Pivot Points and Support/Resistance Levels')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 5. Donchian Channels ---\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(df['close'], label='Close Price')\n",
    "plt.plot(df['donchian_high_20d'], label='Donchian High 20d', linestyle='-.')\n",
    "plt.plot(df['donchian_low_20d'], label='Donchian Low 20d', linestyle='-.')\n",
    "plt.title('Donchian Channel High/Low')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 6. Volume and VWAP ---\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14,7))\n",
    "\n",
    "ax1.plot(df['close'], color='blue', label='Close Price')\n",
    "ax1.set_ylabel('Price', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df['vwap'], color='purple', label='VWAP', linestyle='--')\n",
    "ax2.bar(df.index, df['volume'], alpha=0.3, color='grey', label='Volume')\n",
    "ax2.set_ylabel('Volume', color='grey')\n",
    "ax2.tick_params(axis='y', labelcolor='grey')\n",
    "\n",
    "fig.suptitle('Close Price, VWAP and Volume')\n",
    "fig.legend(loc=\"upper left\", bbox_to_anchor=(0.1,0.9))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 7. Predicted Future Volatility (Standard Deviation of Returns) ---\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(df['target_volatility_next_3d'], label='Future Volatility Next 3 Days')\n",
    "plt.title('Predicted Future Volatility (Standard Deviation of Returns)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 8. Direction Prediction Signals ---\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(df['target_direction_1d'], label='Direction 1 Day', marker='o', linestyle='')\n",
    "plt.plot(df['target_direction_3d'], label='Direction 3 Days', marker='x', linestyle='')\n",
    "plt.title('Target Direction Signals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Direction (0 = Down, 1 = Up)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 9. Golden Cross and Death Cross Events ---\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "# Plot close price\n",
    "plt.plot(df.index, df['close'], label='Close Price', color='blue', alpha=0.6)\n",
    "\n",
    "# Plot SMA 50 and SMA 200\n",
    "plt.plot(df.index, df['sma_50d'], label='SMA 50 Days', color='green', linewidth=1.5)\n",
    "plt.plot(df.index, df['sma_200d'], label='SMA 200 Days', color='red', linewidth=1.5)\n",
    "\n",
    "# Highlight Golden Crosses\n",
    "golden_crosses = df[df['golden_cross_signal'] == 1]\n",
    "plt.scatter(golden_crosses.index, df.loc[golden_crosses.index, 'close'], marker='^', color='gold', s=100, label='Golden Cross')\n",
    "\n",
    "# Highlight Death Crosses\n",
    "death_crosses = df[df['death_cross_signal'] == -1]\n",
    "plt.scatter(death_crosses.index, df.loc[death_crosses.index, 'close'], marker='v', color='black', s=100, label='Death Cross')\n",
    "\n",
    "plt.title('Golden Cross and Death Cross on Close Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis and Clean Target Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 7. 📈 Safe Correlation Analysis and Clean Target Table (Annotated Version)\n",
    "# ==========================================================\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 7.1 Safe Copy ---\n",
    "df_copy = df.copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "# --- 7.2 Temporary Corr Matrix ---\n",
    "corr_matrix_temp = df_copy.corr()\n",
    "\n",
    "# --- 7.3 Filter out Features where correlation with 'close' is NaN ---\n",
    "\n",
    "# Focus only on features where correlation with 'close' exists\n",
    "valid_features = corr_matrix_temp['close'].dropna().index.tolist()\n",
    "\n",
    "# Now build cleaned correlation matrix based on valid features only\n",
    "corr_matrix_clean = df_copy[valid_features].corr()\n",
    "\n",
    "# --- 7.4 Plot the Clean Correlation Matrix with Annotated Values ---\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix_clean, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "#plt.figure(figsize=(24, 20))\n",
    "\n",
    "# Draw the heatmap\n",
    "#sns.heatmap(\n",
    " #   corr_matrix_clean,\n",
    "  #  mask=mask,\n",
    "   # cmap='coolwarm',\n",
    "    #center=0,\n",
    "    #square=True,\n",
    "    #linewidths=0.5,\n",
    "    #annot=True,               # <== ✅ Add numbers inside the squares\n",
    "    #fmt=\".2f\",                 # <== 2 decimal points\n",
    "    #annot_kws={\"size\": 8},\n",
    "    #cbar_kws={\"shrink\": 0.75}\n",
    "#)\n",
    "\n",
    "#plt.title('🔗 Clean Correlation Matrix with Values (Lower Triangle)', fontsize=24)\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.yticks(rotation=0)\n",
    "#plt.show()\n",
    "\n",
    "# --- 7.5 Correlation Table for 'close' (Cleaned) ---\n",
    "\n",
    "# Focus only on correlation with 'close' (drop close itself)\n",
    "target_corr = corr_matrix_clean['close'].drop('close').sort_values(ascending=False)\n",
    "\n",
    "# Build a pretty table\n",
    "target_corr_table = target_corr.reset_index()\n",
    "target_corr_table.columns = ['Feature', 'Correlation_with_Close']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Feature Selector Based on Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 8. 🛠️ Smart Feature Selector Based on Correlation\n",
    "# ==========================================================\n",
    "\n",
    "def smart_feature_selector(df, target_col='close', threshold=0.001):\n",
    "    \"\"\"\n",
    "    Selects the best features based on their correlation with the target column.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Full feature DataFrame (must be numeric).\n",
    "    target_col : str\n",
    "        Name of the target column (default 'close').\n",
    "    threshold : float\n",
    "        Minimum absolute correlation required to keep the feature.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    selected_features : list\n",
    "        List of feature names selected.\n",
    "    selected_corr_table : pd.DataFrame\n",
    "        Pretty table of feature names and their correlation values.\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Step 1: Safe copy\n",
    "    df_copy = df.copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "    # Step 2: Calculate initial correlation matrix\n",
    "    corr_matrix = df_copy.corr()\n",
    "\n",
    "    # Step 3: Filter only valid features (correlated to target, no NaN)\n",
    "    valid_features = corr_matrix[target_col].dropna().index.tolist()\n",
    "\n",
    "    # Step 4: Clean correlation matrix\n",
    "    corr_matrix_clean = df_copy[valid_features].corr()\n",
    "\n",
    "    # Step 5: Target correlation\n",
    "    target_corr = corr_matrix_clean[target_col].drop(target_col)\n",
    "\n",
    "    # Step 6: Select features with absolute correlation above threshold\n",
    "    selected = target_corr[abs(target_corr) > threshold]\n",
    "\n",
    "    selected_features = selected.index.tolist()\n",
    "\n",
    "    # Step 7: Pretty correlation table\n",
    "    selected_corr_table = selected.reset_index()\n",
    "    selected_corr_table.columns = ['Feature', f'Correlation_with_{target_col}']\n",
    "    selected_corr_table = selected_corr_table.sort_values(by=f'Correlation_with_{target_col}', ascending=False)\n",
    "\n",
    "    return selected_features, selected_corr_table\n",
    "\n",
    "# ==========================================================\n",
    "# ✅ Usage Example\n",
    "# ==========================================================\n",
    "\n",
    "# Call the smart selector on your df\n",
    "selected_features, selected_corr_table = smart_feature_selector(df, target_col='close', threshold=0.2)\n",
    "\n",
    "# Show selected features\n",
    "print(\"\\nSelected Features based on Correlation Threshold:\")\n",
    "for feature in selected_features:\n",
    "    print(f\"- {feature}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 🧹 DATA PREPARATION FOR ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 6. 🧹 DATA PREPARATION FOR ML (Including Given Feature Set)\n",
    "# ==========================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 6.3 Ensure DataFrame is sorted by date ---\n",
    "df = df.sort_index()\n",
    "\n",
    "# --- 6.4 Define Explicit Feature Columns and Target (Updated) ---\n",
    "\n",
    "# Explicit feature columns selected\n",
    "features = selected_features\n",
    "\n",
    "# Target variable\n",
    "target = 'close'\n",
    "\n",
    "# --- 6.5 Create Feature Matrix (X) and Target Vector (y) ---\n",
    "\n",
    "X = df[features]  # Only use the selected feature columns\n",
    "y = df[target]    # Predict the close price\n",
    "\n",
    "# --- 6.6 Split into Train and Test Sets based on Time Order (No Shuffle) ---\n",
    "\n",
    "# Number of last days to use for testing\n",
    "n_test_days = 10\n",
    "\n",
    "# Split manually based on time\n",
    "X_train = X.iloc[:-n_test_days]    # Train on all data except last 5 days\n",
    "X_test = X.iloc[-n_test_days:]     # Test on last 5 days\n",
    "y_train = y.iloc[:-n_test_days]\n",
    "y_test = y.iloc[-n_test_days:]\n",
    "\n",
    "# --- 6.7 Feature Scaling ---\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform both train and test\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- 6.8 Print Dataset Shapes to Confirm ---\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 🤖 MODEL TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell Purpose**\n",
    "\n",
    "This cell implements a small ensemble of Random Forest models—three regressors and one classifier—to illustrate how we can tackle different prediction tasks on our engineered stock‐market features:\n",
    "\n",
    "1. **RandomForestRegressor for Close Price**  \n",
    "   - **What**: Learns the mapping from scaled input features (momentum, volatility, seasonality, etc.) to the actual next‐day closing price.  \n",
    "   - **Why**: Accurate price forecasts enable better position sizing, P&L estimation and can feed into portfolio optimization.\n",
    "\n",
    "2. **RandomForestRegressor for Daily Range (High – Low)**  \n",
    "   - **What**: Predicts the magnitude of the day’s intraday swing.  \n",
    "   - **Why**: Anticipating range size helps set realistic stop-loss/take-profit levels and gauge intraday risk and liquidity.\n",
    "\n",
    "3. **RandomForestRegressor for 5-Day Volatility**  \n",
    "   - **What**: Forecasts the standard deviation of returns over the next five trading days.  \n",
    "   - **Why**: Volatility forecasts drive risk management, option pricing inputs, and position sizing under Value-at-Risk frameworks.\n",
    "\n",
    "4. **RandomForestClassifier for Direction (Up/Down)**  \n",
    "   - **What**: Classifies whether the price will close higher or lower the next day, outputting both binary labels and class probabilities.  \n",
    "   - **Why**: Directional signals support trade-signal generation. Probabilities feed into threshold‐based entry/exit criteria or portfolio weighting.\n",
    "\n",
    "---\n",
    "\n",
    "**Evaluation Strategy**\n",
    "\n",
    "- **Training vs. Test Split**: Models are fit on the scaled training set and then evaluated on the unseen test set to reveal any overfitting.  \n",
    "- **Regression Metrics**  \n",
    "  - *MSE / MAE / R²* on training set to assess in-sample fit and bias–variance tradeoff.  \n",
    "- **Classification Metrics**  \n",
    "  - *Accuracy, Precision, Recall, F1, ROC-AUC* on the test set to measure real-world directional performance.  \n",
    "- **Confusion Matrix Heatmap**  \n",
    "  - Visualizes true vs. predicted classes, highlighting types of classification errors (false positives vs. false negatives) and informing threshold tuning.\n",
    "\n",
    "By comparing in-sample and out-of-sample results across these tasks, you get a holistic view of model generalization, risk‐return trade-offs, and where further feature or hyperparameter refinement is needed.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 7. 🤖 Updated Full ML Modeling: Train vs Test Evaluation\n",
    "# ==========================================================\n",
    "\n",
    "# 7.0 Imports\n",
    "# ----------------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier  # Models for regression & classification\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    explained_variance_score, max_error, mean_squared_log_error, mean_absolute_percentage_error,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, log_loss, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt  # Plotting library for visual evaluation\n",
    "import seaborn as sns           # Statistical data visualization\n",
    "import pandas as pd             # Data handling\n",
    "import numpy as np              # Numerical operations\n",
    "\n",
    "# ==========================================================\n",
    "# 7.1 Regression Models\n",
    "# ----------------------------------------------------------\n",
    "# (1) Predict Close Price\n",
    "# ----------------------------------------------------------\n",
    "# Initialize random forest regressor for closing price prediction\n",
    "model_close = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train model on scaled training features and target y_train\n",
    "model_close.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Generate predictions on training set for diagnostic evaluation\n",
    "y_pred_close_train = model_close.predict(X_train_scaled)\n",
    "# Generate predictions on test set for out-of-sample performance\n",
    "y_pred_close_test = model_close.predict(X_test_scaled)\n",
    "\n",
    "# (2) Predict Price Range (high - low)\n",
    "# ----------------------------------------------------------\n",
    "# Derive range target for training and testing from original DataFrame\n",
    "y_range_train = (df['high'] - df['low']).iloc[:-n_test_days]  # All but last n days\n",
    "y_range_test  = (df['high'] - df['low']).iloc[-n_test_days:]   # Last n days\n",
    "\n",
    "# Initialize regressor for daily price range prediction\n",
    "model_range = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train on training portion of range target\\ n\n",
    "model_range.fit(X_train_scaled, y_range_train)\n",
    "\n",
    "# Predict on train and test splits\n",
    "y_pred_range_train = model_range.predict(X_train_scaled)\n",
    "y_pred_range_test  = model_range.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# (3) Predict Volatility\n",
    "# ----------------------------------------------------------\n",
    "# Targets for next-5-day volatility from engineered features\n",
    "y_volatility_train = df['target_volatility_next_5d'].iloc[:-n_test_days]\n",
    "y_volatility_test  = df['target_volatility_next_5d'].iloc[-n_test_days:]\n",
    "\n",
    "# Initialize random forest regressor for volatility\n",
    "model_volatility = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit model on training volatility\\ n\n",
    "model_volatility.fit(X_train_scaled, y_volatility_train)\n",
    "\n",
    "# Generate volatility predictions\n",
    "y_pred_volatility_train = model_volatility.predict(X_train_scaled)\n",
    "y_pred_volatility_test  = model_volatility.predict(X_test_scaled)\n",
    "\n",
    "# ==========================================================\n",
    "# 7.2 Classification Model\n",
    "# ----------------------------------------------------------\n",
    "# (4) Predict Direction\n",
    "# ----------------------------------------------------------\n",
    "# Define binary direction target for training and testing\n",
    "y_direction_train = df['target_direction_1d'].iloc[:-n_test_days]\n",
    "y_direction_test  = df['target_direction_1d'].iloc[-n_test_days:]\n",
    "\n",
    "# Initialize random forest classifier for direction\n",
    "model_direction = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Train classifier on training data\n",
    "model_direction.fit(X_train_scaled, y_direction_train)\n",
    "\n",
    "# Predict class labels and probabilities on training set\n",
    "y_pred_direction_train        = model_direction.predict(X_train_scaled)        # Label predictions\n",
    "y_pred_direction_proba_train  = model_direction.predict_proba(X_train_scaled)[:, 1]  # Probability of class=1\n",
    "\n",
    "# Predict labels and probabilities on test set for evaluation\n",
    "y_pred_direction_test         = model_direction.predict(X_test_scaled)\n",
    "y_pred_direction_proba_test   = model_direction.predict_proba(X_test_scaled)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Evaluation Metrics Explained\n",
    "\n",
    "After fitting each model, we gather:\n",
    "\n",
    "| Metric         | Definition                                        | Ideal Value                              | Interpretation                          |\n",
    "| -------------- | ------------------------------------------------- | ---------------------------------------- | --------------------------------------- |\n",
    "| **RMSE**       | √(mean squared error) – average error magnitude    | **Lower** (0 = perfect)                  | How far predictions stray from true values on average. |\n",
    "| **R²**         | Proportion of variance explained                   | **Closer to 1** (negative = worse than constant) | Fraction of variability captured by the model. |\n",
    "| **Accuracy**   | (TP + TN) / total                                   | **Higher** (1 = perfect)                 | Overall rate of correct direction calls. |\n",
    "| **Precision**  | TP / (TP + FP)                                      | **Higher**                                | When model predicts “up,” how often it’s right. |\n",
    "| **Recall**     | TP / (TP + FN)                                      | **Higher**                                | Of all actual “up” days, how many were caught. |\n",
    "| **F1 Score**   | 2·(Precision·Recall)/(Precision+Recall)             | **Higher**                                | Balance between precision and recall.    |\n",
    "| **F2 Score**   | (1+2²)·(P·R)/(4·P+R)                                | **Higher**                                | Weights recall more heavily than precision. |\n",
    "| **ROC-AUC**    | Area under ROC curve                                | **Closer to 1** (0.5 = random)            | Ability to rank “up” vs. “down” days.    |\n",
    "\n",
    "**Good vs. Bad**  \n",
    "- **Good**: low RMSE, high R² on **test** set; high Accuracy/Precision/Recall/F1/AUC on **test**.  \n",
    "- **Bad**: high RMSE, low R² on test; poor classification metrics on test.\n",
    "\n",
    "**Overfitting vs. Underfitting**  \n",
    "- **Overfitting**:  \n",
    "  - Train metrics much better than test (e.g. RMSE_train ≪ RMSE_test, R²_train ≫ R²_test, Accuracy_train ≫ Accuracy_test).  \n",
    "  - Model learned noise, won’t generalize well.  \n",
    "- **Underfitting**:  \n",
    "  - Both train and test metrics are poor (e.g. high RMSE, low R², low Accuracy/F1/AUC).  \n",
    "  - Model too simple, missing key patterns.\n",
    "\n",
    "Aim for good absolute scores **and** a small gap between train and test to ensure robust, generalizable performance.```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 7.3 Evaluation Metrics (Corrected Version)\n",
    "# ==========================================================\n",
    "\n",
    "# Create separate dictionaries for each model\n",
    "model_results = []\n",
    "\n",
    "# --- Close Price Regression ---\n",
    "results_close = {}\n",
    "results_close['model'] = 'model_close - RandomForestRegressor'\n",
    "results_close['close_rmse_train'] = np.sqrt(mean_squared_error(y_train, y_pred_close_train))\n",
    "results_close['close_rmse_test'] = np.sqrt(mean_squared_error(y_test, y_pred_close_test))\n",
    "results_close['close_r2_train'] = r2_score(y_train, y_pred_close_train)\n",
    "results_close['close_r2_test'] = r2_score(y_test, y_pred_close_test)\n",
    "model_results.append(results_close)\n",
    "\n",
    "# --- Price Range Regression ---\n",
    "results_range = {}\n",
    "results_range['model'] = 'model_range - RandomForestRegressor'\n",
    "results_range['range_rmse_train'] = np.sqrt(mean_squared_error(y_range_train, y_pred_range_train))\n",
    "results_range['range_rmse_test'] = np.sqrt(mean_squared_error(y_range_test, y_pred_range_test))\n",
    "results_range['range_r2_train'] = r2_score(y_range_train, y_pred_range_train)\n",
    "results_range['range_r2_test'] = r2_score(y_range_test, y_pred_range_test)\n",
    "model_results.append(results_range)\n",
    "\n",
    "# --- Volatility Regression ---\n",
    "results_volatility = {}\n",
    "results_volatility['model'] = 'model_volatility - RandomForestRegressor'\n",
    "results_volatility['volatility_rmse_train'] = np.sqrt(mean_squared_error(y_volatility_train, y_pred_volatility_train))\n",
    "results_volatility['volatility_rmse_test'] = np.sqrt(mean_squared_error(y_volatility_test, y_pred_volatility_test))\n",
    "results_volatility['volatility_r2_train'] = r2_score(y_volatility_train, y_pred_volatility_train)\n",
    "results_volatility['volatility_r2_test'] = r2_score(y_volatility_test, y_pred_volatility_test)\n",
    "model_results.append(results_volatility)\n",
    "\n",
    "# --- Direction Classification ---\n",
    "results_direction = {}\n",
    "results_direction['model'] = 'model_direction - RandomForestClassifier'\n",
    "results_direction['direction_accuracy_train'] = accuracy_score(y_direction_train, y_pred_direction_train)\n",
    "results_direction['direction_f1_train'] = f1_score(y_direction_train, y_pred_direction_train)\n",
    "results_direction['direction_auc_train'] = roc_auc_score(y_direction_train, y_pred_direction_proba_train)\n",
    "results_direction['direction_precision_train'] = precision_score(y_direction_train, y_pred_direction_train)\n",
    "results_direction['direction_recall_train'] = recall_score(y_direction_train, y_pred_direction_train)\n",
    "results_direction['direction_f2_train'] = fbeta_score(y_direction_train, y_pred_direction_train, beta=2)\n",
    "\n",
    "results_direction['direction_accuracy_test'] = accuracy_score(y_direction_test, y_pred_direction_test)\n",
    "results_direction['direction_f1_test'] = f1_score(y_direction_test, y_pred_direction_test)\n",
    "results_direction['direction_auc_test'] = roc_auc_score(y_direction_test, y_pred_direction_proba_test)\n",
    "results_direction['direction_precision_test'] = precision_score(y_direction_test, y_pred_direction_test)\n",
    "results_direction['direction_recall_test'] = recall_score(y_direction_test, y_pred_direction_test)\n",
    "results_direction['direction_f2_test'] = fbeta_score(y_direction_test, y_pred_direction_test, beta=2)\n",
    "model_results.append(results_direction)\n",
    "\n",
    "# Convert to DataFrame for easy view\n",
    "results_df = pd.DataFrame(model_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for Deep Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Full Evaluation Summary: Interpreting Model Results\n",
    "\n",
    "In this section, we evaluate all trained models based on **both Regression** and **Classification** metrics across **Train** and **Test** sets.\n",
    "\n",
    "### 🧠 How the Code Works:\n",
    "\n",
    "- We first **convert all model results** (stored as dictionaries) into a **single DataFrame**.\n",
    "- We **split** the metric names into:\n",
    "  - **Target** (e.g., close price, range, volatility, direction)\n",
    "  - **Metric Type** (e.g., RMSE, R2, Accuracy, Precision, F1, F2, AUC)\n",
    "  - **Set** (train or test)\n",
    "- We **pivot** the data for clean tabular visualization.\n",
    "- Finally, we **plot bar charts** to easily compare how each model performed on different metrics across training and testing.\n",
    "\n",
    "### 📈 How to Interpret the Metrics:\n",
    "\n",
    "#### Regression Models (Price, Range, Volatility):\n",
    "- **RMSE (Root Mean Squared Error)**:  \n",
    "  ➔ Lower is better. Measures prediction error in original units.\n",
    "- **R2 Score (Coefficient of Determination)**:  \n",
    "  ➔ Closer to 1 means better fit. Shows % of variance explained by the model.\n",
    "\n",
    "#### Classification Model (Direction: Up/Down):\n",
    "- **Accuracy**:  \n",
    "  ➔ % of correctly predicted up/down directions.\n",
    "- **Precision**:  \n",
    "  ➔ Out of predicted \"ups,\" how many were actually up.\n",
    "- **Recall**:  \n",
    "  ➔ Out of actual \"ups,\" how many were correctly caught.\n",
    "- **F1 Score**:  \n",
    "  ➔ Balance between Precision and Recall.\n",
    "- **F2 Score**:  \n",
    "  ➔ Similar to F1 but puts even more weight on Recall (catching true positives).\n",
    "- **ROC-AUC Score**:  \n",
    "  ➔ Measures model's ability to distinguish between up/down overall (0.5 = random, 1.0 = perfect).\n",
    "\n",
    "### ⚡ Key Insights:\n",
    "\n",
    "- **Compare Train vs Test**:  \n",
    "  ➔ If Train is much better than Test → possible **overfitting**.\n",
    "- **Watch R2 and RMSE Together**:  \n",
    "  ➔ High R2 + Low RMSE = Strong model.\n",
    "- **Classification:**  \n",
    "  ➔ High F1, F2, and AUC are signs of good classification balance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 7.5 Plots for Deep Interpretation\n",
    "# ==========================================================\n",
    "\n",
    "# --- 1. Close Price Actual vs Predicted (Train and Test) ---\n",
    "\n",
    "# First, make sure y_train and y_test have proper datetime index\n",
    "# Bring the original index from X_train and X_test\n",
    "\n",
    "y_train.index = X_train.index\n",
    "y_test.index = X_test.index\n",
    "\n",
    "# Define your time cutoff properly\n",
    "time_index_filter = pd.to_datetime('2024-09-01')\n",
    "\n",
    "# Correct filtering after fixing index\n",
    "y_train_filtered = y_train.loc[y_train.index > time_index_filter]\n",
    "y_pred_close_train_filtered = pd.Series(y_pred_close_train, index=y_train.index).loc[y_train.index > time_index_filter]\n",
    "\n",
    "y_test_filtered = y_test.loc[y_test.index > time_index_filter]\n",
    "y_pred_close_test_filtered = pd.Series(y_pred_close_test, index=y_test.index).loc[y_test.index > time_index_filter]\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.plot(y_train_filtered.index, y_train_filtered, label='Actual Close Train', color='blue', alpha=0.5)\n",
    "plt.plot(y_train_filtered.index, y_pred_close_train_filtered, label='Predicted Close Train', color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.plot(y_test_filtered.index, y_test_filtered, label='Actual Close Test', color='orange', alpha=0.5)\n",
    "plt.plot(y_test_filtered.index, y_pred_close_test_filtered, label='Predicted Close Test', color='green', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.title(f'Close Price: Train and Test Predictions (After {time_index_filter})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ==========================================================\n",
    "# 🚀 Done: Full ML Model Evaluation Train vs Test\n",
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 📋 7.4 Full Evaluation Summary (Updated for Extra Classification Metrics)\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Convert List of Model Results into a Full DataFrame ---\n",
    "\n",
    "# If you have multiple model results (as a list of dicts):\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# --- 2. Melt to Long Format for Easier Processing ---\n",
    "\n",
    "results_long = results_df.melt(id_vars=['model'], var_name='Metric', value_name='Score')\n",
    "\n",
    "# --- 3. Split Metric into parts (target/metric/set) ---\n",
    "\n",
    "results_long[['Target', 'Metric_Type', 'Set']] = results_long['Metric'].str.rsplit('_', n=2, expand=True)\n",
    "\n",
    "# Clean ordering\n",
    "results_long = results_long[['model', 'Target', 'Metric_Type', 'Set', 'Score']]\n",
    "\n",
    "# --- 4. Pivot Table for Clean View (optional) ---\n",
    "\n",
    "pivot_table = results_long.pivot_table(index=['model', 'Target', 'Metric_Type'], columns='Set', values='Score')\n",
    "display(pivot_table.style.format(\"{:.4f}\"))\n",
    "\n",
    "# --- 5. Plotting Per Model ---\n",
    "\n",
    "models = results_long['model'].unique()\n",
    "\n",
    "for model_name in models:\n",
    "    model_data = results_long[results_long['model'] == model_name]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    sns.barplot(\n",
    "        data=model_data,\n",
    "        x='Metric_Type',\n",
    "        y='Score',\n",
    "        hue='Set',\n",
    "        palette='coolwarm'\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"📊 {model_name} - Train vs Test Metrics\", fontsize=16)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Metric Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend(title='Dataset')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Prediction Evaluation Error and Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Advanced Prediction Evaluation\n",
    "\n",
    "We visualize the model performance in multiple ways to better understand prediction behavior, model bias, uncertainty, and reliability.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Residuals vs Fitted Values\n",
    "\n",
    "- **Purpose**: Check if prediction errors are random.\n",
    "- **Interpretation**:\n",
    "  - ✅ Random scatter around 0 → Model is likely well-specified.\n",
    "  - ❌ Visible patterns → Model might be biased or missing features.\n",
    "- **Goal**: No visible structure in residuals!\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Prediction Error Distribution (Histogram)\n",
    "\n",
    "- **Purpose**: Analyze how errors are distributed.\n",
    "- **Interpretation**:\n",
    "  - ✅ Bell-shaped (normal) distribution centered at 0 → Healthy model.\n",
    "  - ❌ Skewed or fat-tailed → Model risk or data issues.\n",
    "- **Goal**: Errors are centered and symmetrically spread around 0.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Predicted vs Actual Scatter Plot\n",
    "\n",
    "- **Purpose**: See how predictions match true values.\n",
    "- **Interpretation**:\n",
    "  - ✅ Points lie close to the diagonal line → Accurate model.\n",
    "  - ❌ Wide scatter → Model struggling to predict correctly.\n",
    "- **Goal**: Tight clustering along the ideal line.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Prediction Uncertainty Bands\n",
    "\n",
    "- **Purpose**: Visualize confidence intervals around predictions.\n",
    "- **Interpretation**:\n",
    "  - ✅ Most actual points inside ±1 Standard Deviation band → Reliable uncertainty estimate.\n",
    "  - ❌ Many points outside → Unstable predictions or high risk.\n",
    "- **Goal**: Small bands and most points inside.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 Summary\n",
    "\n",
    "| Check | Good Sign | Warning Sign |\n",
    "|:---|:---|:---|\n",
    "| Residuals | Random scatter around 0 | Patterns or trends |\n",
    "| Error Histogram | Bell-shaped around 0 | Skewness or heavy tails |\n",
    "| Prediction Scatter | Tight to diagonal | Wide spread |\n",
    "| Prediction Band | Tight bands | Wide bands or many escapes |\n",
    "\n",
    "---\n",
    "\n",
    "> ✅ Good behavior in all plots increases confidence in your ML model performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# --- Make sure index is restored for train and test predictions ---\n",
    "y_train.index = X_train.index\n",
    "y_test.index = X_test.index\n",
    "\n",
    "# --- Create Prediction Series aligned ---\n",
    "y_pred_close_train_series = pd.Series(y_pred_close_train, index=y_train.index)\n",
    "y_pred_close_test_series = pd.Series(y_pred_close_test, index=y_test.index)\n",
    "\n",
    "# --- Calculate Residuals (errors) ---\n",
    "train_residuals = y_train - y_pred_close_train_series\n",
    "test_residuals = y_test - y_pred_close_test_series\n",
    "\n",
    "# ==========================================================\n",
    "# 1. Residuals vs Fitted Plot\n",
    "# ==========================================================\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.scatter(y_pred_close_train_series, train_residuals, color='blue', label='Train Residuals', alpha=0.5)\n",
    "plt.scatter(y_pred_close_test_series, test_residuals, color='orange', label='Test Residuals', alpha=0.5)\n",
    "plt.axhline(0, linestyle='--', color='black')\n",
    "plt.title('Residuals vs Fitted Values (Train and Test)')\n",
    "plt.xlabel('Fitted (Predicted Close Price)')\n",
    "plt.ylabel('Residuals (Error)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ➡️ Purpose: if model is good, residuals should be randomly scattered around zero.\n",
    "\n",
    "# ==========================================================\n",
    "# 2. Prediction Error Histogram\n",
    "# ==========================================================\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.histplot(train_residuals, color='blue', label='Train Residuals', kde=True, stat=\"density\", bins=30)\n",
    "sns.histplot(test_residuals, color='orange', label='Test Residuals', kde=True, stat=\"density\", bins=30)\n",
    "plt.axvline(0, linestyle='--', color='black')\n",
    "plt.title('Prediction Error Distribution (Train vs Test)')\n",
    "plt.xlabel('Residuals (Error)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ➡️ Purpose: Check if errors are normally distributed (good sign!).\n",
    "\n",
    "# ==========================================================\n",
    "# 3. Predicted vs Actual Scatter Plot\n",
    "# ==========================================================\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.scatter(y_train, y_pred_close_train_series, color='blue', label='Train', alpha=0.5)\n",
    "plt.scatter(y_test, y_pred_close_test_series, color='orange', label='Test', alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], color='black', linestyle='--', label='Ideal Line')\n",
    "plt.title('Predicted vs Actual Close Prices')\n",
    "plt.xlabel('Actual Close Price')\n",
    "plt.ylabel('Predicted Close Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ➡️ Purpose: Points close to diagonal means better model.\n",
    "\n",
    "# ==========================================================\n",
    "# 4. Optional: Prediction Uncertainty Bands (Simple Version)\n",
    "# ==========================================================\n",
    "\n",
    "# Only approximate basic bands (not full Bayesian confidence)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(y_test.index, y_test, label='Actual Close Test', color='blue')\n",
    "plt.plot(y_test.index, y_pred_close_test_series, label='Predicted Close Test', color='red', linestyle='--')\n",
    "# Fake simple \"uncertainty\" bands (+/- 1 standard deviation of test residuals)\n",
    "std_test_residual = test_residuals.std()\n",
    "plt.fill_between(y_test.index, y_pred_close_test_series - std_test_residual, y_pred_close_test_series + std_test_residual, color='gray', alpha=0.3, label='Prediction ±1 STD')\n",
    "plt.title('Test Set Predictions with Uncertainty Band')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ➡️ Purpose: See approximate model uncertainty visually (larger bands = lower confidence).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔥 Feature Importance Analysis\n",
    "\n",
    "- **Native Feature Importance**: Based on how much each feature reduces node impurity in the Random Forest trees.\n",
    "- **Permutation Importance**: Measures the performance drop when a feature's values are randomly shuffled, providing a model-agnostic view.\n",
    "- This helps understand which features the model relies on the most for predicting stock price and volatility.\n",
    "\n",
    "We visualize the **Top 20 Most Important Features** based on both methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 📦 8. Feature Importance Analysis (Random Forest Models)\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Make sure you have the feature names ready\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# --- 8.1 Native Feature Importance (from Random Forest) ---\n",
    "\n",
    "# Example for Close Price Model\n",
    "importances = model_close.feature_importances_\n",
    "\n",
    "# Create DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display\n",
    "display(feature_importance_df.head(20))  # Top 20 important features\n",
    "\n",
    "# --- 8.2 Plot Feature Importance ---\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=feature_importance_df.head(30), x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Top 30 Feature Importances for Close Price Model')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 8.3 Bonus: Permutation Importance ---\n",
    "\n",
    "perm_importance = permutation_importance(model_close, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "perm_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Permutation_Importance': perm_importance.importances_mean\n",
    "}).sort_values(by='Permutation_Importance', ascending=False)\n",
    "\n",
    "# Display\n",
    "display(perm_df.head(20))\n",
    "\n",
    "# --- Plot Permutation Importance ---\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=perm_df.head(30), x='Permutation_Importance', y='Feature', palette='rocket')\n",
    "plt.title('Top 30 Permutation Importances for Close Price Model')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
