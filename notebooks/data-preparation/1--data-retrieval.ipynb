{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval\n",
    "\n",
    "In this notebook, we collect the relevant data sets from different sources and store them to disk. Besides of renaming the columns, no cleaning tasks are performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import holidays\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from fredapi import Fred\n",
    "\n",
    "from signal_sigma.config.cfg_legacy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "NB_NUMBER = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocks from `yfinance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = \"2000-01-01\"\n",
    "END_DATE = \"2025-04-22\"\n",
    "INTERVAL = \"1d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in STOCK_TICKERS:\n",
    "    time.sleep(0.123)  \n",
    "    \n",
    "    df = yf.download(\n",
    "        ticker,\n",
    "        start=START_DATE,\n",
    "        end=END_DATE,\n",
    "        interval=INTERVAL,\n",
    "        # auto_adjust=True,\n",
    "    )\n",
    "\n",
    "    # Get rid of the two-level column name scheme\n",
    "    # (one level only indicates the tickler).\n",
    "    df.columns = df.columns.get_level_values(0).rename(None)\n",
    "\n",
    "    df = df.reset_index(names=\"date\")\n",
    "    df.index.name = IDX\n",
    "\n",
    "    # Convert column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    csvpath_rel = os.path.join(DATA_STOCKS_DIR_RELPATH, ticker.lower() + \".csv\")\n",
    "    store_df_as_csv(df, csvpath_rel, NB_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Indices/Indexoids from `yfinance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE_YF = \"2000-01-01\"\n",
    "END_DATE_YF = \"2025-04-23\"\n",
    "\n",
    "# Macro indicators and market indices\n",
    "MACRO_TICKERS = {\n",
    "    # Indices\n",
    "    \"^GSPC\": \"S&P500_Index\",\n",
    "    \"^DJI\": \"Dow_Jones_Index\",\n",
    "    \"^IXIC\": \"NASDAQ_Composite\",\n",
    "    \"^RUT\": \"Russell2000_Index\",\n",
    "    \"^VIX\": \"VIX_Index\",\n",
    "    # Commodities\n",
    "    \"DX-Y.NYB\": \"Dollar_Index_DXY\",\n",
    "    \"GC=F\": \"Gold_Futures\",\n",
    "    \"CL=F\": \"WTI_Oil_Futures\",\n",
    "    \"HG=F\": \"Copper_Futures\",\n",
    "    \"BZ=F\": \"Brent_Crude_Futures\",\n",
    "    # Sector ETFs (Proxies)\n",
    "    \"XLK\": \"Tech_Sector_ETF\",\n",
    "    \"XLE\": \"Energy_Sector_ETF\",\n",
    "    \"XLF\": \"Financial_Sector_ETF\",\n",
    "    \"XLY\": \"ConsumerDiscretionary_ETF\",\n",
    "    # Other Market Metrics\n",
    "    \"LIT\": \"Lithium_ETF\",\n",
    "    \"SMH\": \"Semiconductor_ETF\",\n",
    "    \"XLU\": \"Electricity_Proxy\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output DataFrame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Download data\n",
    "for ticker, label in MACRO_TICKERS.items():\n",
    "    # NOTE: Althoug yfinance provides open/close and min/max values\n",
    "    # for each indicator, we only incorporate the close value.\n",
    "    df_tmp = yf.download(ticker, start=START_DATE_YF, end=END_DATE_YF)\n",
    "    df[label] = df_tmp[\"Close\"]\n",
    "\n",
    "# Drop completely empty columns (failed downloads).\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# Get rid of the two-level column name scheme.\n",
    "df.columns = df.columns.get_level_values(0).rename(None)\n",
    "\n",
    "df = df.reset_index(names=\"date\")\n",
    "df.index.name = IDX\n",
    "\n",
    "# Convert column names to lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "relpath_df = DATA_YF_MIF_RELPATH\n",
    "store_df_as_csv(df, relpath_df, NB_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Alternative that would store (similar to stocks)\n",
    "# each indicator with open/end and high/low in a \n",
    "# separate file\n",
    "# TODO: Set up data processing type in complete analogy to\n",
    "# the stock case (or simply merge the ticker lists ...)\n",
    "\n",
    "for ticker in MACRO_TICKERS:\n",
    "    time.sleep(0.123)  \n",
    "    df = yf.download(ticker, start=START_DATE_YF, end=END_DATE_YF)\n",
    "\n",
    "    # Get rid of the two-level column name scheme\n",
    "    # (one level only indicates the tickler).\n",
    "    df.columns = df.columns.get_level_values(0).rename(None)\n",
    "\n",
    "    df = df.reset_index(names=\"date\")\n",
    "    df.index.name = IDX\n",
    "\n",
    "    # Convert column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # csvpath_rel = os.path.join(DIR_DATA_INDICATORS, ticker.lower() + \".csv\")\n",
    "    # store_df_as_csv(df, csvpath_rel, NB_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Indicators from `fred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY_FED = os.getenv(\"FRED_API_KEY\")\n",
    "\n",
    "INDICATOR_TICKERS = {\n",
    "    # Consumer Price Index (CPI): Measures inflation and purchasing power.\n",
    "    \"CPIAUCSL\": \"cpi\",\n",
    "    # Federal Funds Rate: Target rate for overnight lending between banks.\n",
    "    \"FEDFUNDS\": \"fed_rate\",\n",
    "    # Crude Oil Prices: Reflects energy costs and global economic conditions.\n",
    "    \"DCOILWTICO\": \"oil\",\n",
    "    # Gross Domestic Product (GDP): Measures overall economic activity and growth.\n",
    "    \"GDP\": \"gdp\",\n",
    "    # Nonfarm Payrolls: Number of jobs added or lost in the economy.\n",
    "    \"PAYEMS\": \"nonfarm_payrolls\",\n",
    "    # 10-Year Treasury Yield: Reflects long-term interest rates.\n",
    "    \"DGS10\": \"treasury_yield\",\n",
    "    # Industrial Production Index: Measures output of industrial sectors.\n",
    "    \"INDPRO\": \"industrial_production\",\n",
    "    # Retail Sales: Reflects consumer spending and economic health.\n",
    "    \"RSXFS\": \"retail_sales\",\n",
    "    # Manufacturing PMI: Indicates business conditions in the manufacturing sector.\n",
    "    \"MANEMP\": \"pmi\",\n",
    "    # Consumer Confidence Index: Reflects consumer sentiment and spending outlook.\n",
    "    \"UMCSENT\": \"consumer_confidence\",\n",
    "}\n",
    "\n",
    "START_DATE = \"2000-01-01\"\n",
    "END_DATE = \"2025-04-22\"\n",
    "INTERVAL = \"1d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Fred object using the API key\n",
    "fred = Fred(api_key=API_KEY_FED)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for id, name in INDICATOR_TICKERS.items():\n",
    "    # Fetch the data for the current indicator using its series ID\n",
    "    data = fred.get_series(id)\n",
    "    # Resample to daily frequency and forward fill missing values\n",
    "    data = data.resample(\"D\").ffill()\n",
    "    # Add the data as a column to the DataFrame using the indicator name as the column name\n",
    "    df[name] = data\n",
    "\n",
    "df = df.reset_index(names=\"date\")\n",
    "df.index.name = IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter-out non-working day rows\n",
    "\n",
    "# NOTE: Further date-related features for the merged data frame\n",
    "# will be introduced in the feature engineering notebook.\n",
    "\n",
    "# XXX: Does it make a difference to first filter out non-working days\n",
    "# and then to perform an inner join for all frames or first to perform\n",
    "# an outer join and then to filter out non-working days?\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df[df[\"date\"] >= pd.to_datetime(START_DATE)]\n",
    "\n",
    "# 0 = Monday, ..., 6 = Sunday.\n",
    "df[\"weekday\"] = df[\"date\"].dt.weekday\n",
    "\n",
    "# US holidays indicator\n",
    "us_holidays = holidays.US()\n",
    "df[\"is_holiday\"] = df[\"date\"].apply(lambda dt: dt in us_holidays)\n",
    "\n",
    "# Working day indicator\n",
    "df[\"is_trading_day\"] = ~df[\"is_holiday\"] & ~df[\"weekday\"].isin([5, 6])\n",
    "\n",
    "# Filter the DataFrame to include only working days for analysis purposes\n",
    "df = df[df[\"is_trading_day\"]]\n",
    "\n",
    "relpath_df = DATA_FED_CEI_RELPATH\n",
    "store_df_as_csv(df, relpath_df, NB_NUMBER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
