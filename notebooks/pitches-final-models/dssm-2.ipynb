{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ccf888",
   "metadata": {},
   "source": [
    "# Deep State Space Models (DSSMs) -- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c34075",
   "metadata": {},
   "source": [
    "**WARNING:** Please do not read this notebook. This notebook is simply a conglomeration of unordered code cells. Particularly, it does not (yet) contain an implementation of DSSMs that works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a69d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "# Target Stock Ticker\n",
    "TARGET_TICKER = \"TSLA\"\n",
    "\n",
    "# Dimensionality of the latent state\n",
    "# Commonly used values are 10, 20, 50.\n",
    "STATE_DIM = 10\n",
    "\n",
    "# Sliding window length\n",
    "SW_LENGTH = 60\n",
    "\n",
    "# Learning rate for the Model\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4fde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load\n",
    "\n",
    "df = pd.read_csv(\"../data/processed_combined_data.csv\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "targets = [f\"close_{TARGET_TICKER}\"]\n",
    "non_features = [*targets, \"date\"]\n",
    "\n",
    "X_cols = [col for col in df.columns if col not in non_features]\n",
    "y_cols = targets\n",
    "\n",
    "# X_df = df[X_cols]\n",
    "X_df = df[[\"close_NVDA\", \"oil\"]]\n",
    "y_df = df[y_cols]\n",
    "\n",
    "# As PyTorch wants the np type to be definitive\n",
    "# and the np dtype is inferred from the df dtype,\n",
    "# the following line is needed to ensure the dtype is float64\n",
    "X_df = X_df.astype(np.float64) \n",
    "# X_df = X_df.select_dtypes(include=[np.number]) \n",
    "\n",
    "X_np = X_df.to_numpy().astype(np.float64)\n",
    "y_np = y_df.to_numpy().astype(np.float64)\n",
    "\n",
    "X_np.shape, y_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5612bac",
   "metadata": {},
   "source": [
    "## PyTorch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing for PyTorch\n",
    "\n",
    "# We must provide writeable arrays to PyTorch with definite dtypes (here, float32).\n",
    "\n",
    "# Size of X_seq shall be (n_samples - SW_LENGTH + 1, SW_LENGTH, n_features).\n",
    "X_seq = np.lib.stride_tricks.sliding_window_view(X_np, window_shape=SW_LENGTH, axis=0)\n",
    "X_seq = X_seq.transpose(0, 2, 1)\n",
    "X_seq = np.array(X_seq, dtype=np.float32)\n",
    "\n",
    "# Size of y_seq shall be (n_samples - SW_LENGTH + 1, 1).\n",
    "y_seq = y_np[SW_LENGTH - 1 :].reshape(-1, 1)\n",
    "y_seq = np.array(y_seq, dtype=np.float32)\n",
    "\n",
    "# Conversion to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_seq)\n",
    "y_tensor = torch.tensor(y_seq)\n",
    "\n",
    "X_tensor.shape, y_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d82bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSSM Architecture\n",
    "\n",
    "\n",
    "class DSSM(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "    ) -> None:\n",
    "        super(DSSM, self).__init__()\n",
    "        # Trainable parameters can be defined as nn.Parameter.\n",
    "        # State transition matrix\n",
    "        self.A = nn.Parameter(torch.randn(state_dim, state_dim))\n",
    "        # Control matrix\n",
    "        self.B = nn.Parameter(torch.randn(state_dim, input_dim))\n",
    "        # Output matrix\n",
    "        self.C = nn.Parameter(torch.randn(output_dim, state_dim))\n",
    "        # Transition matrix\n",
    "        self.D = nn.Parameter(torch.randn(output_dim, input_dim))\n",
    "\n",
    "    # The forward function turns the model into a callable object\n",
    "    # and will be executed if the model is called.\n",
    "    def forward(\n",
    "        self,\n",
    "        s_prev: torch.Tensor,\n",
    "        x_t: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # State update\n",
    "        s_t = torch.matmul(self.A, s_prev) + torch.matmul(self.B, x_t)\n",
    "        # Output calculation\n",
    "        y_t = torch.matmul(self.C, s_t) + torch.matmul(self.D, x_t)\n",
    "        return s_t, y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np.shape, y_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize the prices so our training is more stable\n",
    "# prices_norm = (prices - np.mean(prices)) / np.std(prices)\n",
    "\n",
    "\n",
    "X_tensor = torch.tensor(X_np)\n",
    "y_tensor = torch.tensor(y_np)\n",
    "\n",
    "# --------------------------------\n",
    "# 3. Train-Test Split (80/20 split)\n",
    "# --------------------------------\n",
    "train_size = int(0.8 * X_tensor.shape[0])\n",
    "X_np_train = X_np[:train_size]\n",
    "y_np_train = y_np[:train_size]\n",
    "X_np_test = X_np[train_size:]\n",
    "y_np_test = y_np[train_size:]\n",
    "\n",
    "# Convert to PyTorch tensors.\n",
    "X_train = torch.tensor(X_np_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_np_train, dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_np_test, dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_np_test, dtype=torch.float32)\n",
    "\n",
    "# Normalize the data\n",
    "\n",
    "# Compute mean and std using only training data\n",
    "X_train_mean = X_train.mean(dim=0)\n",
    "X_train_std = X_train.std(dim=0)\n",
    "\n",
    "# Normalize training data\n",
    "X_train_norm = (X_train - X_train_mean) / X_train_std\n",
    "\n",
    "# Normalize test data using training statistics\n",
    "X_test_norm = (X_test - X_train_mean) / X_train_std\n",
    "\n",
    "# y_mean = y_train.mean(dim=0)\n",
    "# y_std = y_train.std(dim=0)\n",
    "\n",
    "# y_train_norm = (y_train - y_mean) / y_std\n",
    "# y_test_norm = (y_test - y_mean) / y_std  # Use training stats\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Train the Model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "state_dim = STATE_DIM\n",
    "input_dim = X_tensor.shape[-1]\n",
    "output_dim = y_tensor.shape[-1]\n",
    "model = DSSM(state_dim, input_dim, output_dim)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0.0\n",
    "    # Initialize the state for the training sequence.\n",
    "    s = torch.zeros(state_dim, 1)\n",
    "    \n",
    "    # Iterate sequentially over the training data.\n",
    "    for i in range(train_size):\n",
    "        x_t = X_train_norm[i].view(input_dim, 1)   # current feature vector.\n",
    "        s, y_pred = model(s, x_t)\n",
    "        # Teacher forcing: use the true stock price as target.\n",
    "        loss += criterion(y_pred, y_train[i].view(output_dim, 1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Multi-Step Forecast on Test Data\n",
    "# -----------------------------------------------------------------------------\n",
    "# We want to see how well the model predicts a stock sample path.\n",
    "# We'll use the external test features (from X_test) and carry forward the state.\n",
    "\n",
    "# First, obtain the final state from the training dataset by replaying the training inputs.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    s = torch.zeros(state_dim, 1)\n",
    "    for i in range(train_size):\n",
    "        x_t = X_train[i].view(input_dim, 1)\n",
    "        s, _ = model(s, x_t)\n",
    "    \n",
    "    # Now, generate predictions over the test period.\n",
    "    predictions = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x_t = X_test[i].view(input_dim, 1)\n",
    "        s, y_pred = model(s, x_t)\n",
    "        predictions.append(y_pred.item())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    actual = y_test.squeeze().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Comparison: Plot Predicted vs. Actual Stock Sample Paths\n",
    "# -----------------------------------------------------------------------------\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(predictions, label=\"Predicted Stock Price\")\n",
    "plt.plot(actual, label=\"Actual Stock Price\", alpha=0.7)\n",
    "plt.xlabel(\"Time Step in Test Set\")\n",
    "plt.ylabel(\"Normalized Stock Price\")\n",
    "plt.title(\"Multi-Step Forecast vs. Actual Stock Sample Path\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6418b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialisation and Training Setup\n",
    "\n",
    "state_dim = STATE_DIM\n",
    "input_dim = X_tensor.shape[-1]\n",
    "output_dim = y_tensor.shape[-1]\n",
    "\n",
    "model = DSSM(\n",
    "    state_dim,\n",
    "    input_dim,\n",
    "    output_dim,\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initialize the state as zeros\n",
    "s_prev = torch.zeros(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3232aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "# Number of future steps to predict\n",
    "FUTURE_STEPS = 10\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Use the last observed return as the starting input.\n",
    "x_now = X_tensor[-1, -1, :]\n",
    "\n",
    "for i in range(FUTURE_STEPS):\n",
    "    # Forward pass using the last state and input.\n",
    "    s_now, y_now = model(s_prev, x_now)\n",
    "    predictions.append(y_now.item())\n",
    "    # Update state and input.\n",
    "    s_prev = s_now.detach()\n",
    "    # x_now = y_now.detach()\n",
    "\n",
    "# print(\"Future Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce657616",
   "metadata": {},
   "source": [
    "## Tensorflow/Keras Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing for TensorFlow\n",
    "\n",
    "# We must provide writeable arrays to PyTorch with definite dtypes (here, float32).\n",
    "\n",
    "# Size of X_seq shall be (n_samples - SW_LENGTH + 1, SW_LENGTH, n_features).\n",
    "X_seq = np.lib.stride_tricks.sliding_window_view(X_np, window_shape=SW_LENGTH, axis=0)\n",
    "X_seq = X_seq.transpose(0, 2, 1)\n",
    "X_seq = np.array(X_seq, dtype=np.float32)\n",
    "\n",
    "# Size of y_seq shall be (n_samples - SW_LENGTH + 1, 1).\n",
    "y_seq = y_np[SW_LENGTH - 1 :].reshape(-1, 1)\n",
    "y_seq = np.array(y_seq, dtype=np.float32)\n",
    "\n",
    "# Conversion to PyTorch tensors\n",
    "X_tensor = X_seq\n",
    "y_tensor = y_seq\n",
    "\n",
    "X_tensor.shape, y_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b291361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSSM Architecture\n",
    "\n",
    "\n",
    "class DSSM(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "    ) -> None:\n",
    "        super(DSSM, self).__init__()\n",
    "\n",
    "        # Initialize learnable state-space matrices.\n",
    "        # Transition matrix\n",
    "        self.A = tf.Variable(\n",
    "            tf.random.normal([state_dim, state_dim]), trainable=True, name=\"A\"\n",
    "        )\n",
    "        # Control matrix\n",
    "        self.B = tf.Variable(\n",
    "            tf.random.normal([state_dim, input_dim]), trainable=True, name=\"B\"\n",
    "        )\n",
    "        # Output matrix\n",
    "        self.C = tf.Variable(\n",
    "            tf.random.normal([output_dim, state_dim]), trainable=True, name=\"C\"\n",
    "        )\n",
    "        # Transition matrix\n",
    "        self.D = tf.Variable(\n",
    "            tf.random.normal([output_dim, input_dim]), trainable=True, name=\"D\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Unpack the inputs\n",
    "        s_prev, x_t = inputs  \n",
    "        # State update\n",
    "        s_t = tf.matmul(self.A, s_prev) + tf.matmul(self.B, x_t)\n",
    "        # Output calculatuon\n",
    "        y_t = tf.matmul(self.C, s_t) + tf.matmul(self.D, x_t)\n",
    "        return s_t, y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d397d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dimension setup (adjust based on your data shapes):\n",
    "state_dim = 10\n",
    "input_dim = X_tensor.shape[-1]  # e.g., number of features per time step\n",
    "output_dim = y_tensor.shape[-1]  # e.g., 1 for a single stock return\n",
    "\n",
    "# Initialize the DSSM model with a tanh activation.\n",
    "model = DSSM(state_dim, input_dim, output_dim, activation=tf.keras.activations.tanh)\n",
    "\n",
    "# Define a loss function and an optimizer.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of future steps to predict.\n",
    "FUTURE_STEPS = 10  \n",
    "predictions = []\n",
    "\n",
    "# In a forecasting scenario, you'll have an initial state and an initial input.\n",
    "# Here, we assume the state is initialized to zeros.\n",
    "s_prev = tf.zeros([state_dim, 1], dtype=tf.float32)\n",
    "\n",
    "# For the starting input, decide whether you want to use the last observed price (from y_tensor)\n",
    "# or the last available feature vector (from X_tensor). For demonstration, let's use y_tensor.\n",
    "# If y_tensor is (num_samples, 1), take the last row and reshape as (1, 1).\n",
    "x_now = tf.reshape(y_tensor[-1], (1, 1))\n",
    "\n",
    "for _ in range(FUTURE_STEPS):\n",
    "    # Forward pass: model takes state and last_input.\n",
    "    s_now, y_now = model([s_prev, x_now])\n",
    "    \n",
    "    # Append prediction (convert tensor to scalar float)\n",
    "    predictions.append(y_now.numpy().item())\n",
    "    \n",
    "    # Update state: detach gradients to prevent backprop through time.\n",
    "    s_prev = tf.stop_gradient(s_now)\n",
    "    \n",
    "    # Update last_input with the predicted output.\n",
    "    # Note: Ensure the shape matches the model's expectation.\n",
    "    x_now = tf.reshape(tf.stop_gradient(y_now), (1, 1))\n",
    "    \n",
    "print(\"Future Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_forecast(\n",
    "    model: DSSM,\n",
    "    initial_state: torch.Tensor,\n",
    "    forecast_horizon: int,\n",
    "    autoregressive: bool = False,\n",
    "    start_input: torch.Tensor = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate a multi-step forecast with the DSSM model.\n",
    "\n",
    "    Parameters:\n",
    "      model (DSSM): Instance of the DSSM model.\n",
    "      initial_state (torch.Tensor): Initial state with shape [state_dim, 1].\n",
    "      forecast_horizon (int): Number of time steps to forecast.\n",
    "      autoregressive (bool): If True, uses model prediction as the next input.\n",
    "      start_input (torch.Tensor, optional): The initial input for autoregressive mode \n",
    "            (shape [input_dim, 1]), required if autoregressive is True.\n",
    "\n",
    "    Returns:\n",
    "      torch.Tensor: Stacked forecast outputs with shape [forecast_horizon, output_dim, 1].\n",
    "    \"\"\"\n",
    "    predictions = []  # Collect forecast outputs for each step.\n",
    "    s_t = initial_state  # Starting state (shape: [state_dim, 1])\n",
    "\n",
    "    if start_input is None:\n",
    "        raise ValueError(\"start_input must be provided for autoregressive forecasting.\")\n",
    "    x = start_input.view(-1, 1)\n",
    "    for _ in range(forecast_horizon):\n",
    "        s_t, y_t = model(s_t, x)\n",
    "        predictions.append(y_t)\n",
    "        # Replace x with the model's output for the next step.\n",
    "        # This assumes output_dim == input_dim.\n",
    "        x = y_t\n",
    "    \n",
    "    # Stack the predictions into a single tensor of shape [forecast_horizon, output_dim, 1].\n",
    "    return torch.stack(predictions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = STATE_DIM\n",
    "input_dim = X_tensor.shape[-1]\n",
    "output_dim = y_tensor.shape[-1]\n",
    "\n",
    "# Instantiate the DSSM model.\n",
    "model = DSSM(state_dim, input_dim, output_dim)\n",
    "\n",
    "# Define an initial state (e.g., zeros).\n",
    "initial_state = torch.zeros(state_dim, 1)\n",
    "\n",
    "# Set forecast horizon.\n",
    "forecast_horizon = 5\n",
    "\n",
    "start_input = torch.randn(input_dim, 1)\n",
    "forecast_autoregressive = multi_step_forecast(\n",
    "    model,\n",
    "    initial_state,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    autoregressive=True,\n",
    "    start_input=start_input\n",
    ")\n",
    "print(\"\\nMulti-step forecast (autoregressive):\")\n",
    "print(forecast_autoregressive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ea41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train-Test Split\n",
    "\n",
    "# L = y_0.shape[0]\n",
    "\n",
    "# tts_chunk_size = TTS_CHUNK_SIZE\n",
    "\n",
    "# # Create overlapping windows\n",
    "# X = np.lib.stride_tricks.sliding_window_view(\n",
    "#     X_0,\n",
    "#     window_shape=tts_chunk_size,\n",
    "#     axis=0,\n",
    "# )[:-1]\n",
    "# y = y_0[tts_chunk_size:]\n",
    "\n",
    "# X.shape, X_0.shape, y.shape, y_0.shape\n",
    "# # X, y = create_sequences(scaled_data, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scaling\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_np_ = scaler.fit_transform(X_np.reshape(-1, 1))\n",
    "# y_0 = y_0\n",
    "\n",
    "X_np.shape, y_np.shape\n",
    "# y_0.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DSSMForecast(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple DSSM-style model for stock forecasting.\n",
    "\n",
    "    The model consists of two towers:\n",
    "      - stock_branch: processes historical stock features.\n",
    "      - aux_branch: processes auxiliary (e.g., market indicator) features.\n",
    "      \n",
    "    Their latent representations are concatenated and fed into a final\n",
    "    fully connected layer to produce the forecasted stock value.\n",
    "    \"\"\"\n",
    "    def __init__(self, stock_input_dim, aux_input_dim, embed_dim, hidden_dim):\n",
    "        super(DSSMForecast, self).__init__()\n",
    "        \n",
    "        # Branch to process historical stock features.\n",
    "        self.stock_branch = nn.Sequential(\n",
    "            nn.Linear(stock_input_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Branch to process auxiliary market indicators.\n",
    "        self.aux_branch = nn.Sequential(\n",
    "            nn.Linear(aux_input_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # The final fully connected network combines the two embeddings.\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Output a single forecast value (e.g., next day's closing price)\n",
    "        )\n",
    "    \n",
    "    def forward(self, stock_input, aux_input):\n",
    "        # Process each input branch separately.\n",
    "        stock_embed = self.stock_branch(stock_input)  # Shape: [batch, embed_dim]\n",
    "        aux_embed = self.aux_branch(aux_input)          # Shape: [batch, embed_dim]\n",
    "        \n",
    "        # Concatenate the embeddings from both branches.\n",
    "        combined = torch.cat((stock_embed, aux_embed), dim=1)  # Shape: [batch, 2*embed_dim]\n",
    "        \n",
    "        # Generate the final forecast.\n",
    "        output = self.fc(combined)\n",
    "        return output\n",
    "\n",
    "# Example usage of the model:\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration parameters.\n",
    "    batch_size = 32\n",
    "    stock_input_dim = 10    # For instance, features like open, high, low, close, volume, etc.\n",
    "    aux_input_dim = 5       # For instance, sentiment scores, technical indicators, etc.\n",
    "    embed_dim = 64          # Dimension of the latent space for each branch.\n",
    "    hidden_dim = 32         # Intermediate hidden dimension for combining the embeddings.\n",
    "    \n",
    "    # Instantiate the model.\n",
    "    model = DSSMForecast(stock_input_dim, aux_input_dim, embed_dim, hidden_dim)\n",
    "    \n",
    "    # Define a mean squared error loss function (typical for regression tasks).\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Use an optimizer such as Adam.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Dummy training loop (replace with your own dataset and dataloader in practice).\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        # Generate dummy data for demonstration:\n",
    "        stock_data = torch.randn(batch_size, stock_input_dim)\n",
    "        aux_data = torch.randn(batch_size, aux_input_dim)\n",
    "        target = torch.randn(batch_size, 1)  # Dummy target values (e.g., next-day closing prices)\n",
    "        \n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model.\n",
    "        outputs = model(stock_data, aux_data)\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        # Backward pass: compute gradient and update weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
