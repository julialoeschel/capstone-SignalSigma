{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9fe731",
   "metadata": {},
   "source": [
    "# Deep State Space Models (DSSMs) -- Theoretical Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a630c24",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "A **deep state space model** (DSSM) merges the principles of **state space models** (SSMs) into **deep neural networks** (DNNs) to create a framework for sequential data modeling.\n",
    "\n",
    "First, the theoretical foundations are established. Then, a DSSM is implemented and evaluated for forecasting the stock price of TSLA.\n",
    "\n",
    "**RESULTS:**\n",
    "\n",
    "- Metrics\n",
    "  - RMSE: ??.??\n",
    "  - MAE:  ??.??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1938467",
   "metadata": {},
   "source": [
    "## DSSMs in a Nutshell\n",
    "\n",
    "DSSMs integrate the latent state representations of SSMs into sequential predictions. The latent state of an SSM should encode information about underlying hidden factors (in the context of stock market prices, these could include bull/bear phases, volatility shifts, ...)\n",
    "\n",
    "Like many models, it is promised to be able to detect hidden trends before they emerge in price movements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05e8ad",
   "metadata": {},
   "source": [
    "## (Continuous) SSMs\n",
    "\n",
    "The concept of SSMs originates from system/control theory in engineering.\n",
    "\n",
    "A (continuous-time) SSM is defined by four matrices:\n",
    "\n",
    "- $A \\in \\mathbb R^{p \\times p}$: (Latent) State matrix.\n",
    "- $B \\in \\mathbb R^{p \\times m}$: Control matrix.\n",
    "- $C \\in \\mathbb R^{n \\times p}$: Output matrix.\n",
    "- $D \\in \\mathbb R^{n \\times m}$: Transition matrix (**skip connection**).\n",
    "\n",
    "For a given input sample path $x(t) \\in \\mathbb R^{m}$, the SSM computes the corresponding output sample path $y(t) \\in \\mathbb R^{m}$ by solving\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dot s(t) & = A s(t) + B x(t) \\\\\n",
    "y (t) & = C s(t) + D x(t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $s(t) \\in \\mathbb R^{p}$ represents the internal (latent) **state sample path**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1aad4",
   "metadata": {},
   "source": [
    "![SSM in Continous Time](./img/ssm-continuous.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72cd9e9",
   "metadata": {},
   "source": [
    "## DSSMs as Discretisation of a Continuous SSM\n",
    "\n",
    "Computers cannot handle ordinary differential equations (ODEs) in continuous time directly, they must be discretised.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "s_t & = A s_{t-1} + B x_t \\\\\n",
    "y_t & = C s_t + D x_t\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "**CAVEAT:** The matrices $A$, $B$, $C$, $D$ are not exactly the same as those in the continuous time equation system!\n",
    "\n",
    "DSSMs incorporate skip connections: The input influences directly both the state and the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf567ad4",
   "metadata": {},
   "source": [
    "![DSSM Recurrent](./img/dssm-recurrent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61afec7a",
   "metadata": {},
   "source": [
    "## DSSMs vs. (Continuous) SSMs\n",
    "\n",
    "### Input and Output Quantities\n",
    "\n",
    "- A DSSM sequentially processes discrete-time vectors (similar to other recurrent neural networks (RNNs)).\n",
    "- An SSM turns continuous sample paths into continuous sample paths. \n",
    "\n",
    "### Roles of Parameters/Variables\n",
    "\n",
    "| Quantity | Machine Learning | Control Theory |\n",
    "| --- | --- | --- |\n",
    "| $m$ | determined by $x_t$ | determined by $x(t)$ | \n",
    "| $n$ | determined by $y_t$ | determined by $y(t)$ | \n",
    "| $x_t$, $x(t)$ | given (training data) | wanted |\n",
    "| $y_t$, $y(t)$ | given (training data) | given |\n",
    "| $p$ | hyperparameter | given |\n",
    "| $A$, $B$, $C$, $D$ | wanted (training parameters) | given |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae6f4f0",
   "metadata": {},
   "source": [
    "## DSSMs vs. LSTMs\n",
    "\n",
    "Like LSTMs, DSSMs internally capture  temporal dependencies across time steps. However:\n",
    "\n",
    "- DSSMs use the discretisation of a linear system of ODEs.\n",
    "- LSTMs use many (non-linear) activation functions inside cells (see graphic below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b5322",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![LSTM cell](./img/lstm-cell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf981e8",
   "metadata": {},
   "source": [
    "# DSSMs -- Recurrent or Convolutional?\n",
    "\n",
    "By a bit of algebra, one can derive from the defining system of equations that\n",
    "\n",
    "$$\n",
    "y_k = \\left( C A^k B u_0 + C A^{k-1} B u_1 + \\cdots + C A B u_{k-1} + C B u_k \\right) + D u_k.\n",
    "$$\n",
    "\n",
    "Now, the expression in parentheses can be written as convolution $K * u$ with the so-called SSM kernel\n",
    "\n",
    "$$\n",
    "K = (C B, CAB, \\ldots, CA^{i}B, \\ldots).\n",
    "$$\n",
    "\n",
    "Thus, (continuous-time) SSMs can also be interpreted in terms of convolutional neural networks (CNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc07cc37",
   "metadata": {},
   "source": [
    "![DSSM Convolutional](./img/dssm-convolutional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00598040",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "## State Prediction\n",
    "\n",
    "It is said that in some cases, predicting the state is more practical than directly predicting the output. This approach engages estimators like **Kálmán filters** or **Bayesian filters** for state estimation.\n",
    "\n",
    "## Forecasting\n",
    "\n",
    "DSSMs can be engaged for forecasting by applying positive or negative lags to the original features or the target variable with missing values imputed as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c0934",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Gu, Albert et al.: *Structured State Space: Combining Continuous-Time, Recurrent, and Convolutional Models* (2022)](https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3)\n",
    "- [Bourdois, Loïck: *Introduction to State Space Models (SSM)* (2024)](https://huggingface.co/blog/lbourdois/get-on-the-ssm-train)\n",
    "- [Rangapuram, Syama Sundar et al.: *Deep State Space Models for Time Series Forecasting* (2018)](https://papers.nips.cc/paper_files/paper/2018/file/5cf68969fb67aa6082363a6d4e6468e2-Paper.pdf)\n",
    "- [Turing, Janelle: *Advanced Time Series Analysis: State Space Models and Kalman Filtering* (2023)](https://janelleturing.medium.com/advanced-time-series-analysis-state-space-models-and-kalman-filtering-3b7eb7157bf2)\n",
    "- [Murphy, Kevin and Linderman, Scott et al.: *State Space Models: A Modern Approach*. Chapter SSM](https://probml.github.io/ssm-book/chapters/ssm/ssm_index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a8c14",
   "metadata": {},
   "source": [
    "![dss](./img/dssm-strikethrough.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b8aa9d",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
