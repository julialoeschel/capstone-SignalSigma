{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import holidays \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install missingn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import missingno as msno\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = '2012-12-30'\n",
    "\n",
    "path_stock = \"../data/stock\"\n",
    "path_fed = \"../data/fed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 Tech Companies based on Market Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_df = pd.read_csv(f\"{path_stock}/AAPL_stock.csv\")\n",
    "MSFT_df = pd.read_csv(f\"{path_stock}/MSFT_stock.csv\")\n",
    "GOOGL_df = pd.read_csv(f\"{path_stock}/GOOGL_stock.csv\")\n",
    "NVDA_df = pd.read_csv(f\"{path_stock}/NVDA_stock.csv\")\n",
    "AMZN_df = pd.read_csv(f\"{path_stock}/AMZN_stock.csv\")\n",
    "META_df = pd.read_csv(f\"{path_stock}/META_stock.csv\")\n",
    "TSLA_df = pd.read_csv(f\"{path_stock}/TSLA_stock.csv\")\n",
    "AVGO_df = pd.read_csv(f\"{path_stock}/AVGO_stock.csv\")\n",
    "AMD_df = pd.read_csv(f\"{path_stock}/AMD_stock.csv\")\n",
    "CRM_df = pd.read_csv(f\"{path_stock}/CRM_stock.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tach companies stock Data Frame processing\n",
    "- Remove the null / header\n",
    "- Make some features Engineering\n",
    "- Change the column name\n",
    "- Change the time type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_stock_data(df, ticker_symbol):\n",
    "    \"\"\"\n",
    "    Processes a stock data DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with stock data (Price, Close, High, Low, Open, Volume, Ticker).\n",
    "        ticker_symbol (str): Stock ticker symbol (e.g., 'AAPL').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with calculated features and renamed columns.\n",
    "    \"\"\"\n",
    "    df.dropna(inplace=True)  # Remove rows containing any missing values.\n",
    "\n",
    "    columns_to_convert = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
    "    df[columns_to_convert] = df[columns_to_convert].astype(float)  # Convert specified price/volume columns to floating-point numbers.\n",
    "\n",
    "    # Calculate new features based on price data:\n",
    "    df[\"delta_price\"] = df[\"High\"] - df[\"Low\"]  # Calculate the difference between the high and low price for each day.\n",
    "    df[\"avg_price\"] = (df[\"Close\"] + df[\"High\"] + df[\"Low\"] + df[\"Open\"]) / 4  # Calculate the average of the close, high, low, and open prices.\n",
    "    df[\"price_ratio\"] = df[\"delta_price\"] / df[\"avg_price\"]  # Calculate the ratio of the delta price to the average price.\n",
    "    df[\"invest\"] = df[\"Volume\"] * df[\"avg_price\"]  # Calculate the difference between the trading volume and the average price (note: this might not be a standard financial metric and could be re-evaluated).\n",
    "\n",
    "    # Rename the columns for clarity and to include the ticker symbol:\n",
    "    df.rename(columns={\"Price\": \"date\",  # Rename the 'Price' column to 'date'.\n",
    "                        \"Close\": f\"close_{ticker_symbol}\",  # Rename 'Close' to 'cl_ticker'.\n",
    "                        \"High\": f\"high_{ticker_symbol}\",  # Rename 'High' to 'hi_ticker'.\n",
    "                        \"Low\": f\"low_{ticker_symbol}\",  # Rename 'Low' to 'lo_ticker'.\n",
    "                        \"Open\": f\"open_{ticker_symbol}\",  # Rename 'Open' to 'op_ticker'.\n",
    "                        \"delta_price\": f\"delta_price_{ticker_symbol}\",  # Rename 'delta_price' to 'de_ticker'.\n",
    "                        \"avg_price\": f\"avg_price_{ticker_symbol}\",  # Rename 'avg_price' to 'av_ticker'.\n",
    "                        \"invest\": f\"invest_{ticker_symbol}\",  # Rename 'invest' to 'va_ticker'.\n",
    "                        \"price_ratio\": f\"price_ratio_{ticker_symbol}\",  # Rename 'ratio' to 'ra_ticker'.\n",
    "                        'Volume': f'volume_{ticker_symbol}'}, inplace=True)  # Rename 'Volume' to 'Vo_ticker'.\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])  # Convert the 'date' column to datetime objects for proper time series handling.\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)  # Reset the DataFrame's index to a default integer index and drop the original index.\n",
    "\n",
    "    # Drop the 'Ticker' column as the ticker information is now embedded in the column names:\n",
    "    if 'Ticker' in df.columns:\n",
    "        df.drop('Ticker', axis=1, inplace=True)\n",
    "\n",
    "    return df  # Return the processed DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech companies stock clean Data Frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_clean_df = process_stock_data(AAPL_df, 'AAPL')\n",
    "MSFT_clean_df = process_stock_data(MSFT_df, 'MSFT')\n",
    "GOOGL_clean_df = process_stock_data(GOOGL_df, 'GOOGL')\n",
    "NVDA_clean_df = process_stock_data(NVDA_df, 'NVDA')\n",
    "AMZN_clean_df = process_stock_data(AMZN_df, 'AMZN')\n",
    "META_clean_df = process_stock_data(META_df, 'META')\n",
    "TSLA_clean_df = process_stock_data(TSLA_df, 'TSLA')\n",
    "AVGO_clean_df = process_stock_data(AVGO_df, 'AVGO')\n",
    "AMD_clean_df = process_stock_data(AMD_df, 'AMD')\n",
    "CRM_clean_df = process_stock_data(CRM_df, 'CRM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Find the Max and Min od Data column in each companies stock Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_ranges = {}\n",
    "\n",
    "dataframes = {\n",
    "    \"AAPL\": AAPL_clean_df,\n",
    "    \"MSFT\": MSFT_clean_df,\n",
    "    \"GOOGL\": GOOGL_clean_df,\n",
    "    \"NVDA\": NVDA_clean_df,\n",
    "    \"AMZN\": AMZN_clean_df,\n",
    "    \"META\": META_clean_df,\n",
    "    \"TSLA\": TSLA_clean_df,\n",
    "    \"AVGO\": AVGO_clean_df,\n",
    "    \"AMD\": AMD_clean_df,\n",
    "    \"CRM\": CRM_clean_df,\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    if 'date' in df.columns:\n",
    "        min_date = df['date'].min()\n",
    "        max_date = df['date'].max()\n",
    "        stock_data_ranges[name] = {'min_date': min_date, 'max_date': max_date}\n",
    "    else:\n",
    "        print(f\"Warning: 'date' column not found in {name}_clean_df\")\n",
    "\n",
    "# Create a Pandas DataFrame to display the results\n",
    "date_range_df = pd.DataFrame.from_dict(stock_data_ranges, orient='index')\n",
    "date_range_df.index.name = 'Stock'\n",
    "\n",
    "print(date_range_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above result , It seems that the META is started from 2012 while almost the others started from 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro Indicators from Yahoo Finance:\n",
    "- Indices\n",
    "- Commodities\n",
    "- Sector ETFs (Proxies)\n",
    "- Other Market Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_df = pd.read_csv(f\"{path_stock}/macro_indicators_full.csv\")\n",
    "# Convert the 'date' column to datetime objects\n",
    "macro_df['Date'] = pd.to_datetime(macro_df['Date'])\n",
    "macro_df.rename(columns={\"Date\": \"date\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Frame : macro_df ---> Has some missing values that need to be check according to the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let Filter the time after the '2012-05-31'. This is exactly after the time which we have the META stock data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_df_filter = macro_df[macro_df['date'] > data ]\n",
    "min_date_macro_df_filter = macro_df_filter['date'].min()\n",
    "max_date_macro_df_filter = macro_df_filter['date'].max()\n",
    "macro_df_filter.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#macro_df_filter = macro_df_filter.drop('Brent_Crude_Futures',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install missingn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import missingno as msno\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = macro_df_filter\n",
    "data_name = 'macro_df_filter'\n",
    "# 1. Matrix Plot: Visualize the pattern of missingness\n",
    "plt.figure(figsize=(10, 6))\n",
    "msno.matrix(df)\n",
    "plt.title(f'Missing Value Matrix - {data_name}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_clean_df = macro_df_filter.dropna()\n",
    "macro_clean_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_df = pd.read_csv(f\"{path_fed}/combined_economic_indicators.csv\")\n",
    "\n",
    "# Rename the 'Unnamed: 0' column to 'date'\n",
    "fed_df.rename(columns={'Unnamed: 0': 'date'}, inplace=True)\n",
    "\n",
    "# Convert the 'date' column to datetime objects\n",
    "fed_df['date'] = pd.to_datetime(fed_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_df_filter = fed_df[fed_df['date'] > data]\n",
    "min_date_fed_df_filter = fed_df_filter['date'].min()\n",
    "max_date_fed_df_filter = fed_df_filter['date'].max()\n",
    "fed_df_filter.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fed_df_filter\n",
    "data_name = 'fed_df_filter'\n",
    "# 1. Matrix Plot: Visualize the pattern of missingness\n",
    "plt.figure(figsize=(10, 6))\n",
    "msno.matrix(df)\n",
    "plt.title(f'Missing Value Matrix - {data_name}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_clean_df = fed_df_filter[['date', 'cpi', 'fed_rate', 'consumer_confidence','vix', 'oil', 'nonfarm_payrolls',\n",
    "       'treasury_yield', 'industrial_production', 'retail_sales', 'pmi',\n",
    "        'day_of_week', 'is_holiday', 'is_working_day']].dropna()\n",
    "fed_clean_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the Date Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the first DataFrame\n",
    "merged_stock_data = AAPL_clean_df.copy()\n",
    "\n",
    "# List of stock DataFrames (excluding the first one)\n",
    "stock_dfs = [MSFT_clean_df, GOOGL_clean_df, NVDA_clean_df, AMZN_clean_df,\n",
    "             META_clean_df, TSLA_clean_df, AVGO_clean_df, AMD_clean_df, CRM_clean_df]\n",
    "\n",
    "# Merge each stock DataFrame on 'date' using a left join\n",
    "for df in stock_dfs:\n",
    "    merged_stock_data = pd.merge(merged_stock_data, df, on='date', how='inner')\n",
    "\n",
    "# 2. Merge with Macro and Fed DataFrames\n",
    "\n",
    "# Merge stock data with macro data\n",
    "merged_data = pd.merge(merged_stock_data, macro_clean_df, on='date', how='inner')\n",
    "\n",
    "# Merge with fed data\n",
    "final_merged_df = pd.merge(merged_data, fed_clean_df, on='date', how='inner')\n",
    "\n",
    "stock_df = merged_stock_data\n",
    "stock_macro_df = merged_data\n",
    "stock_macro_fed_df = final_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 Companies Investment over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_plot = stock_macro_fed_df[[\n",
    "    'date',\n",
    "    'invest_AAPL','invest_MSFT','invest_GOOGL','invest_NVDA','invest_AMZN',\n",
    "    'invest_META','invest_TSLA','invest_AVGO','invest_AMD','invest_CRM',\n",
    "]]\n",
    "\n",
    "plt.figure(figsize=(12, 6))  # Adjust figure size for better date visibility\n",
    "\n",
    "# Plot each investment column against 'date'\n",
    "for column in stock_plot.columns:\n",
    "    if column != 'date':  # Exclude the 'date' column from the y-axis\n",
    "        plt.plot(stock_plot['date'], stock_plot[column], label=column)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Investment Value')\n",
    "plt.title('Investment over Time')\n",
    "plt.legend(loc='upper left')  # Add legend to distinguish lines\n",
    "plt.grid(True)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlapping labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federal Indicators Over Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fed_date = stock_macro_fed_df[[\n",
    "    'date', 'cpi', 'fed_rate', 'consumer_confidence', 'vix', 'oil',\n",
    "    'nonfarm_payrolls', 'treasury_yield', 'industrial_production', 'retail_sales', 'pmi'\n",
    "]]\n",
    "\n",
    "# Set 'date' as index for easier plotting\n",
    "if 'date' in fed_date.columns:\n",
    "    fed_date['date'] = pd.to_datetime(fed_date['date'])\n",
    "    fed_date.set_index('date', inplace=True)\n",
    "else:\n",
    "    print(\"Error: 'date' column not found in fed_date DataFrame.\")\n",
    "    exit()\n",
    "\n",
    "# --- Assess Data Ranges and Scales ---\n",
    "print(\"Data Ranges and Scales:\")\n",
    "for column in fed_date.columns:\n",
    "    print(f\"- {column}: Range [{fed_date[column].min():.2f}, {fed_date[column].max():.2f}], Scale: {np.ptp(fed_date[column]):.2f}\")\n",
    "\n",
    "# --- Updated Plotting with Adjusted Scale ---\n",
    "plt.figure(figsize=(15, 8))  # Increased figure size for better readability\n",
    "\n",
    "for column in fed_date.columns:\n",
    "    plt.plot(fed_date.index, fed_date[column], label=column)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Federal Indicators Over Time (Raw Scale)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plotting with Standardized Scale ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "fed_date_scaled = fed_date.copy()\n",
    "fed_date_scaled[fed_date_scaled.columns] = scaler.fit_transform(fed_date_scaled)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "for column in fed_date_scaled.columns:\n",
    "    plt.plot(fed_date_scaled.index, fed_date_scaled[column], label=column)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Standardized Value (Mean=0, Std=1)')\n",
    "plt.title('Federal Indicators Over Time (Standardized Scale)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro Indicators Over Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "macro_date = stock_macro_fed_df[[\n",
    "    'date', 'S&P500_Index', 'Dow_Jones_Index', 'NASDAQ_Composite',\n",
    "    'Russell2000_Index', 'VIX_Index', 'Dollar_Index_DXY', 'Gold_Futures',\n",
    "    'WTI_Oil_Futures', 'Copper_Futures', 'Brent_Crude_Futures',\n",
    "    'Tech_Sector_ETF', 'Energy_Sector_ETF', 'Financial_Sector_ETF',\n",
    "    'ConsumerDiscretionary_ETF', 'Lithium_ETF', 'Semiconductor_ETF',\n",
    "    'Electricity_Proxy'\n",
    "]]\n",
    "\n",
    "# Set 'date' as index for easier plotting\n",
    "if 'date' in macro_date.columns:\n",
    "    macro_date['date'] = pd.to_datetime(macro_date['date'])\n",
    "    macro_date.set_index('date', inplace=True)\n",
    "else:\n",
    "    print(\"Error: 'date' column not found in macro_date DataFrame.\")\n",
    "    exit()\n",
    "\n",
    "# --- Assess Data Ranges and Scales ---\n",
    "print(\"Data Ranges and Scales:\")\n",
    "for column in macro_date.columns:\n",
    "    print(f\"- {column}: Range [{macro_date[column].min():.2f}, {macro_date[column].max():.2f}], Scale: {np.ptp(macro_date[column]):.2f}\")\n",
    "\n",
    "# --- Updated Plotting with Adjusted Scale ---\n",
    "plt.figure(figsize=(20, 10))  # Increased figure size for better readability\n",
    "\n",
    "for column in macro_date.columns:\n",
    "    plt.plot(macro_date.index, macro_date[column], label=column)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Macro Indicators Over Time (Raw Scale)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plotting with Standardized Scale ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "macro_date_scaled = macro_date.copy()\n",
    "macro_date_scaled[macro_date_scaled.columns] = scaler.fit_transform(macro_date_scaled)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for column in macro_date_scaled.columns:\n",
    "    plt.plot(macro_date_scaled.index, macro_date_scaled[column], label=column)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Standardized Value (Mean=0, Std=1)')\n",
    "plt.title('Macro Indicators Over Time (Standardized Scale)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas_ta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ta as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 📦 Feature Engineering\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# 🛠️ 1. Investment Total and Individual Stock Investment Ratios\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# ➡️ Calculate total daily investment across top 10 tech stocks\n",
    "stock_macro_fed_df[\"invest_total\"] = (\n",
    "    stock_macro_fed_df[\"invest_AAPL\"] + stock_macro_fed_df[\"invest_MSFT\"] + stock_macro_fed_df[\"invest_GOOGL\"] +\n",
    "    stock_macro_fed_df[\"invest_NVDA\"] + stock_macro_fed_df[\"invest_AMZN\"] + stock_macro_fed_df[\"invest_META\"] +\n",
    "    stock_macro_fed_df[\"invest_TSLA\"] + stock_macro_fed_df[\"invest_AVGO\"] + stock_macro_fed_df[\"invest_AMD\"] +\n",
    "    stock_macro_fed_df[\"invest_CRM\"]\n",
    ")\n",
    "\n",
    "# ➡️ Calculate each company's investment share (ratio)\n",
    "for stock in ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'AMZN', 'META', 'TSLA', 'AVGO', 'AMD', 'CRM']:\n",
    "    stock_macro_fed_df[f\"invest_{stock}_ratio\"] = stock_macro_fed_df[f\"invest_{stock}\"] / stock_macro_fed_df[\"invest_total\"]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 🛠️ 2. Time Features Extraction\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# ➡️ Extract time-based features from 'date'\n",
    "stock_macro_fed_df['day_of_week'] = stock_macro_fed_df['date'].dt.dayofweek        # 0 = Monday, 6 = Sunday\n",
    "stock_macro_fed_df['month'] = stock_macro_fed_df['date'].dt.month                  # 1 = January, 12 = December\n",
    "stock_macro_fed_df['week_number'] = stock_macro_fed_df['date'].dt.isocalendar().week  # ISO week number (1-53)\n",
    "stock_macro_fed_df['is_month_end'] = stock_macro_fed_df['date'].dt.is_month_end.astype(int)  # 1 if last trading day of month\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 🛠️ 3. Set Up DataFrame for Feature Engineering\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# ➡️ Make a working copy\n",
    "df = stock_macro_fed_df.copy()\n",
    "\n",
    "# ➡️ Ensure 'date' is datetime type and set it as index\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 🛠️ 4. First Differencing for Macroeconomic and Indices Variables\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# ➡️ Columns related to macroeconomics, indices, ETFs\n",
    "macro_and_indices_cols = [\n",
    "    'cpi', 'fed_rate', 'consumer_confidence', 'vix', 'oil', 'nonfarm_payrolls', 'treasury_yield',\n",
    "    'industrial_production', 'retail_sales', 'pmi',\n",
    "    'S&P500_Index', 'Dow_Jones_Index', 'NASDAQ_Composite', 'Russell2000_Index', 'VIX_Index',\n",
    "    'Dollar_Index_DXY', 'Gold_Futures', 'WTI_Oil_Futures', 'Copper_Futures', 'Brent_Crude_Futures',\n",
    "    'Tech_Sector_ETF', 'Energy_Sector_ETF', 'Financial_Sector_ETF', 'ConsumerDiscretionary_ETF',\n",
    "    'Lithium_ETF', 'Semiconductor_ETF', 'Electricity_Proxy'\n",
    "]\n",
    "\n",
    "# ➡️ Apply first differencing to remove trend\n",
    "for col in macro_and_indices_cols:\n",
    "    if col in df.columns:\n",
    "        df[f'{col}_diff'] = df[col].diff()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 🛠️ 5. Stock Price Feature Engineering\n",
    "# -------------------------------------------------------\n",
    "\n",
    "stocks = ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'AMZN', 'META', 'TSLA', 'AVGO', 'AMD', 'CRM']\n",
    "\n",
    "for stock in stocks:\n",
    "    for field in ['close', 'open', 'high', 'low']:\n",
    "        col = f'{field}_{stock}'\n",
    "        if col in df.columns:\n",
    "            # ➡️ Differencing for stationarity\n",
    "            df[f'{col}_diff'] = df[col].diff()\n",
    "            # ➡️ Rolling mean and std dev\n",
    "            df[f'{col}_rolling_mean_5'] = df[col].rolling(window=5).mean()\n",
    "            df[f'{col}_rolling_std_5'] = df[col].rolling(window=5).std()\n",
    "            df[f'{col}_rolling_mean_20'] = df[col].rolling(window=20).mean()\n",
    "            df[f'{col}_rolling_std_20'] = df[col].rolling(window=20).std()\n",
    "            # ➡️ Create lag features\n",
    "            for lag in [1, 3, 5, 10]:\n",
    "                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "\n",
    "    # ➡️ Technical Indicators (RSI and MACD)\n",
    "    close_col = f'close_{stock}'\n",
    "    if close_col in df.columns:\n",
    "        df[f'{stock}_RSI'] = ta.rsi(df[close_col], length=14)\n",
    "        macd = ta.macd(df[close_col])\n",
    "        if macd is not None:\n",
    "            df[f'{stock}_MACD'] = macd['MACD_12_26_9']\n",
    "            df[f'{stock}_MACD_signal'] = macd['MACDs_12_26_9']\n",
    "            df[f'{stock}_MACD_hist'] = macd['MACDh_12_26_9']\n",
    "\n",
    "    # ➡️ Volume-based Features\n",
    "    vol_col = f'volume_{stock}'\n",
    "    if vol_col in df.columns:\n",
    "        df[f'{vol_col}_log'] = np.log1p(df[vol_col])\n",
    "        df[f'{vol_col}_diff'] = df[f'{vol_col}_log'].diff()\n",
    "\n",
    "    # ➡️ Price Delta, Avg, Ratio, Investment Differencing\n",
    "    for suffix in ['delta_price', 'avg_price', 'price_ratio', 'invest']:\n",
    "        derived_col = f'{suffix}_{stock}'\n",
    "        if derived_col in df.columns:\n",
    "            df[f'{derived_col}_diff'] = df[derived_col].diff()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 🛠️ 6. Portfolio Level Investment Features\n",
    "# -------------------------------------------------------\n",
    "\n",
    "portfolio_cols = [\n",
    "    'invest_total', 'invest_AAPL_ratio', 'invest_MSFT_ratio', 'invest_GOOGL_ratio',\n",
    "    'invest_NVDA_ratio', 'invest_AMZN_ratio', 'invest_META_ratio',\n",
    "    'invest_TSLA_ratio', 'invest_AVGO_ratio', 'invest_AMD_ratio', 'invest_CRM_ratio'\n",
    "]\n",
    "\n",
    "# ➡️ Apply first differencing to investment ratios\n",
    "for col in portfolio_cols:\n",
    "    if col in df.columns:\n",
    "        df[f'{col}_diff'] = df[col].diff()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 🛠️ 7. Additional Time and Investment Trend Features\n",
    "# -------------------------------------------------------\n",
    "\n",
    "\n",
    "# ➡️ Quarter, Year, Quarter-End, Year-End flags\n",
    "df['quarter'] = df.index.quarter\n",
    "df['year'] = df.index.year\n",
    "df['month'] = df.index.month\n",
    "df['is_quarter_end'] = df.index.is_quarter_end.astype(int)\n",
    "df['is_year_end'] = df.index.is_year_end.astype(int)\n",
    "\n",
    "#(Optional) Standardize Day of Week\n",
    "# If 'day_of_week' is a string like \"Monday\", map to numeric\n",
    "day_mapping = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2,\n",
    "               'Thursday': 3, 'Friday': 4}\n",
    "if df['day_of_week'].dtype == 'object':\n",
    "    df['day_of_week_num'] = df['day_of_week'].map(day_mapping)\n",
    "\n",
    "\n",
    "# ➡️ Lagged and Rolling Investment Total\n",
    "df['invest_total_lag_1'] = df['invest_total'].shift(1)\n",
    "df['invest_total_diff'] = df['invest_total'].diff()\n",
    "df['invest_total_rolling_mean_5'] = df['invest_total'].rolling(window=5).mean()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 🧹 8. Final Cleaning Step\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# ➡️ Drop rows with NaN values caused by differencing, rolling, and shifting\n",
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "check_corr_df = df[[\n",
    "    'close_AAPL', 'close_MSFT', 'close_GOOGL', 'close_NVDA', 'close_AMZN',\n",
    "    'close_META', 'close_TSLA', 'close_AVGO', 'close_AMD', 'close_CRM',\n",
    "    'S&P500_Index', 'Dow_Jones_Index', 'NASDAQ_Composite',\n",
    "    'Russell2000_Index', 'VIX_Index', 'Dollar_Index_DXY', 'Gold_Futures',\n",
    "    'WTI_Oil_Futures', 'Copper_Futures', 'Brent_Crude_Futures',\n",
    "    'Tech_Sector_ETF', 'Energy_Sector_ETF', 'Financial_Sector_ETF',\n",
    "    'ConsumerDiscretionary_ETF', 'Lithium_ETF', 'Semiconductor_ETF',\n",
    "    'Electricity_Proxy',\n",
    "    'cpi', 'fed_rate', 'consumer_confidence', 'vix', 'oil',\n",
    "    'nonfarm_payrolls', 'treasury_yield', 'industrial_production',\n",
    "    'retail_sales', 'pmi'\n",
    "]]\n",
    "\n",
    "corr_df = check_corr_df.corr()\n",
    "\n",
    "# 1. Create a Mask for the Upper Triangle\n",
    "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "\n",
    "# 2. Create a Styled Correlation Matrix with Lower Triangle and Meaningful Colors\n",
    "def styled_lower_triangle_heatmap(df, title=\"Correlation Matrix\"):\n",
    "    \"\"\"\n",
    "    Displays a styled correlation matrix with only the lower triangle visible.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The correlation matrix.\n",
    "        title (str, optional): The title of the heatmap. Defaults to \"Correlation Matrix\".\n",
    "    \"\"\"\n",
    "\n",
    "    styled_df = df.style.format(precision=2).background_gradient(\n",
    "        cmap=sns.diverging_palette(220, 10, as_cmap=True),  # Use a diverging color palette\n",
    "        axis=None,\n",
    "    ).set_properties(**{'font-size': '8pt'})  # Adjust font size as needed\n",
    "\n",
    "    # Apply the mask to hide the upper triangle (does not work directly with style)\n",
    "    masked_df = df.copy()\n",
    "    masked_df[mask] = np.nan  # Replace upper triangle with NaN\n",
    "\n",
    "    # Convert to a DataFrame suitable for display (including index as a column)\n",
    "    display_df = masked_df.reset_index().melt('index')\n",
    "    display_df.dropna(inplace=True)  # Remove NaN values\n",
    "\n",
    "    # Create the heatmap using seaborn for better control\n",
    "    plt.figure(figsize=(20, 20))  # Adjust figure size as needed\n",
    "    sns.heatmap(\n",
    "        masked_df,\n",
    "        annot=True,  # Display correlation values in the heatmap\n",
    "        fmt=\".2f\",   # Format of the annotations\n",
    "        cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "        mask=mask,  # Apply the mask in seaborn\n",
    "        cbar=True,  # Show the color bar\n",
    "        square=True, # Ensure the heatmap cells are square\n",
    "    )\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "    return styled_df\n",
    "\n",
    "styled_corr_df = styled_lower_triangle_heatmap(corr_df, title=\"Lower Triangle Correlation Matrix\")\n",
    "\n",
    "# 3. Print the Styled Correlation Matrix (Table)\n",
    "print(\"\\nStyled Correlation Matrix (Lower Triangle):\\n\")\n",
    "display(styled_corr_df)  # Use display() in a notebook environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📦 Advanced Time Series Decomposition (Trend, Seasonality, Residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 📦 Advanced Time Series Decomposition: Trend, Seasonality, Residual\n",
    "# -------------------------------------------------------\n",
    "\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# --------------------------\n",
    "# 🛠️ Parameters\n",
    "# --------------------------\n",
    "\n",
    "# ➡️ Choose important columns to decompose (you can expand this list)\n",
    "columns_to_decompose = [\n",
    "    'close_AAPL', 'close_MSFT', 'close_GOOGL', 'close_NVDA', 'close_AMZN',\n",
    "    'close_META', 'close_TSLA', 'close_AVGO', 'close_AMD', 'close_CRM',\n",
    "    'S&P500_Index', 'Dow_Jones_Index', 'NASDAQ_Composite',\n",
    "    'Russell2000_Index', 'VIX_Index', 'Dollar_Index_DXY', 'Gold_Futures',\n",
    "    'WTI_Oil_Futures', 'Copper_Futures', 'Brent_Crude_Futures',\n",
    "    'Tech_Sector_ETF', 'Energy_Sector_ETF', 'Financial_Sector_ETF',\n",
    "    'ConsumerDiscretionary_ETF', 'Lithium_ETF', 'Semiconductor_ETF',\n",
    "    'Electricity_Proxy',\n",
    "    'cpi', 'fed_rate', 'consumer_confidence', 'vix', 'oil',\n",
    "    'nonfarm_payrolls', 'treasury_yield', 'industrial_production',\n",
    "    'retail_sales', 'pmi'\n",
    "]\n",
    "\n",
    "# ➡️ Set seasonality period\n",
    "# (252 trading days ≈ 1 year seasonality for stock data)\n",
    "seasonal_period = 252\n",
    "\n",
    "# --------------------------\n",
    "# 🛠️ Decomposition\n",
    "# --------------------------\n",
    "\n",
    "# Loop through each selected feature\n",
    "for col in columns_to_decompose:\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            # ➡️ Apply STL decomposition\n",
    "            stl = STL(df[col].dropna(), period=seasonal_period)\n",
    "            result = stl.fit()\n",
    "            \n",
    "            # ➡️ Save trend, seasonal, and residual components as new columns\n",
    "            df[f'{col}_trend'] = result.trend\n",
    "            df[f'{col}_seasonal'] = result.seasonal\n",
    "            df[f'{col}_residual'] = result.resid\n",
    "            \n",
    "            print(f\"✅ Decomposed: {col}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error decomposing {col}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Column not found: {col}\")\n",
    "\n",
    "# --------------------------\n",
    "# 🧹 Cleaning (optional)\n",
    "# --------------------------\n",
    "\n",
    "# ➡️ After decomposition, drop rows with NaN if needed (optional)\n",
    "df.dropna(inplace=True)\n",
    "# (Because decomposition will create NaNs at the start and end due to windowing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 STL Decomposition Summary\n",
    "\n",
    "This section decomposes the selected time series into three components using **STL (Seasonal-Trend Decomposition using Loess)**:\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "\\[\n",
    "\\text{Original}_t = \\text{Trend}_t + \\text{Seasonality}_t + \\text{Residual}_t\n",
    "\\]\n",
    "\n",
    "- **Original**: The raw time series (e.g., `close_AAPL`)\n",
    "- **Trend**: Long-term smooth movement (extracted via Loess)\n",
    "- **Seasonality**: Repeating patterns (e.g., yearly cycles, period = 252 trading days)\n",
    "- **Residual**: Leftover noise after removing trend and seasonality\n",
    "\n",
    "Each plot visualizes:\n",
    "1. Original time series\n",
    "2. Trend component\n",
    "3. Seasonal component\n",
    "4. Residual (random noise)\n",
    "\n",
    "This decomposition helps us:\n",
    "- Understand underlying structure\n",
    "- Remove noise or seasonality before modeling\n",
    "- Check if residuals are stationary for forecasting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Choose columns to decompose\n",
    "columns_to_decompose = [\n",
    "    'close_AAPL', 'close_MSFT', 'close_GOOGL', # you can add more\n",
    "    'S&P500_Index', 'Gold_Futures', 'cpi' # examples\n",
    "]\n",
    "\n",
    "# Set seasonality period\n",
    "seasonal_period = 252\n",
    "\n",
    "# Loop through each selected feature\n",
    "for col in columns_to_decompose:\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            # Apply STL decomposition\n",
    "            stl = STL(df[col].dropna(), period=seasonal_period)\n",
    "            result = stl.fit()\n",
    "            \n",
    "            # Save trend, seasonal, residual\n",
    "            df[f'{col}_trend'] = result.trend\n",
    "            df[f'{col}_seasonal'] = result.seasonal\n",
    "            df[f'{col}_residual'] = result.resid\n",
    "            \n",
    "            print(f\"✅ Decomposed and plotting: {col}\")\n",
    "            \n",
    "            # --- Plotting each component separately ---\n",
    "            fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
    "            fig.suptitle(f\"Decomposition of {col}\", fontsize=16)\n",
    "\n",
    "            axes[0].plot(df.index, df[col], label='Original', color='blue')\n",
    "            axes[0].set_ylabel('Original')\n",
    "            axes[0].legend()\n",
    "\n",
    "            axes[1].plot(df.index, df[f'{col}_trend'], label='Trend', color='green')\n",
    "            axes[1].set_ylabel('Trend')\n",
    "            axes[1].legend()\n",
    "\n",
    "            axes[2].plot(df.index, df[f'{col}_seasonal'], label='Seasonality', color='orange')\n",
    "            axes[2].set_ylabel('Seasonal')\n",
    "            axes[2].legend()\n",
    "\n",
    "            axes[3].plot(df.index, df[f'{col}_residual'], label='Residual', color='red')\n",
    "            axes[3].set_ylabel('Residual')\n",
    "            axes[3].legend()\n",
    "\n",
    "            plt.xlabel('Date')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error decomposing {col}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Column not found: {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity Testing (ADF) for Time Series Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 📦 Double Stationarity Test: ADF + KPSS for All Columns\n",
    "# -------------------------------------------------------\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# --------------------------\n",
    "# 🛠️ Parameters\n",
    "# --------------------------\n",
    "\n",
    "# ➡️ Columns to test\n",
    "columns_to_test = df.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# ➡️ Prepare to collect results\n",
    "stationarity_results = []\n",
    "\n",
    "# --------------------------\n",
    "# 🛠️ Testing Loop\n",
    "# --------------------------\n",
    "\n",
    "for col in columns_to_test:\n",
    "    try:\n",
    "        # ➡️ Drop NaN values\n",
    "        series = df[col].dropna()\n",
    "\n",
    "        # --- ADF Test ---\n",
    "        adf_result = adfuller(series, autolag='AIC')\n",
    "        adf_pvalue = adf_result[1]\n",
    "        adf_stationary = adf_pvalue < 0.05  # Stationary if p-value < 0.05\n",
    "\n",
    "        # --- KPSS Test ---\n",
    "        kpss_result = kpss(series, regression='c', nlags=\"auto\")\n",
    "        kpss_pvalue = kpss_result[1]\n",
    "        kpss_stationary = kpss_pvalue > 0.05  # Stationary if p-value > 0.05\n",
    "\n",
    "        # --- Final Conclusion ---\n",
    "        if adf_stationary and kpss_stationary:\n",
    "            final_conclusion = True\n",
    "        else:\n",
    "            final_conclusion = False\n",
    "\n",
    "        # ➡️ Append results\n",
    "        stationarity_results.append({\n",
    "            'Feature': col,\n",
    "            'ADF p-value': round(adf_pvalue, 5),\n",
    "            'ADF Stationary': adf_stationary,\n",
    "            'KPSS p-value': round(kpss_pvalue, 5),\n",
    "            'KPSS Stationary': kpss_stationary,\n",
    "            'is_Stationary': final_conclusion\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error testing {col}: {e}\")\n",
    "\n",
    "# --------------------------\n",
    "# 📜 Convert to DataFrame\n",
    "# --------------------------\n",
    "\n",
    "stationarity_df = pd.DataFrame(stationarity_results)\n",
    "\n",
    "# ➡️ Display results\n",
    "stationarity_df.sort_values('is_Stationary', ascending=False).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_df['is_Stationary'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Stationarity Fix: Second Diff, Residuals, Log Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# 🧠 Advanced Stationarity Fix: Second Diff, Residuals, Log Returns\n",
    "# -------------------------------------------------------\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 1. Detect non-stationary features from previous results\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Filter only non-stationary features\n",
    "non_stationary_cols = stationarity_df[stationarity_df['is_Stationary'] == False]['Feature'].tolist()\n",
    "\n",
    "# Prepare result collector\n",
    "fixed_results = []\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 2. Define Helper: Run ADF + KPSS and interpret\n",
    "# -----------------------------------------------\n",
    "\n",
    "def test_stationarity(series):\n",
    "    try:\n",
    "        adf_p = adfuller(series.dropna())[1]\n",
    "    except:\n",
    "        adf_p = np.nan\n",
    "    try:\n",
    "        kpss_p = kpss(series.dropna(), regression='c', nlags=\"auto\")[1]\n",
    "    except:\n",
    "        kpss_p = np.nan\n",
    "    adf_stat = adf_p < 0.05 if not np.isnan(adf_p) else False\n",
    "    kpss_stat = kpss_p > 0.05 if not np.isnan(kpss_p) else False\n",
    "    conclusion = True if adf_stat and kpss_stat else False\n",
    "    return adf_p, adf_stat, kpss_p, kpss_stat, conclusion\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3. Loop through each non-stationary feature and fix\n",
    "# -----------------------------------------------\n",
    "\n",
    "for col in non_stationary_cols:\n",
    "    try:\n",
    "        series = df[col].dropna()\n",
    "\n",
    "        # --- Try log return if column has only positive values\n",
    "        if (series > 0).all():\n",
    "            transformed = np.log(series / series.shift(1))\n",
    "            method = \"log_return\"\n",
    "\n",
    "        # --- Otherwise, try second differencing\n",
    "        else:\n",
    "            transformed = series.diff().diff()\n",
    "            method = \"second_diff\"\n",
    "\n",
    "        # --- If still looks trended, try residual from STL\n",
    "        if transformed.dropna().std() == 0 or transformed.isna().mean() > 0.5:\n",
    "            stl = STL(series, period=252)\n",
    "            transformed = stl.fit().resid\n",
    "            method = \"stl_residual\"\n",
    "\n",
    "        # --- Run stationarity tests on transformed version\n",
    "        adf_p, adf_ok, kpss_p, kpss_ok, conclusion = test_stationarity(transformed)\n",
    "\n",
    "        # --- Append result\n",
    "        fixed_results.append({\n",
    "            'Feature': col,\n",
    "            'Fix Method': method,\n",
    "            'ADF p-value': round(adf_p, 5) if adf_p is not None else None,\n",
    "            'ADF Stationary': adf_ok,\n",
    "            'KPSS p-value': round(kpss_p, 5) if kpss_p is not None else None,\n",
    "            'KPSS Stationary': kpss_ok,\n",
    "            'is_Stationary': conclusion\n",
    "        })\n",
    "\n",
    "        # Save transformed back into df (optional)\n",
    "        df[f'{col}_{method}'] = transformed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {col}: {e}\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 4. Display Fixed Result Summary\n",
    "# -----------------------------------------------\n",
    "\n",
    "fixed_stationarity_df = pd.DataFrame(fixed_results)\n",
    "fixed_stationarity_df.sort_values('is_Stationary', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_df['is_Stationary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_stationarity_df['is_Stationary'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Notebook: Stock market prediction with DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aktienmarkt-Vorhersage mit DeepAR\n",
    "\n",
    "In this capstone project, we use the probabilistic DeepAR model from GluonTS to forecast daily closing prices of a stock (e.g., AAPL). We evaluate the forecasts using RMSE and MAE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Envirnmment and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"gluonts[mxnet]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gluonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basis-Imports\\import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GluonTS-Komponenten\n",
    "# Stelle sicher, dass du eine aktuelle GluonTS-Version installiert hast: pip install gluonts\n",
    "from gluonts.dataset.common import ListDataset\n",
    "# Bei neueren Versionen liegen Modelle in gluonts.mx.model\n",
    "try:\n",
    "    from gluonts.mx.model.deepar import DeepAREstimator\n",
    "    from gluonts.mx.trainer import Trainer\n",
    "except ModuleNotFoundError:\n",
    "    # Fallback für ältere Versionen\n",
    "    from gluonts.model.deepar import DeepAREstimator\n",
    "    from gluonts.trainer import Trainer\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Hyperparameter-Tuning\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2 Data load (already exists)\n",
    "processed_path = '../data/processed_combined_data.csv'\n",
    "df = pd.read_csv(processed_path, index_col='date', parse_dates=True)\n",
    "print(f\"✅ Loded DataFrame: {df.shape} Records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_dataset_with_feats(series: pd.Series, feats: np.ndarray, start_date):\n",
    "    return ListDataset([\n",
    "        {\n",
    "            'start': pd.Timestamp(start_date),\n",
    "            'target': series.values.astype(float),\n",
    "            'feat_dynamic_real': feats\n",
    "        }\n",
    "    ], freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 0.3 Train/Test Split\n",
    "horizon = 30\n",
    "series = df['close_AAPL']\n",
    "features = df.drop(columns=['close_AAPL'])\n",
    "train_series = series[:-horizon]\n",
    "test_series = series[-horizon:]\n",
    "train_features = features[:-horizon]\n",
    "test_features = features[-horizon:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 0.4 Feature-Arrays prepararion\n",
    "train_feat_df = train_features.apply(pd.to_numeric, errors='coerce').ffill().fillna(0)\n",
    "feat_dynamic_train = train_feat_df.values.T.astype(float)\n",
    "test_feat_df = test_features.apply(pd.to_numeric, errors='coerce').ffill().fillna(0)\n",
    "feat_dynamic_test = test_feat_df.values.T.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5 Create Datasets \n",
    "train_ds = make_dataset_with_feats(train_series, feat_dynamic_train, train_series.index[0])\n",
    "test_ds  = make_dataset_with_feats(test_series,  feat_dynamic_test,  test_series.index[0])\n",
    "\n",
    "print(\"✅ Initialization complete. You can now continue working on the notebook from step 1 onwards.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Split data in Training und Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use 'close'-Price as target variable and all other comlums as features\n",
    "horizon = 30  # predicted horizont\n",
    "\n",
    "# complete DataFrame: df (index=date)\n",
    "# TArget variable (close_AAPL') substarct\n",
    "\n",
    "series = df['close_AAPL']\n",
    "\n",
    "# Feature-columns (appart 'close_AAPL')\n",
    "feature_cols = [c for c in df.columns if c != 'close_AAPL']\n",
    "features = df[feature_cols]\n",
    "\n",
    "# Train/Test Split\n",
    "train_series = series[:-horizon]\n",
    "test_series  = series[-horizon:]\n",
    "\n",
    "train_features = features[:-horizon]\n",
    "test_features  = features[-horizon:]\n",
    "\n",
    "print(f\"Training point : {len(train_series)}, Test point : {len(test_series)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. DeepAR-Modelling with Multi-Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.common import ListDataset\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Features as numeric Array \n",
    "# copy training features\n",
    "train_feat_df = train_features.copy()\n",
    "# Convert missing NaN\n",
    "train_feat_df = train_feat_df.apply(pd.to_numeric, errors='coerce')\n",
    "# Replace NaN (with  0 or interpolate )\n",
    "train_feat_df = train_feat_df.fillna(method='ffill').fillna(0)\n",
    "# Transponieren → shape: (num_features, time_length)\n",
    "feat_dynamic_train = train_feat_df.values.T.astype(float)\n",
    "\n",
    "# 2) Create trainings-data set \n",
    "def make_dataset_with_feats(series: pd.Series, feats: np.ndarray, start_date):\n",
    "    return ListDataset([\n",
    "        {\n",
    "            'start': pd.Timestamp(start_date),\n",
    "            'target': series.values.astype(float),\n",
    "            'feat_dynamic_real': feats\n",
    "        }\n",
    "    ], freq='D')\n",
    "\n",
    "train_ds = make_dataset_with_feats(\n",
    "    train_series,\n",
    "    feat_dynamic_train,\n",
    "    train_series.index[0]\n",
    ")\n",
    "\n",
    "# 3) Configure DeepAR-Estimator \n",
    "try:\n",
    "    from gluonts.mx.model.deepar import DeepAREstimator\n",
    "    from gluonts.mx.trainer import Trainer\n",
    "except ModuleNotFoundError:\n",
    "    from gluonts.model.deepar import DeepAREstimator\n",
    "    from gluonts.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    epochs=20,\n",
    "    learning_rate=1e-3,\n",
    "    hybridize=True\n",
    ")\n",
    "\n",
    "deepar_estimator = DeepAREstimator(\n",
    "    freq='D',\n",
    "    prediction_length=horizon,\n",
    "    trainer=trainer,\n",
    "    num_layers=3,\n",
    "    num_cells=64,\n",
    "    cell_type='lstm'\n",
    ")\n",
    "\n",
    "deepar_predictor = deepar_estimator.train(training_data=train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Forecast and evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Test-Features \n",
    "\n",
    "test_feat_df = test_features.copy()\n",
    "\n",
    "test_feat_df = test_feat_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "test_feat_df = test_feat_df.fillna(method='ffill').fillna(0)\n",
    "\n",
    "feat_dynamic_test = test_feat_df.values.T.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Create Test-data set \n",
    "from gluonts.dataset.common import ListDataset\n",
    "\n",
    "test_ds = ListDataset([\n",
    "    {\n",
    "        'start': pd.Timestamp(test_series.index[0]),  # Timestamp-Start\n",
    "        'target': test_series.values.astype(float),\n",
    "        'feat_dynamic_real': feat_dynamic_test\n",
    "    }\n",
    "], freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Create forecast \n",
    "# Predictor is deepar_predictor from training\n",
    "forecast_it = deepar_predictor.predict(test_ds)\n",
    "forecast_obj = next(forecast_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Extract prediction\n",
    "\n",
    "y_pred = np.array(forecast_obj.mean)  # shape = (horizon,)\n",
    "y_true = test_series.values.astype(float)  # shape = (horizon,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Calculate Metric\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"DeepAR mit Multifeatures - RMSE: {rmse:.2f}, MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Visualize\n",
    "t = pd.date_range(start=test_series.index[0], periods=horizon, freq='D')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(t, y_true, label='Ground Truth')\n",
    "plt.plot(t, y_pred, '--', label='DeepAR Forecast mit Features')\n",
    "plt.title('Vorhersage vs. tatsächlicher Kurs mit Multi-Features')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Kurs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Hyperparameter-Optimierung mit Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Objective-Funktion definieren\n",
    "def objective(trial):\n",
    "    # Hyperparameter-Räume\n",
    "    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    epochs = trial.suggest_int('epochs', 10, 50)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "    num_cells = trial.suggest_int('num_cells', 16, 128)\n",
    "\n",
    "    # Trainer und Estimator neu konfigurieren\n",
    "    trainer_opt = Trainer(\n",
    "        epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        hybridize=True\n",
    "    )\n",
    "    est_opt = DeepAREstimator(\n",
    "        freq='D',\n",
    "        prediction_length=horizon,\n",
    "        trainer=trainer_opt,\n",
    "        num_layers=num_layers,\n",
    "        num_cells=num_cells,\n",
    "        cell_type='lstm'\n",
    "    )\n",
    "\n",
    "    # Training auf train_ds\n",
    "    pred = est_opt.train(training_data=train_ds)\n",
    "\n",
    "    # Vorhersage auf test_ds\n",
    "    forecast_it = pred.predict(test_ds)\n",
    "    forecast_obj = next(forecast_it)\n",
    "    y_pred_opt = forecast_obj.mean\n",
    "\n",
    "    # RMSE als Ziel\n",
    "    return mean_squared_error(y_true, y_pred_opt, squared=False)\n",
    "\n",
    "# Studie initialisieren und optimieren\n",
    "# Falls der Run zu lange dauert, kann man mit STRG+C abbrechen oder einen Timeout setzen, z.B.: study.optimize(objective, n_trials=20, timeout=600)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "#try:\n",
    "#    study.optimize(objective, n_trials=20)\n",
    "#except KeyboardInterrupt:\n",
    "#    print(\"Optimierung manuell abgebrochen. Verwende bisher erzielte beste Parameter.\")\n",
    "\n",
    "#Alternativ mit Timeout:\n",
    "study.optimize(objective, n_trials=100, timeout=300)  # 5 Minuten Limit\n",
    "\n",
    "print('Beste Parameter:', study.best_params)\n",
    "print('Bestes RMSE:', study.best_value)\n",
    "\n",
    "print('Beste Parameter:', study.best_params)\n",
    "print('Bestes RMSE:', study.best_value)\n",
    "# Optional: Verlauf der Trials visualisieren\n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Fehlerdiagnose und Modellverbesserung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuen berechnen\n",
    "errors = y_true - y_pred\n",
    "\n",
    "# Zeitreihen-Plot der Fehler\n",
    "date_index = t  # aus Visualisierung\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(date_index, errors)\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.title('Residuals über Zeit')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Fehler (y_true - y_pred)')\n",
    "plt.show()\n",
    "\n",
    "# Histogramm der Residuen\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(errors, bins=20)\n",
    "plt.title('Verteilung der Residuen')\n",
    "plt.xlabel('Fehler')\n",
    "plt.ylabel('Häufigkeit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2 Feature-Skalierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Skalieren aller feat_dynamic_real\n",
    "scaler = StandardScaler()\n",
    "scaled_train = scaler.fit_transform(train_feat_df)\n",
    "scaled_test  = scaler.transform(test_feat_df)\n",
    "# Ersetze feat_dynamic_real durch scaled_* und wiederhole Training & Forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.3 Hyperparameter-Justierung\n",
    "\n",
    "Lernrate (learning_rate): ggf. kleinere Werte wie 1e-4 testen\n",
    "\n",
    "Epochen (epochs): Erhöhung auf 50–100\n",
    "\n",
    "Zellanzahl (num_cells): Reduzieren oder Erhöhen (z.B. 32, 128)\n",
    "\n",
    "Anzahl Layer (num_layers): Variieren zwischen 1–4\n",
    "\n",
    "Nutze dazu Optuna- oder Grid-Search-Logik auf diesen Parametern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Neustart: Modell mit optimalen Hyperparametern und skalierten Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1) Features skalieren\n",
    "scaler = StandardScaler()\n",
    "train_feat_scaled = scaler.fit_transform(train_feat_df)\n",
    "test_feat_scaled  = scaler.transform(test_feat_df)\n",
    "# Transponieren\n",
    "feat_train_scaled = train_feat_scaled.T.astype(float)\n",
    "feat_test_scaled  = test_feat_scaled.T.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Datasets neu erstellen\n",
    "def make_scaled_dataset(series, feats, start):\n",
    "    return ListDataset([\n",
    "        {'start': pd.Timestamp(start),\n",
    "         'target': series.values.astype(float),\n",
    "         'feat_dynamic_real': feats}\n",
    "    ], freq='D')\n",
    "\n",
    "train_ds_scaled = make_scaled_dataset(train_series, feat_train_scaled, train_series.index[0])\n",
    "test_ds_scaled  = make_scaled_dataset(test_series,  feat_test_scaled,  test_series.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3) Estimator mit besten Optuna-Parametern erstellen\n",
    "best_params = study.best_params\n",
    "trainer_best = Trainer(\n",
    "    epochs=best_params['epochs'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    hybridize=True\n",
    ")\n",
    "est_best = DeepAREstimator(\n",
    "    freq='D',\n",
    "    prediction_length=horizon,\n",
    "    trainer=trainer_best,\n",
    "    num_layers=best_params['num_layers'],\n",
    "    num_cells=best_params['num_cells'],\n",
    "    cell_type='lstm'\n",
    ")\n",
    "predictor_best = est_best.train(training_data=train_ds_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Forecast erstellen und auswerten\n",
    "test_iter = predictor_best.predict(test_ds_scaled)\n",
    "forecast_best = next(test_iter)\n",
    "y_pred_best = np.array(forecast_best.mean)\n",
    "rmse_best = mean_squared_error(test_series.values.astype(float), y_pred_best, squared=False)\n",
    "mae_best  = mean_absolute_error(test_series.values.astype(float), y_pred_best)\n",
    "print(f\"Optimiertes Modell RMSE: {rmse_best:.2f}, MAE: {mae_best:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Visualisierung optimiertes Modell\n",
    "dates = pd.date_range(start=test_series.index[0], periods=horizon, freq='D')\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(dates, test_series.values.astype(float), label='Ground Truth')\n",
    "plt.plot(dates, y_pred_best, '--', label='DeepAR optimiert')\n",
    "plt.title('Optimiertes DeepAR-Modell: Vorhersage vs. Realität')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Kurs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.1 GluonTS DeepState (DSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install gluonts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. LSTM mit Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "def build_keras_lstm(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, input_shape=input_shape),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare data: sliding window\n",
    "def create_sequences(series, features, seq_len=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - seq_len):\n",
    "        X.append(features[i:i+seq_len].T)\n",
    "        y.append(series.values[i+seq_len])\n",
    "    return np.stack(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_sequences(train_series, train_feat_df)\n",
    "\n",
    "\n",
    "X_test, y_test   = create_sequences(test_series, test_feat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "keras_model = build_keras_lstm(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "keras_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred_keras = keras_model.predict(X_test)\n",
    "rmse_keras = np.sqrt(np.mean((y_test - y_pred_keras.squeeze())**2))\n",
    "print(f\"Keras LSTM RMSE: {rmse_keras:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
